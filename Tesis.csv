Institución,Título de la tesis,Nombre del tesista,Grado,Nombre del asesor,Resumen,Año
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Estimación del ciclo financiero utilizando métodos de análisis exploratorios: aplicación a Perú en el periodo 2000-2021,"Atoche Murrieta, Lilian Katherine",Magíster en Estadística,"Camiz, Sergio","La presente tesis tiene por objetivo identificar el ciclo financiero del Perú utilizando series temporales financieras como por ejemplo tasas de interés activas y pasivas, monto de ahorro total en el sistema financiero, la liquidez de las entidades financieras, entre otros. La información tiene frecuencia mensual y abarca los periodos 2000-2021. Para determinar el ciclo financiero se utilizan técnicas de análisis exploratorias. Primero empezamos con el análisis de componentes principales el cual nos permitió identificar si existe componentes comunes entre las variables. Luego utilizamos una clasificación jerárquica (HFC por sus siglas en inglés) para agrupar las variables en grupos homogéneos. Este análisis nos permitó asociar a cada grupo una variable representativa el cual permitió identificar los componentes del ciclo financiero. Finalmente utilizamos una tercera técnica exploratoria conocida como análisis evolutivo el cual tiene por objetivo identificar si la correlación de las series varía a lo largo del tiempo. Los resultados del análisis exploratorio confirman la existencia de tres componentes del ciclo financiero. El primer componente identifica el componente de larga duración del ciclo financiero. Este componente mide la evolución del sistema financiero a lo largo del tiempo. El segundo componente mide la evolución de las tasas activas y pasivas en el sistema financiero el cual es un componente de mediano plazo. El tercer componente podemos asociarlo con la volatilidad que tiene el mercado financiero peruano.",2022-09-22
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Método para la fusión de categorías usando técnicas de agrupamiento,"Farro Diaz, Victor Daniel",Magíster en Estadística,"Bayes Rodriguez, Cristian Luis","En la actualidad, muchas organizaciones disponen o tienen acceso a una gran cantidad y variedad de datos que les permiten tomar decisiones acordes en temas económicos, sociales, de educación, de salud, entre otros. Con frecuencia, los estudios que se realizan se enfocan en el objetivo de explicar una variable de interés utilizando un conjunto de variables explicativas; y si la relación de dependencia es lineal, se le conoce como modelo de regresión lineal. Los modelos de regresión lineal presentan su principal reto en la estimación de los parámetros de la regresión, que se consiguen a partir de la información obtenida mediante el análisis de las observaciones de una muestra previamente recogida. La complejidad de los modelos de regresión lineal aumenta con la existencia de covariables que son medidas en una escala nominal u ordinal, y que en muchas ocasiones presentan una gran cantidad de categorías, como por ejemplo: estado civil, grupo sanguíneo, entre otros. Lo habitual para modelar el efecto total de una covariable categórica es definir una categoría (o nivel) como línea base y utilizar variables ficticias para las otras categorías (o niveles).
La presente tesis tiene como principal objetivo el desarrollo del método de fusión de efectos de covariables categóricas usando técnicas de agrupamiento PAM, propuesto por Malsiner-Walli, Pauger y Wagner (2018), y aplicarlo en un conjunto de datos reales relacionados a los ingresos monetarios de la población de Lima Metropolitana y Callao del primer trimestre del 2020.",2022-04-28
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Redes neuronales convolucionales para datos composicionales: Una aplicación a la industria textil de la moda,"Cotacallapa Amanqui, Pavel Arturo",Magíster en Estadística,"Benites Sanchez, Luis Enrique","En muchas situaciones prácticas es necesario el uso de modelos que puedan predecir una colección
de datos limitados por un intervalo cuya suma sea una constante por cada unidad estadística.
Este tipo de variable respuesta se conoce como datos composicionales. Por otro lado, el número de
covariables que se usan para el entrenamiento de este tipo de modelos pueden provenir de datos
asociados a imágenes como la intensidad de los pixeles. En ese contexto, se propone el uso de las
redes neuronales convolucionales como una primera alternativa para intentar estimar este tipo de
variable respuesta. Se utiliza la distribución de Dirichlet como distribución condicional de los datos
y  nalmente se propone una aplicación del modelo utilizando imágenes de prendas de vestir que se
venden por catálogo en donde el objetivo es predecir las participaciones de las tallas que se venden
por cada unidad estadística.",2022-04-07
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Modelo de regresión cuantílica para respuestas positivas con censura intervalar,"Manrique Urbina, Justo Andrés",Magíster en Estadística,"Bayes Rodriguez, Cristian Luis","La presente tesis propone un modelo de regresi on cuant  lica en d onde la variable es no
negativa y posee censura intervalar, es decir que esta no es directamente observable, y la  unica
informaci on que se conoce sobre ella es que se encuentra en cierto intervalo. Para evaluar si
la metodolog  a de estimaci on captura adecuadamente los par ametros poblacionales desde el
punto de vista de la inferencia cl asica, se desarrolla un estudio de simulaci on. Finalmente, se
aplica el modelo a los datos de la Encuesta Nacional de Satisfacci on de Salud ejecutada el
a~no 2015. La estructura del modelo permite evaluar los factores relacionados al sueldo de los
profesionales en salud (el cual hab  a sido censurado desde el proceso de recolecci on de datos).
El presente modelo es una extensi on al modelo de regresi on de censura intervalar expuesto
en Sal y Rosas et al. (2019), pues eval ua los factores subyacentes a una variable respuesta a
lo largo de sus cuantiles.",2022-03-21
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Clasiﬁcación de riesgo para frecuencias y severidades en un seguro de automóviles usando modelos GAMLSS,"Hernández Bello, Diana Patricia",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","En la tariﬁcación de seguros generales, en particular en seguros de vehículos, es valioso incorporar toda la información disponible del asegurado, del bien asegurado y de los siniestros que se han presentado, con el ﬁn de obtener modelos que consideren las variables relevantes en la estimación y así generar una prima de riesgo adecuada para el riesgo que se está analizando.
Los modelos a considerar están construidos con base en las reclamaciones que ha presentado el asegurado y su estimaci´on se obtiene mediante distribuciones del número y monto de siniestros dando como resultado tarifas que incluyen recargos y descuentos en base a la experiencia siniestral, lo que se conoce como Sistema Bonus-Malus. Adicionalmente se han analizado modelos de regresión que incluyen información tanto del asegurado como del vehículo y cuya estimación de la prima de riesgo se realiza a través de la media tanto de la frecuencia como de la severidad. Sin embargo, dado que los riesgos en la cartera expuesta son heterogéneos, se plantean también modelos de regresión en los que la estimación de la frecuencia y la severidad se realiza a través de parámetros como: la media, la varianza, el sesgo y la curtosis, estos últimos son denominados modelos aditivos generalizados de localización, escala y forma (GAMLSS).",2022-03-16
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Approximate bayesian inference for directed acyclic graph autoregressive models,"Buendía Narváez, Julio César",Magíster en Estadística,"Quiroz Cornejo, Zaida Jesús","La prevalencia de enfermedades epidemiológicas recolectadas en áreas geográficamente limitadas, como distritos o provincias, son cruciales para la toma de decisiones en salud pública. Usualmente esta variable respuesta presenta dependencia espacial, es decir, es similar en áreas vecinas, debido a la naturaleza de la enfermedad, clima, nivel económico y cultural, entre otras razones. En este sentido, se proponen modelos espaciales de datos áreas para identificar tendencias y factores asociados a enfermedades epidemiológicas, tomando en cuenta la dependencia espacial entre áreas geográficas. Por lo general, estos modelos ajustan a la dependencia espacial a través de efectos aleatorios derivados a través de grafos. En particular, el modelo autorregresivo de gráfico acíclico dirigido (DAGAR) se basa en un grafo acíclico dirigido y algunos efectos aleatorios \del pasado"". Como consecuencia, la matriz de precisión (inversa de la covarianza) del modelo es dispersa. Este modelo tiene una interpretación intuitiva de los parámetros asociados con la dependencia espacial y se puede representar como un modelo gaussiano latente. En este contexto, en esta tesis se propone implementar el modelo DAGAR a través del método de inferencia bayesiano aproximado INLA que es determinista, bastante preciso y eficiente. Dentro de este enfoque, la estimación de datos grandes se puede realizar en segundos o minutos, y permite ajustar los datos con distribución gaussiana o no gaussiana. Finalmente, para mostrar el aporte de esta propuesta, el modelo DAGAR se ajusta a datos reales.",2022-02-02
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Fusión de efectos para modelos de regresión con respuesta positiva bajo un enfoque bayesiano,"Dongo Román, Andie Bryan",Magíster en Estadística,"Bayes Rodriguez, Cristian Luis","El presente trabajo tiene como objetivo adaptar el modelo bayesiano para fusión de efectos presentado por Pauger y Wagner (2019), de tal manera que sea adecuado para modelos de regresión con respuesta positiva bajo una distribución gamma. El modelo plantea como distribución a priori de los coeficientes de cada covariable cualitativa a una normal multivariada, deducida a partir de una distribución a priori spike y slab para la diferencia de cada par de efectos, cuya matriz de precisión permite conocer qué niveles pueden fusionarse.
La estructura de la matriz de precisión depende de un hiperparámetro que permite estimar las probabilidades de fusión a posteriori entre cada par de niveles, con las cuales se pueden agrupar aquellos niveles con efectos similares mediante la función de pérdida de Binder. La estimación a posteriori del modelo es realizada con métodos MCMC utilizando el programa JAGS en R.
Se aplicó la metodología a un conjunto de datos reales extraído de la Encuesta Nacional de Hogares (ENAHO) del año 2019, donde se pudo verificar la existencia de una brecha salarial por etnicidad en los entrevistados de la macro región sur del Perú. Así mismo, se incluyó en el caso aplicativo a la interacción entre los efectos de la etnicidad y el sexo, revelándose que la brecha por género existente es mayor en la población aymara y en la no indígena, en comparación con la población quechua.",2022-01-10
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Identificación de conglomerados espaciales de acuerdo a niveles de morosidad de empresas en el Perú,"Tristán Gómez, Alex Edward",Magíster en Estadística,"Quiroz Cornejo, Zaida Jesús","El cumplimiento de las obligaciones  financieras que tienen las empresas es respaldado por
una correcta gestión de riesgo de crédito, esto evita problemas de liquidez y solvencia. Por
ello es importante detectar los niveles de riesgo de morosidad en las empresas. La presente
tesis tiene como objetivo identifi car conglomerados de provincias del Perú, en funciona de la
tasa de incumplimiento de pagos, conocida también como la tasa de morosidad. Para ello se
propone un modelamiento en dos niveles. En el primer nivel se usan modelos aglomerativos
jerárquicos para seleccionar n conglomerados candidatos a priori, donde el número fi nal de
conglomerados se escoge mediante criterios de selección de modelos. Posteriormente, en un
segundo nivel, modelaremos el nivel de riesgo haciendo uso del modelo de Poisson y prioris
condicionales autoregresivas en base a los conglomerados de nidos en el primer nivel e incluyendo
covariables. Los modelos pueden ser reescritos como modelos Gaussianos latentes, y se
puede usar inferencia bayesiana para estimar sus parámetros, específicamente a través de la
aproximación de Laplace anidada integrada. Finalmente, como resultado de la aproximación
se obtienen conglomerados de provincias de acuerdo a sus niveles de morosidad, permitiendo
clasi ficar las provincias en conglomerado de alto, medio y bajo nivel de riesgo de morosidad.",2021-11-07
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Mixtura finita de una distribución Birnbaum-Saunders basado en la familia de mixtura en parámetros de escala de distribuciones normal asimétrica,"Gavidia Pucllas, Daniel Elías",Magíster en Estadística,"Benites Sánchez, Luis Enrique","La presente tesis muestra la distribución mixtura de distribuciones Birnbaum-Saunders basados en mixturas de escala normal asimétrica (MF-BS-MENA). Este modelo es una extensión a la propuesta de Maehara (2018a) para datos unimodales basados en distribuciones con mixtura de escala normal asimétrica utilizada para modelar datos con percentiles extremos y altamente concentrados a la izquierda de la distribución. El modelo propuesto permite modelar datos con dos o más componentes de mixtura de distribuciones asimétricas como la t de Student asimétrica (TA), la Slash asimétrica (SLA), y la normal contaminada asimétrica (NCA). Para estimar los parámetros del modelo propuesto se presenta un método de estimación basado en el algoritmo de maximización condicional de la esperanza (una extensión del algoritmo EM). Además, se desarrollan simulaciones que muestran la precisión de las estimaciones y los errores estándar. Por último, se realizan aplicaciones con un conjunto de datos reales.",2021-10-06
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Modelo de regresión no lineal basado en una mixtura de la distribución senh-normal/independiente en el error,"Ocampo Corrales, Carlos Iván",Magíster en Estadística,"Benites Sánchez, Luis Enrique","La distribución normal, si bien útil para explicar la distribución de muchos conjuntos
de datos, a veces es inadecuada para ello. En este sentido, en muchos casos es conveniente
trabajar con transformaciones de la distribución normal por ejemplo log-normal, Birnbaum-
Saunders (BS) y Senh-Normal (SN). En esta tesis se presenta un modelo de regresión no lineal
basado en una mixtura  finita de distribuciones Senh-Normal/Independiente (SNI) en el error
considerando dos casos específicos de esta distribución, SN y Senh-t-Student (SSt), respectivamente.
En el contexto de regresión se plantea una metodología de estimación mediante la
aplicación del algoritmo EM y también para el cálculo de los errores estándar.
Se realizaron estudios de simulación para evaluar las propiedades de las estimaciones. Los
resultados muestran que el modelo estima de manera satisfactoria los parámetros, más aún,
evaluando el sesgo y el RSME de las estimaciones se observa que el modelo cumple con las
propiedades asintóticas de los estimadores de máxima verosimilitud. Asimismo, se realizaron
estudios de aplicación tanto para el modelo SN como SSt.",2021-09-22
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Modelos de regresión a la media con efectos mixtos para variable respuesta semicontinua,"Bautista Bautista, Luis Alberto",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","En muchas situaciones se dispone de una variable aleatoria continua no negativa con
asimetría positiva que eventualmente podría tomar el valor cero. Datos de esta naturaleza son llamados semicontinuos o cero-inflacionados y fueron tradicionalmente modelados usando el modelo de regresión de dos partes propuesto por Duan et al. (1983). En este modelo la variable respuesta sigue una distribución mixta de probabilidades conformada por una distribución de Bernoulli y una distribución continua no negativa. Una versión longitudinal de este modelo de regresión, pero que apunta a explicar la media de la variable de respuesta, fue propuesto por Smith et al. (2017). Este modelo planteaba, para su componente continua de respuesta, una distribución Log Skew Normal. El objetivo de este trabajo es estudiar un modelo alternativo al de Smith et al. (2017), que llamaremos, en general, un modelo de regresión a la media con efectos mixtos para respuestas semicontinuas, pues plantea una parametrización que permite estimar e interpretar los efectos de un conjunto de covariables sobre la media de las respuestas y no sobre la media condicionada a valores positivos. A diferencia del modelo de Smith et al. (2017), que hace uso de la distribución Log Skew Normal cero-inflacionada, nosotros modelaremos la respuesta con una distribución Gamma Generalizada cero-inflacionada. Este modelamiento, como se muestra, permite capturar de manera flexible ciertas características de los datos de respuesta, tales como, la asimetría y el comportamiento de las colas. Los resultados del estudio de simulación para el nuevo modelo mostraron un adecuado desempeño en la recuperación de sus parámetros, donde para la estimación de estos utilizamos un enfoque bayesiano y el uso de métodos MCMC Hamiltonianos. Por último, los resultados de su aplicación en el estudio longitudinal del efecto que ciertas variables podrán ejercer sobre la media de los gastos en educación de los hogares en el Perú, mostraron un mejor ajuste a los datos respecto al modelo de Smith et al. (2017), en base a los criterios de información ampliamente aplicado y de validación cruzada de Leave-one-out.",2021-09-01
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Modelo de regresión lineal con censura basado en una distribución senh-normal/independiente: una perspectiva frecuentista,"Alonzo Huaman, Max Walter",Magíster en Estadística,"Benites Sánchez, Luis Enrique","En esta tesis se estudia el modelo de regresión lineal para datos censurados considerando
una distribución senh-normal/independiente para los errores desde un enfoque frecuentista.
Este trabajo considera la revisión de la teoría existente, la construcción del nuevo modelo,
estimación de parámetros, estudios de simulación para recuperar los parámetros del modelo
y la aplicación a un conjunto de datos reales.",2021-08-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Inferencia bayesiana en un modelo de regresión cuantílica autorregresivo,"Quintos Choy, Manuel Alejandro",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","El modelo de regresión cuantílica autorregresivo permite modelar el cuantil condicional
de una serie de tiempo a partir de los rezagos de la serie. En el presente trabajo se presenta
la estimación de este modelo desde la perspectiva bayesiana asumiendo que los errores se
distribuyen según la distribución asimétrica de Laplace (ALD). Luego, el proceso de generación de muestras de la distribución a posteriori es simplificado utilizando una representación estocástica de la ALD propuesta por Kotz et al. (2001) y el algoritmo de datos aumentados de Tanner y Wong (1987), siguiendo la propuesta de Kozumi y Kobayashi (2011), así como las adaptaciones para el modelamiento de series de tiempo de Cai et al. (2012) y Liu y Luger (2017). Los estudios de simulación demuestran que el supuesto sobre la distribución del término error no es limitante para estimar el cuantil condicional de series de tiempo con otras distribuciones. El modelo es aplicado en la predicción del Valor en Riesgo (VaR) en la serie de tiempo de los retornos diarios de la tasa de cambio de PEN a USD, y sus resultados son comparados con las predicciones obtenidas por las metodologías RiskMetrics, GARCH(1,1) y CAVIaR. Al respecto, la evidencia numérica permite concluir que el modelo QAR es una alternativa válida para estimar el VaR.",2021-06-14
Pontificia Universidad Católica del Perú. Escuela de Posgrado.,Modelo de regresión semiparamétrico robusto,"Esquivel Segura, Henry John",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","El presente trabajo de tesis presenta un modelo de regresión semiparamétrico con errores
t-Student, que permite estudiar el comportamiento de una variable dependiente dado un
conjunto de variables explicativas cuando los supuestos de linealidad y normalidad no se
cumplen. La estimación de los parámetros se realiza bajo el enfoque bayesiano a través del
algoritmo de Gibbs. En el estudio de simulación se observa que el modelo propuesto es más
robusto ante la presencia de valores atípicos que el usual modelo regresión semiparamétrico
normal. Asimismo se presenta una aplicación con datos reales para ilustrar esta característica.",2021-05-11
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo de regresión Dirichlet bayesiano: aplicación para estimar la prevalencia del nivel de anemia infantil en centros poblados del Perú,"Andrade Chávez, Francisco Mauricio",Magíster en Estadística,"Quiroz Cornejo, Zaida Jesús","La anemia es una afección causada por un bajo nivel de hemoglobina en la sangre causada
principalmente por un déficit en el consumo de hierro. En el Perú, es un problema de
salud pública y nutrición principalmente en niñas y niños menores de cinco años, por ello el
Instituto Nacional de Estadística (INEI) realiza una prueba para determinar anemia en niñas
y niños a través de la Encuesta Demográfica y de Salud Familiar (ENDES). En esta encuesta
se clasifica los niveles de anemia como severa si es menor a 7,0 g/dl, moderada si está entre
7,0 y 9,9 g/dl o leve si varía entre 10,0 y 11,9 g/dl. En este contexto, en esta tesis se propone
aplicar el modelo de regresión de Dirichlet para estimar la prevalencia de los niveles de
anemia infantil a nivel de centros poblados en el año 2017. Se propone estimar los parámetros
usando inferencia bayesiana, a través del método Halmitoniano de Monte Carlo (HMC) usando
Rstan. El modelo propuesto también permite identificar posibles factores determinantes
de la prevalencia de la anemia infantil y tiene el propósito de mejorar las políticas públicas
dirigidas a la reducción de la anemia en el país.",2021-03-29
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Endpoint-inflated beta-binomial regression for correlated count data,"Fazio Luna, Boris Manuel",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","El modelo de regresión binomial con in acción en los extremos permite modelar datos de
conteo acotados en los que una alta proporción de las observaciones se encuentra en los extremos.
Extendemos el modelo considerando una función de enlace de logit ordenado, la cual
aprovecha la información de orden implícita en las probabilidades de in
acción y exploramos
el uso de efectos aleatorios y marginalización para manejar la presencia de observaciones
repetidas. Empleamos un conjunto de datos previamente analizado en la literatura mediante
un modelo de regresión binomial con in
acción en los extremos que emplea el enlace softmax
para mostrar el mejor ajuste logrado por nuestro modelo.",2021-03-29
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Aplicación del modelo de espacio de estados con errores correlacionados a la tasa de desempleo en Perú,"Visa Flores, Rafael",Magíster en Estadística,"Sikov, Anna","En este trabajo se presenta los modelos de espacio de estados con errores correlacionados,
propuesto por Pfeffermann y Tiller (2006), aplicado a datos reales de la tasa de desempleo
para Lima Metropolitana, cuya información es recolectada mediante la Encuesta Permanente
del Empleo - EPE por el Instituto Nacional de Estadística e Informática. Estos modelos
permiten dar tratamiento a series de tiempo con errores de medición correlacionados, la
estimación de los componentes del modelo se realiza mediante el algoritmo recursivo de
Pfeffermann y Tiller, y cuando los errores son independientes se utiliza el algoritmo recursivo
del filtro de Kalman. Se realizó un estudio de simulación con series de tiempo con errores correlacionados con
el objetivo de comparar las predicciones obtenidas con el algoritmo del filtro de Kalman y el
algoritmo de Pfeffermann y Tiller, resultando este último con menores errores de predicción. Con la finalidad de comparar la aplicación del modelo de espacio de estados con errores
correlacionados con una metodología muy conocida como el desarrollado por Box and Jenkins,
se ajustó los datos de la tasa de desempleo a un modelo ARIMA, se comparó las predicciones
de ambos modelos con las verdaderas observaciones, donde los errores de las predicciones
fueron similares, sin embargo, el menor error cuadrático medio se obtuvo con el modelo de
espacio de estados con errores correlacionados.",2021-02-25
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelamiento del tiempo a la ocurrencia de un evento con tiempos discretos,"Huertas Quispe, Anthony Enrique",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","En este trabajo de tesis, se plantea estudiar el tiempo a la ocurrencia de un evento en un proceso discreto. Para ello, se considera un modelo mixtura de fracción de cura sobre una población segmentada en dos tipos de individuos: sujetos curados, o también denominados sobrevivientes a largo plazo, haciendo referencia a aquellos sujetos que no alcanzarán el evento de interés en estudio; y sujetos no curados, o también denominados sujetos susceptibles, quienes en un tiempo específico, experimentarán dicho evento de interés. Los objetivos principales de esta tesis, son el de estimar la fracción de cura, la cual está definida como la proporción de individuos curados al final del estudio, y estimar el tiempo de falla para los individuos susceptibles, entendiéndose como el tiempo a la ocurrencia del evento. Este análisis se llevará a cabo con la presencia de covariables y datos censurados, siendo la simulación e inferencia de los datos efectuados vía el software estadístico R, en donde los procesos de simulación abordarán distintos escenarios para evaluar la performance del modelo propuesto.",2021-01-18
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Métodos de selección de variables bajo el enfoque bayesiano para el modelo lineal normal,"Blas Oyola, Sthip Frank",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","En muchos casos prácticos, al realizar un análisis de regresión, se cuenta con un gran
número de potenciales variables explicativas de las cuáles sólo algunas serán importantes para explicar la variable respuesta. Por lo tanto, un problema importante para la construcción de un modelo de regresión es encontrar un adecuado conjunto de variables explicativas. A los métodos que lidian con este problema se les denomina métodos de selección de variables. En el presente proyecto de tesis, se estudiarán tres métodos de selección de variables bajo inferencia bayesiana para el modelo de regresión lineal normal los cuales fueron propuestos por George y McCulloch (1993), Kuo y Mallick (1998) y Dellaportas et al. (2002). Estos métodos, a diferencia de los métodos tradicionales, consideran la selección de variables dentro del mismo modelo, por ejemplo, introduciendo variables latentes que indiquen la presencia o ausencia de una variable explicativa. Se realizaron comparaciones de estos métodos bayesianos con los métodos Lasso y Stepwise por ser los más tradicionales. A través de un estudio con datos simulados, en diversos escenarios se observa que los métodos bayesianos permiten una adecuada selección de las variables explicativas. Adicionalmente se presentan los resultados de una aplicación con datos reales.",2021-01-18
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Sistema de tarifación bonus-malus para la rama de seguros de automóvil,"Vivanco Ortiz, Yoshi Abel",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","En la actualidad, las empresas aseguradoras cuentan con productos de seguros
cada vez más personalizados a las características de sus asegurados, de modo que,
cada asegurado no pague el mismo monto de prima sino un monto proporcional a su
comportamiento y perfil de riesgo. Una de las formas de atender esta necesidad de
personalización en la tarifación es el Sistema Bonus-Malus (SBM), el cual ajusta una
prima base considerando la historia de siniestros reportados por cada asegurado. En
ese sentido, una historia sin siniestros crea bonificaciones (bonus) y por ende una
reducción en la prima de seguro; y, una historia con siniestros genera penalizaciones
(malus) y por ende un incremento en la prima de seguro. Por tanto, el objetivo de
esta tesis es aplicar los modelos SBM basados en la frecuencia para un seguro de
tipo vehicular. Para ello, en base a la información disponible de los asegurados, se
construye un modelo de frecuencia de siniestros usando un GLM (Poisson, Binomial
Negativa y sus variantes inflacionadas en ceros), cada modelo permite obtener
una prima base y clases de riesgo basados en características heterogéneas. Luego,
se comparan todos los modelos obtenidos para seleccionar el mejor ajuste para los
datos analizados. Por u´ltimo, se aplica el SBM y se determina en qué nivel se clasifica
a cada asegurado en función al número de siniestros que reporte en el periodo
de análisis, de esa manera, se determina el valor de la prima ajustada para cada
asegurado. En resumen, este trabajo desarrolla un SBM con información a priori y a
posteriori que permite obtener primas más justas para los asegurados de un producto
de seguros vehiculares, de modo que, el asegurado que presente un comportamiento
sin siniestros reportados pagará menos que un asegurado que presente siniestros en
el periodo evaluado.",2021-01-18
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo de supervivencia de larga duración con riesgos proporcionales y estimación del riesgo base vía splines: modelamiento de abandono de seguros,"Mattos Galarza, Hector",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","Los modelos de supervivencia, aquellos que tratan de describir el tiempo a la ocurrencia de uno
o más eventos, han demostrado tener gran versatilidad para poder modelar distintos tipos de eventos
y un alcance mayor al que inicialmente se propuso. Su aplicación varía desde el área de la medicina
hasta usos en actividades financieras como análisis de riesgos de activos, entre otros. Este trabajo
tiene como motivación el análisis del tiempo de permanencia de un cliente con contrato de póliza de
seguros. En esta aplicación, solo una fracción de los clientes son susceptibles a la terminación del
contrato y, en este sentido, se requiere que el modelo cuente con la flexibilidad de asumir que no
todos los clientes son susceptibles al evento de interés. En este trabajo, se propone un modelo de larga
duración asumiendo un modelo de riesgos proporcionales para los clientes susceptibles de abandono
y donde la función de riesgo basal de este último se modela vía funciones de splines monótonas. Este
trabajo empieza con la definición del modelo, el proceso de estimación de parámetros, escenarios
de simulación donde se evalúa el desempeño del proceso de estimación e inferencia y finalmente una
aplicación para estudiar los factores asociados con el abandono de clientes en una compañía de seguros
en el Perú.",2021-01-12
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Jointly modelling of cluster dependent pro les of fractional and binary variables from a Bayesian point of view,"Cortés Tejada, Fernando Javier",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","En la presente tesis se proponen modelos de clasificación basados en regresiones beta inflacionadas cero-uno con efectos mixtos para modelar perfiles longitudinales de variables fraccionarias mixtas y variables binarias de forma conjunta con formación de clústeres. Las distintas parametrizaciones de los modelos propuestos permiten modelar distintos efectos, como modelar directamente la media marginal a través de covariables e interpretar fácilmente su efecto sobre ella o modelar la media condicional y las probabilidades de inflación de forma separada. Además, se forman clústeres de grupos de individuos con perfiles longitudinales similares a través de una variable latente, asumiendo que las variables respuesta siguen un modelo de mixtura finita. Debido a la complejidad de los modelos, los parámetros se estiman desde un punto de vista bayesiano, a partir de simulaciones MCMC utilizando el software JAGS en R. Se prueban los modelos propuestos sobre diferentes bases de datos simulados
para medir el desempeño de los mismos y se comparan con otros modelos a fin de verificar cual ajusta mejor los perfiles longitudinales de variables fraccionarias mixtas y variables binarias. Por último, se aplican los modelos propuestos a datos reales de un banco peruano, con información del ratio de uso de tarjetas de crédito en el periodo de un año, estado de default del cliente y otras covariables correspondientes al cliente poseedor de la tarjeta, con el objetivo de obtener clústeres de individuos con similar ratio de uso de tarjeta de crédito y relacionarlos con la probabilidad de caer en default que presenta cada grupo.",2020-10-27
Pontificia Universidad Católica del Perú. Escuela de Posgrado,"Regresión espacial cuantílica para variables acotadas entre (0,1)","García Céspedes, Carlos Jeffer",Magíster en Estadística,"Quiroz Cornejo, Zaida Jesús","El Perú es un país emergente donde el desarrollo se centra en algunas ciudades y distritos específicos. Esto conlleva a mucha desigualdad económica por ello resulta importante dar seguimiento a la incidencia de pobreza en el país. De acuerdo al nivel de precariedad, la pobreza puede considerarse extrema o no extrema. En este contexto, estudiamos la incidencia de pobreza no extrema a través de un modelo de regresión cuantílica espacial a nivel distrital en la provincia de Lima utilizando la distribución de Kumaraswamy combinada con un efecto espacial intrínseco condicional autorregresivo (ICAR). Para tratar y evaluar la posible confusión espacial entre los efectos espaciales y las covariables de efectos fijos, se considera, también, el enfoque SPOCK (Spatial Orthogonal Centroid \K""orrection). Nuestros modelos pertenecen a la clase de modelos jerárquicos, para los cuales la inferencia se puede realizar utilizando el método de Monte Carlo Hamiltoniano. Por lo tanto, el modelo es computacionalmente factible para grandes conjuntos de datos, puede describir puntos extremos de la distribución de la incidencia de pobreza no extrema e identificar qué factores son importantes en las colas de la distribución de los datos.",2020-10-26
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Extensión al modelo DINA reparametrizado con covariable,"Sáenz Egúsquiza, Miguel Angel",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","En el campo educacional, cuando los estudiantes resuelven problemas su habilidad en un tema particular puede influir en el desempeño de los mismos en un área de estudio similar pero diferente. Por ejemplo, la habilidad en ciencias podría tener un efecto en su dominio sobre las matemáticas, lo que a su vez afectará la forma en que los evaluados responden a las preguntas o ítems sobre matemáticas de una prueba. Por tanto, resulta natural examinar la relación entre el rendimiento en un área particular de estudio y el dominio de los atributos en un tema relacionado. Los modelos de diagnóstico cognitivo (CDM) proporcionan un marco ideal para realizar un análisis de este tipo, ya que clasifican a los examinados en perfiles de atributos que indican su dominio en las habilidades delimitadas permitiendo obtener información más específica con respecto a sus fortalezas y debilidades. Los CDM resuelven varias limitaciones de los métodos clásicos y los modelos de teoría de respuesta a ítems unidimensionales (TRI).
Para este estudio se amplía el marco de DINA al incorporar una covariable en un modelo de DINA reparametrizado. La covariable se puede especificar en dos niveles: en el nivel inferior, afectando la forma en que los evaluados resuelven los ítems (es decir, la probabilidad de respuesta), y en el nivel superior, influenciando en el dominio de los atributos (es decir, la clasificación latente). En esta tesis, se desarrolla teóricamente el modelo indicado desde el enfoque clásico. Para la estimación desarrollaremos el método de máxima verosimilitud y el método de la moda a posteriori vía el algoritmo de Esperanza-Maximización (EM) y de Newton-Raphson. Para tal fin, se realiza 4 estudios de simulación con la finalidad de observar en primer lugar el efecto de la covariable cuando afecta simultáneamente a los ítems y a los atributos, luego cuando la covariable afecta por separado a ambos, y también cuando la covariable no los afecta. Finalmente, se muestra su aplicación en la evaluación de la prueba de admisión a una Universidad.",2020-10-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo bayesiano geoestadístico beta-inflacionado utilizando NNGP con aplicación a datos de cobertura forestal,"Barriga Pozada, Alfonso Carlos Cesar",Magíster en Estadística,"Quiroz Cornejo, Zaida Jesús","En esta tesis proponemos un nuevo modelo geoestadístico beta inflacionado en ceros y unos utilizando NNGP (del inglés Nearest Neighbor Gaussian Process). La ventaja principal de modelar los efectos espaciales utilizando NNGP es la reducción del elevado tiempo computacional que con lleva modelar un proceso gaussiano, ya que no necesita trabajar con todos los vecinos sino solo con un grupo reducido. La estimación de los parámetros se llevó a cabo desde una perspectiva bayesiana. Además, se llevó a cabo un estudio de simulación en el cual se hicieron pruebas con diferentes cantidades de vecinos para evaluar en términos de RMSE y tiempo computacional la ganancia en la estimación del modelo al agregar más vecinos. Finalmente, se modeló la proporción de cobertura forestal en Hiroshima utilizando el modelo geoestadístico desarrollado, obteniendo buenos resultados.",2020-09-29
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo lineal mixto de clases latentes con respuesta ordinal y su aplicación en la medición de la religiosidad,"Renteria Sacha, Ivonne Mireille",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","Los modelos lineales mixtos de clases latentes desarrollados por Proust-Lima, Philipps y Liquet (2017) son útiles para analizar el aspecto dinámico y la naturaleza multidimensional de un fenómeno de interés en poblaciones no necesariamente homogéneas. Estos permiten identificar las posibles clases latentes en la población bajo estudio y cómo un conjunto de covariables afecta en cada clase a la variable respuesta de interés. En esta tesis se desarrolla el modelo lineal mixto de clases latentes con variable respuesta latente y variable mani-fiesta ordinal, a través de sus dos componentes: el sub-modelo estructural y el sub-modelo de medición, que son complementados con un modelo logístico multinomial para analizar la probabilidad de pertenencia a una clase latente. El modelo se aplicó a un conjunto de datos pertenecientes al Estudio Nacional de Juventud y Religión (NSYR por las siglas en inglés “National Study of Youth and Religion”), con el fin de encontrar clases latentes en el constructo religiosidad y describir su evolución. Como resultado, se identificaron tres clases latentes con trayectorias distintas para cada caso.",2020-01-16
Pontificia Universidad Católica del Perú. Escuela de Posgrado,El Modelo de Respuesta Nominal: Aplicación a datos educacionales,"Rivera Espejo, José Manuel",Magíster en Estadística,"Tarazona Vargas, Enver Gerald","La presente tesis centra sus esfuerzos en presentar y estudiar el Modelo de Respuesta
Nominal o NRM por sus siglas en inglés (Bock, 1972, 1997), en el contexto de la Teoría de Respuesta al ítem (IRT, por sus siglas en inglés). Se realizaron estudios de simulación para determinar la calidad de la recuperación de los parámetros del modelo, bajo la metodología clásica (MML) y bayesiana (MCMC) y finalmente, se aplicó el modelo estudiado en una muestra anónima, aleatoria y representativa de 1641 docentes de la modalidad de Educación Básica Regular de la especialidad de inglés, que fueron expuestos a la sub-prueba de Compresión Lectora del Concurso de Nombramiento 2015.
En relación a la simulación, encontramos que el método bayesiano es un buen sustituto de su contraparte clásica, debido a que el mismo recupera de manera similarmente satisfactoria los parámetros de los ítems; sin embargo, la principal desventaja es que fue entre 620 a 14100 veces más lento que los métodos clásicos, pese a que se puso especial énfasis en hacer paralelos los procesos MCMC.
En relación a los resultados de la aplicación se tiene que el NRM: (i) facilita la recuperación de una mayor proporción información disponible en los ítems, frente a los modelos de respuestas dicotómicas (Bock, 1972; Thissen, 1976; Levine y Drasgow, 1983; Thissen y Steinberg, 1984), (ii) permite hallar el ordenamiento implícito en datos categóricos inicialmente no ordenados (Samejima, 1988; Bock, 1997) y (iii) brinda información relevante para la valoración de la calidad de un ítem (Thissen et al., 1989), especialmente en dos puntos: (a) les permitía identificar alternativas inservibles o forzadas y (b) les permita identificar alternativas que se podían colapsar, dado que estas alternativas registraban similar temática.",2019-07-17
Pontificia Universidad Católica del Perú. Escuela de Posgrado,El modelo de larga duración Weibull-Geométrica,"Torres Salinas, Karina Hesi",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","Los modelos de larga duración son una extensión de los modelos de supervivencia tradicional
y nos permiten modelar una proporción de la población que no llegan a experimentar
un evento de interés, incluso después de un largo periodo de seguimiento. En este trabajo
se presenta y deduce la distribución de larga duración Weibull-Geométrica y su proceso de estimación e inferencia. Se desarrolló un estudio de simulación con el un de evaluar el desempeño de las estimaciones y determinar si se recuperan los parámetros. Asimismo el modelo fue aplicado a una muestra de clientes que adquirieron y activaron una tarjeta de crédito
entre enero a diciembre del año 2015 y donde el principal objetivo del análisis era entender el
comportamiento del tiempo hasta la cancelación de la tarjeta de crédito del cliente. Comparamos
al modelo de larga duración Weibull-Geométrica con otros modelos de larga duración,
Exponencial-Geométrica y Weibull. Los resultados indican que nuestro modelo muestra un
mejor ajuste en los datos.",2019-03-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo G-DINA aplicado al diagnóstico de desórdenes mentales,"Villena Guzmán, Denisse",Magíster en Estadística,"Tarazona Vargas, Enver Gerald","Actualmente, uno de los modelos de diagnóstico cognitivo (MDC) más usados es el modelo
DINA. Sin embargo, este modelo presenta varias restricciones que hacen que en muchas
ocasiones, no sea el que mejor se ajusta a la realidad. En ese contexto, nace una generalización
del modelo DINA, denominado G-DINA (Generalized deterministic input, noisy and gate).
En el presente estudio se presentan los fundamentos y propiedades del modelo G-DINA y
su aplicación en un área en el que su uso todavía no es muy común: la psicología. Así, se
evaluaron los resultados de una muestra de pacientes de un hospital general de Lima a los
que se les aplicó el test SRQ-18 que evalúa la presencia de desórdenes mentales. Se muestra
el proceso de selección del mejor modelo para cada ítem, los resultados de los parámetros
obtenidos, los diagnósticos para los 10 primeros pacientes y una distribución de los perfiles
de estos pacientes. Finalmente se presenta un estudio de simulación que tiene por finalidad
estudiar el efecto del tamaño de muestra en la estimación de los parámetros en el contexto
de la aplicación de este estudio.",2019-02-11
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Análisis bayesiano de modelos de clases latentes para variables politómicas: Confianza hacia instituciones públicas,"Cruz Sarmiento, Marylía Paola",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","El modelo de análisis de clases latentes tiene como  finalidad describir una variable no observable a través del agrupamiento de los individuos en base a sus patrones de respuestas.
La estimación en este modelo se puede realizar mediante el algoritmo de Esperanza-Maximización (EM) y su desarrollo para el caso politómico se encuentra implementado en el paquete poLCA de R. Desde el punto de vista bayesiano, esta estimación ha sido hasta el momento implementada sólo para el caso de variables dicotómicas. En este trabajo, se busca
extender este  ultimo aporte para el caso politómico, haciendo uso del muestrador de Gibbs.
La aplicación del modelo de análisis de clases latentes, bajo el enfoque bayesiano aquí desarrollado, se realizó sobre un conjunto de datos reales relacionados con la con fianza hacia 21 instituciones públicas en una encuesta para Lima Metropolitana. En general, se identificaron tres grupos de encuestados seg un sus niveles de confianza institucional, los cuales se analizaron luego en relación a otras variables.",2019-02-11
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo secuencial con aplicación a la medición del rendimiento estudiantil,"Mejía Campos, Luis Ángel",Magíster en Estadística,"Tarazona Vargas, Enver Gerald","En este trabajo se presenta el Modelo Secuencial para datos politómicos ordinales de la
teoría de respuesta al ítem y sus características. De forma específi ca se estudia el Modelo
Secuencial Logístico de 2 parámetros (2PL-SM). La estimación de este modelo se realiza
utilizando Métodos de Cadenas de Markov de Montecarlo (MCMC), los cuales fueron implementados
en R y WinBUGS.
Se realizó un estudio de simulación con el objetivo de estudiar la precisión en la recuperación de parámetros observándose resultados apropiados según los índices de precisión
utilizados.
El Modelo Secuencial en estudio fue luego aplicado a la prueba de escritura de la Evaluación Muestral 2013 del Ministerio de Educación, evaluación que fue aplicada a una muestra
de 4327 estudiantes de sexto grado de primaria de todo el país. Con la aplicación del modelo a
la prueba se pudo determinar que en general esta contiene ítems cuyas di ficultades son bajas
y que, para los estudiantes, el enfrentarse a esta prueba no debería resultarles complicado.",2019-02-04
Pontificia Universidad Católica del Perú. Escuela de Posgrado,El modelo de larga duración Exponencial-Poisson,"Gonzales Rodriguez, Julia Elena",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","En esta tesis se introducir y estudiar el modelo de supervivencia de larga duración
Exponencial-Poisson. Este modelo permite estudiar el tiempo hasta la ocurrencia de un evento
de interés cuando se asume que existe una fracción de unidades de la población inmunes a
la ocurrencia de este evento. El modelo descrito en esta tesis es un modelo de mixtura que
usa la distribución Exponencial-Poisson para modelar el tiempo a la ocurrencia del evento
de interés en la sub población suceptible al evento de interés. Además se plantea un modelo
de regresión logística sobre la probabilidad de ser inmune al evento de interés. Se realiza
un estudio de simulación en el cual a través del sesgo porcentual y cobertura se comprobó
la buena performancia del modelo. Finalmente, el modelo es aplicado sobre una muestra de
clientes morosos de una entidad del sistema  financiero Peruano donde el evento de interés es
la cancelación de dicha deuda.",2018-12-03
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo de regresión a la media simplex inflacionada para proporciones,"Chámpac Flores, Juan Carlos",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","El presente trabajo de tesis propone el modelo de regresión a la media simplex inflacionada,
que permite modelar variables aleatorias continuas limitadas en el intervalo cerrado [0; 1]
al considerar un conjunto de ecuaciones de regresión para estimar la media de la respuesta
y los parámetros que modelan las probabilidades de los valores extremos 0 y 1. Asimismo,
se desarrolla un estudio de simulación con el  fin de evaluar si el método propuesto permite
recuperar los parámetros del modelo desde el punto de vista de la estadística clásica. Por
otro lado, se desarrolla la aplicación del modelo para determinar el grado de dolarización
de empresas que registran deudas en el Sistema Financiero, y para evaluar el desempeño del
mismo, se compara contra el modelo de regresión a la media beta inflacionada. Los resultados
muestran un mejor ajuste del modelo propuesto en esta tesis.",2018-11-15
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos de regresión gamma generalizada cero-inflacionada para la media con aplicación a gastos en educación,"Vásquez Beltrán, Aníbal Alcides",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","Cuando los valores posibles de una variable aleatoria son continuos y no negativos, incluyendo
el valor cero con probabilidad no nula, la variable es denominada semicontinua o
cero-in acionada y posiblemente sea pertinente suponer que presenta una distribución mixta
de probabilidades constituida por una distribución de Bernoulli para explicar si la respuesta
toma el valor cero o no y una distribución continua positiva para explicar si ésta última
no es cero. En el análisis de regresión, el modelo de dos partes (MDP) es tradicionalmente
usado para explicar una variable semicontinua. En el MDP la respuesta presenta este tipo
de distribución mixta y sus parámetros son expresados de tal manera que posibilite estimar
el efecto de un conjunto de covariables sobre la media de esta respuesta condicionada a que
tome valores positivos y sobre la probabilidad de que la respuesta tome el valor cero.
El objetivo de la tesis es estudiar un modelo alternativo al MDP, que llamaremos modelo
de regresión cero-in acionada a la media (MCIM), cuya parametrización permita estimar e
interpretar efectos de covariables sobre la media total de la respuesta, en lugar de la media
condicionada a valores positivos. Además, optamos por la distribución gamma generalizada
(MCIM-GG) para modelar ciertas características de los valores positivos de la respuesta, tales
como, por ejemplo, la asimetría positiva y la curtosis pronunciada. Estas características, junto
con el exceso de valores cero, son típicas en diferentes ejemplos de variables respuestas en la
Economía y la Medicina.
Los resultados del estudio de simulación muestran un adecuado desempeño de las estimaciones
de máxima verosimilitud del MCIM-GG bajo diferentes escenarios de nidos según
porcentajes de valores ceros de la respuesta y tamaños de muestra. Por último, los resultados
de la aplicación muestran que el MCIM-GG puede tener un mejor ajuste a los datos respecto
al MDP-GG, así como proporcionar una más directa interpretación de los efectos de ciertas
covariables sobre la media de los gastos en educación de adolescentes participantes del estudio
Niños del Milenio en el Perú.",2018-11-13
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo lineal mixto conjunto de clases latentes aplicado a un conjunto de datos longitudinales del sector salud,"Neciosup Vera, Carmen Stéfany",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","Los modelos lineales mixtos conjuntos de clases latentes, propuestos por Proust-Lima
et al. (2015), permiten modelar de manera conjunta un proceso longitudinal y un proceso
de supervivencia, calculando también la probabilidad de pertenencia a determinadas clases
latentes que puedan existir en la población en estudio. En el presente trabajo se describen
los componentes que conforman este modelo, y mediante un estudio de simulación se evalúa
y analiza la implementación de su estimación. El modelo se aplica finalmente a un conjunto
de datos longitudinales de pacientes diagnosticados con Cáncer de Próstata, permitiéndonos
la identificación de clases latentes que se asocian luego con el estadío clínico de los pacientes.",2018-11-13
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Análisis de influencia bajo inferencia bayesiana en evaluaciones escolares de altas consecuencias,"Christiansen Trujillo, Andrés Guillermo",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","La presente investigación estudia una metodología para la detección de observaciones atípicas mediante un análisis de influencia bajo la perspectiva de la inferencia bayesiana. Se utiliza la medida de phi-divergencia y el estimador de Monte Carlo, derivado de ésta, trabajados previamente por Peng y Dey (1995), para el cálculo de las divergencias Kullback-Leibler, distancia rectilínea y ji-cuadrado. Además, en el presente trabajo se busca realizar este análisis de influencia en evaluaciones de altas consecuencias (evaluaciones cuyos resultados tienen un alto impacto en la vida de los estudiantes o docentes). El estudio de simulación revela que es posible recuperar observaciones previamente distorsionadas como atípicas. Finalmente, se aplica la metodología a una evaluación realizada por el Ministerio de Educación. Esta aplicación revela que la metodología estudiada es capaz de identificar escuelas con resultados no esperados dadas sus condiciones y resultados anteriores.",2018-07-30
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Inferencia bayesiana en el modelo de regresión beta rectangular,"Calderón Pozo, Francisco German",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","Se conoce que el modelo lineal normal no es apropiado para situaciones en la que la variable respuesta es una proporción que solo toma valores en un rango limitado (0; 1), pues, se pueden obtener valores ajustados para la variable de inter es que exceden sus límites inferior y superior.
Ante dicha situación, una propuesta es utilizar la distribución beta ya que es bastante flexible para modelar proporciones. Este modelo de regresión, sin embargo, puede ser influenciado por la presencia de valores atípicos o extremos. Debido a ello, se ha propuesto en la literatura, un modelo de mayor robustez llamado modelo de regresión beta rectangular, el cual permite una mayor incidencia de tales valores.
El objetivo general de la tesis es estudiar las propiedades, estimar y aplicar a un conjunto de datos reales el modelo de regresión beta rectangular desde el punto de vista de la estadística bayesiana.
Para cumplir con el objetivo planteado, se estudian las características y propiedades de las distribuciones beta y beta rectangular. Luego, se desarrolla el análisis bayesiano del modelo de regresión beta rectangular considerando las distribuciones a priori y a posteriori, los criterios de selección de modelos y simulaciones de Montecarlo v  a cadenas de Markov. También, se realizan estudios de simulación para demostrar que el nuevo modelo es m as robusto que el modelo de regresión beta. Adicionalmente, se presenta una aplicación para mostrar la utilidad del modelo de regresión beta rectangular.",2018-05-07
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Estimación bayesiana de efectos de red: el modelo Logit mixto,"Chahuara Vargas, Paulo Roberto",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","Los efectos o externalidades de red son factores que pueden condicionar las decisiones de contratación de los consumidores en favor de empresas ya establecidas y en contra de los nuevos competidores, pudiendo limitar la competencia efectiva y potencial de los mercados, en especial, en aquellas industrias donde el número de empresas es bajo y la entrada de nuevos competidores es poco frecuente. Por ello, es importante verificar su existencia y la magnitud de sus efectos sobre las decisiones de compra de los consumidores con el objetivo de justificar o establecer medidas que impulsen una competencia más equilibrada entre las empresas. Además, teniendo en consideración que los consumidores pueden tener cierto grado de heterogeneidad en sus comportamientos de adquisición, también resulta relevante estudiar el grado de diferenciación de los efectos de red entre los consumidores a fin de mejorar las políticas que fomenten la competencia. Este trabajo tiene por objetivo estimar un modelo logit mixto bajo el enfoque de la inferencia bayesiana, para estudiar empíricamente la existencia y heterogeneidad de los efectos de red sobre las decisiones de contratación de los consumidores en la industria de telefonía móvil peruana. El análisis se hace con base a una muestra que combina información de la Encuestas Residencial de Servicios de Telecomunicaciones (ERESTEL) del a˜no 2015 e información de las empresas operadoras del servicio de telefonía móvil. Los resultados de las estimaciones realizadas sugieren que los efectos de red tendrían un condicionamiento importante sobre las decisiones de contración del servicio de telefonía móvil, además de presentar un grado de heterogeneidad estadísticamente significativo en la magnitud de sus efectos.",2017-10-02
Pontificia Universidad Católica del Perú. Escuela de Posgrado,A beta inflated mean regression model with mixed effects for fractional response variables,"Fernández Villegas, Renzo",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","In this article we propose a new mixed effects regression model for fractional bounded response variables. Our model allows us to incorporate covariates directly to the expected
value, so we can quantify exactly the influence of these covariates in the mean of the variable of interest rather than to the conditional mean. Estimation is carried out from a Bayesian perspective and due to the complexity of the augmented posterior distribution we use a Hamiltonian Monte Carlo algorithm, the No-U-Turn sampler, implemented using Stan software. A simulation study for comparison, in terms of bias and RMSE, was performed showing that our model has a better performance than other traditional longitudinal models for bounded variables. Finally, we applied our Beta Inflated mixed-effects regression model to real data which consists of utilization of credit lines in the peruvian financial system.",2017-06-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo Dina aplicado a la evaluación de matemática en estudiantes de segundo grado de secundaria,"Sosa Paredes, Yuriko Kirilovna",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","Los modelos de diagnóstico cognitivo (MDC) tienen como finalidad describir o diagnosticar el comportamiento de los evaluados por medio de clases o perfiles latentes, de tal manera que se obtenga información más específica acerca de las fortalezas y debilidades de ellos.

Uno de los modelos más populares de esta gran familia es el llamado modelo DINA, el cual tuvo su primera aparición en Haertel (1989) enfocado principalmente en el campo educacional. Este modelo considera solo respuestas observadas dicotómicas de parte de los individuos y tiene como restricción principal que ellos deben dominar necesariamente todas las habilidades requeridas por cada ítem; aquellas que se resumen en una matriz llamada Q. Asimismo, el modelo estima parámetros para los ítems, los cuales son denominados de \ruido"": Adivinación y Desliz.

En este trabajo desarrolla teóricamente el modelo expuesto; es decir, sus fundamentos y principales propiedades desde el enfoque bayesiano. Específicamente, las estimaciones se realizan mediante el Muestreador de Gibbs.

Se realizaron 8 estudios de simulación, cada uno de ellos con tres diferentes tamaños de población, donde se probaron combinaciones de los parámetros en estudio con el fin de comparar la recuperación de parámetros mediante el enfoque clásico y el bayesiano. El análisis de ambos enfoques se realizó con rutinas de código del software libre R, usando los paquetes CDM y dina para el enfoque clásico y el bayesiano, respectivamente.

En líneas generales, los resultados muestran estimaciones insesgadas y con valores pequeños de la raíz del error cuadrático medio (RMSE) para ambos enfoques. Incluso, conforme el tamaño de la población incrementa, las estimaciones no tienen mayores diferencias. Aunque en tamaños de población más pequeños el enfoque bayesiano obtiene ligeras ventajas con respecto al otro, especialmente en el parámetro de probabilidad de pertenencia a las clases (π). Además, es necesario mencionar que los parámetros de ruido de los ítems son estimados más precisamente con el enfoque clásico en varios de los estudios. 

Finalmente, se presenta una aplicación enfocada en educación, donde se analiza una muestra de 3040 alumnos del 2do grado de secundaria, evaluados en una prueba de 48 ítems de la competencia matemática realizada por la Oficina de Medición de la Calidad de los Aprendizajes (UMC) en el 2015. A esta prueba se le aplica el modelo de Rasch y el modelo DINA bajo el enfoque bayesiano, con el _n de estudiar la correspondencia entre indicadores de ambos modelos, tanto para los parámetros de los alumnos (habilidad y per_les latentes) como de los ítems (dificultad y parámetros de ruido).",2017-05-31
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Estimation of the disease prevalence when diagnostic tests are subject to classification error: bayesian approach,"Gutiérrez Ayala, Evelyn Patricia",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","La estimación de la prevalencia de una enfermedad, la cual es definida como el número de casos con la enfermedad en una población dividida por el número de elementos en ésta, es realizado con gran precisión cuando existen pruebas 100% exactas, también llamadas gold standard. Sin embargo, en muchos casos, debido a los altos costos de las pruebas de diagnóstico o limitaciones de tecnología, la prueba gold standard no existe y debe ser reemplazada por una o más pruebas diagnósticas no tan caras pero con bajos niveles de sensibilidad o especificidad. Este estudio está enfocado en el estudio de dos enfoques bayesianos para la estimación de prevalencia cuando no es factible tener resultados de una prueba 100% exacta.
El primero es un modelo con dos parámetros que toman en cuenta la asociación entre los
resultados de las pruebas. El segundo es un enfoque que propone el uso del Bayesian Model Averaging para combinar los resultados de cuatro modelos donde cada uno de estos tiene suposiciones diferentes sobre la asociación entre los resultados de las pruebas diagnósticas.
Ambos enfoques son estudiados mediante simulaciones para evaluar el desempeño de estos bajo diferentes escenarios. Finalmente estas técnicas serán usadas para estimar la prevalencia de enfermedad renal crónica en el Perú con datos de un estudio de cohortes de CRONICAS (Francis et al., 2015).",2017-02-02
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos de teoría de respuesta al ítem multidimensional con una aplicación psicológica,"Malaspina Quevedo, Martín Ludgardo",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","La presente investigación, dentro del contexto de la Teoría de Respuesta al Ítem (TRI), estudia un modelo multidimensional logístico compensatorio de dos parámetros (M2PL) para ítems dicotómicos. Para ello, se explican teóricamente los métodos de estimación más conocidos para los parámetros de los ítems y de los rasgos latentes de las personas, priorizando el método bayesiano mediante Cadenas de Markov de Monte Carlo (MCMC). Estos métodos de estimación se exploran mediante implementaciones computacionales con el software R y R2WinBUGS. La calidad de las respectivas estimaciones de los parámetros se analiza mediante un estudio de simulación, en el cual se comprueba que el método de estimación más robusto para el modelo propuesto es el bayesiano mediante MCMC. Finalmente, el modelo y el método de estimación elegidos se ilustran mediante una aplicación que usa un conjunto de datos sobre actitudes hacia la estadística en estudiantes de una universidad privada de Colombia.",2016-11-23
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Combinación de reglas de portafolio con la asignación 1/N y la ponderada por capitalización bursátil,"Rodríguez Alcócer, Augusto Fernando",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","La teoría del portafolio estudia el proceso a través del cual se realiza la asignación óptima de activos. El análisis Media - Varianza (MV) propone que los agentes estructuran portafolios de inversión optimizando el retorno esperado o el riesgo. Así, fijando el nivel deseado de una de estas variables, es posible elaborar una frontera eficiente compuesta por portafolios óptimos. Sin embargo, si bien el análisis MV ha sido trabajado de manera extensa presenta una limitación: los parámetros reales no son conocidos sino estimados a partir de la observación de datos. Ello incorpora el problema de incertidumbre en la modelación, por lo que las reglas de portafolio óptimo están sujetas a errores aleatorios que pueden generar que los parámetros estimados se alejen de los reales. El objetivo del presente trabajo es revisar dicho análisis bajo el enfoque de reglas de portafolio, y si existe la posibilidad de reducir el riesgo de estimación a través de la combinación de las reglas con el portafolio de pesos iguales y con el portafolio ajustado por capitalización bursátil. Para la programación se utiliza el paquete estadístico R - project. Los resultados sugieren que la combinación de las reglas con los dos portafolios seleccionados puede mejorar los resultados fuera de muestra esperados y que bajo ciertas circunstancias, combinar con el portafolio de capitalización bursátil puede ser más eficiente que con el portafolio de pesos iguales.",2016-11-23
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Estudio de tres propuestas de distribución skew-t,"Kantor Benavides, Alejandro",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","Este trabajo compara tres distribuciones skew-t. En particular, las propuestas por Branco y Dey (2001) y Azzalini y Capitanio (2003), Fernández y Steel (1998), y Jones y Faddy (2003).
Se analiza la relación entre los parámetros y el nivel de asimetría a través de la medida de Patil et al. (2014). Se propone una nueva parametrización de la distribución skew-t de Jones y Faddy (2003) que modela mejor la asimetría. Las distribuciones son ajustadas a datos reales basados en el retorno logarítmico de la tasa de cambio de PEN a USD.",2016-06-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo de regresión de clases latentes: factores asociados a la valoración de una universidad privada,"Wiener Ramos, Lucia",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","En diversos campos de análisis, especialmente en las ciencias sociales y humanas, se identifican constructos teóricos a los cuales queremos aproximarnos pero que no son directamente observables ni medibles, como por ejemplo, la calidad o satisfacción con un servicio, el nivel de  estrés, el nivel de conocimiento en matemáticas, entre otras. Este tipo de constructos son llamados variables latentes y su valor solo puede ser aproximado a través de variables observadas o manifiestas que si pueden ser medidas (Bartholomew et al., 2011).
En el Capítulo 2 se presenta consideraciones generales acerca del modelo lineal general de variables latentes y el modelo de clases latentes. En el Capítulo 3 se estudian los modelos de regresión de clases latentes, la estimación de sus parámetros y su implementación computacional. En el Capítulo 4 se presenta los resultados de la aplicación del modelo a un conjunto de datos reales orientados a conocer la valoración de una universidad privada. En el Capítulo 5 se presenta algunas conclusiones, recomendaciones y futuras extensiones que se podrían derivar de este trabajo.",2016-06-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,An application of discrete time survival models to analyze student dropouts at a private university in Peru,"Pebes Trujillo, Miguel Raúl",Magíster en Estadística,"Sal y Rosas Celi, Víctor Giancarlo","Discrete-time survival models are discussed and applied to the study of which factors
are associated with student dropouts at a private university in Lima, Per_u. We studied the characteristics of 26; 790 incoming students enrolled between 2004 and 2012 in all the under-graduate programs at the University. The analysis include the estimation of the survival and hazard functions using the Kaplan-Meier method and the _tting of parametric models using the Cox proportional hazards regression and the Logistic regression for survival analysis, this last one, in order to include time varying variables as predictors. During the period of analysis, the cumulative probability of remain at the University after _ve years was 73.7% [95% CI: 73.1% - 74.4%]. In any period the hazard is greater than 4.4% and this highest value is reached in the 3rd semester. In a multivariate analysis, we found that academic factors (area of study, type of admission, standardized academic performance index, and the percentage of passed credits); economic factors (type of residence, and payment scale); and sociodemographic factors (mother education level, indicators of whether or not parents are alive, and the age of the student) were associated with the risk of dropout.",2016-06-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Una aplicación de la regresión de Cox con puntos de cambio en las covariables,"Trujillo Angeles, Lucía Inés",Magíster en Estadística,"Doig Camino, Elizabeth","El siguiente trabajo de tesis, estudiará el modelo de regresión de Cox con puntos de cambio en las covariables propuesto por Jensen y Lutkebohmert (2008), realizando el desarrollo y la aplicación para una base de líneas móviles postpago. El objetivo es obtener los parámetros de las covariables y el nuevo parámetro en el modelo que es el punto de cambio, para analizar la manera como estas covariables tienen influencia en la desactivación de una línea a solicitud del cliente.",2016-06-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos Chain Ladder estocásticos y aplicaciones al cálculo de reservas en compañías de seguros,"Mazuelos Vizcarra, Gisella Gabriela",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","This document is intented to deepen the study of univariate and multivariate Chain Ladder
methods for estimating reserves in an insurance company. It presents from a theoretical
and applicative perspective both the univariate deterministic and stochastic Chain Ladder
methods. Although, the first is the most used method by insurance companies due to its simplicity
and lack of probabilistic assumptions, the second, proposed by Mack (1993), allows
the construction of confidence intervals for the estimated reserves, which is invaluable for
researchers.
We also develop the General Multivariate Chain Ladder model, which has the basic
premise to analyze the possible relationship that may exist between different development
triangles, thus providing another tool to improve inferences and predictions of reserves.
These methods have been developed and applied to a database of 3 types of health insurance,
thus showing the advantages and disadvantages of each of them in different scenarios
and providing various tools for decision making in meeting the future obligations of insurance
companies.",2015-07-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos alternativos de respuesta graduada con aplicaciones en la calidad de servicios,"Tarazona Vargas, Enver Gerald",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","Los modelos politómicos de la Teoría de Respuesta al Ítem (TRIP) tienen como finalidad explicar la interacción existente entre los sujetos evaluados y los atributos de un test en aquellas situaciones en las cuales los atributos que lo componen tienen varias categorías de respuesta. Dentro de los distintos tipos de modelos TRIP, el Modelo de Respuesta Graduada General (GRM) propuesto originalmente por Samejima (1969, 2010), es un conjunto de modelos diseñados para aplicarse en aquellas situaciones en las cuales las categorías de respuesta son ordinales.
En este trabajo se presenta una formulación general para los GRM, su clasificación y
principales propiedades desde el punto de vista bayesiano. De manera específica, se muestra el Modelo de Respuesta Graduada Logístico de dos parámetros (2PL-GRM) como un caso particular de los GRM simétricos y el Modelo de Respuesta Graduada Logístico de Exponente Positivo (LPE-GRM) como un modelo asimétrico derivado de incorporar un parámetro de penalización que controla la curvatura de las Funciones de Respuesta a las Etapas de los Ítems (FREI). La estimación de ambos modelos fue realizada usando la inferencia bayesiana con Métodos Montecarlo vía Cadenas de Markov (MCMC) e implementada en R y WinBUGS.
Se realizó un estudio de simulación con el _n de estudiar la precisión en la recuperación de parámetros para el Modelo 2PL-GRM obteniéndose resultados apropiados para las medidas
de ajuste consideradas.
Los modelos 2PL-GRM y LPE-GRM estudiados fueron aplicados al estudio de un cuestionario acerca de la satisfacción de clientes y comparados con el tradicional análisis clásico de los test. La muestra del estudio está formada por 5354 clientes de una empresa de telecomunicaciones que se comunicaron con el Call Center de atención al cliente por algún motivo (consulta, reclamo, pedido, etc.). A través del análisis de dimensionalidad de la escala se encontró que el cuestionario evalúa dos dimensiones de la satisfacción con la atención al cliente: la Accesibilidad (4 ítems) y el Desempeño del asesor (7 ítems). Los resultados indican, considerando diferentes criterios, que en ambas dimensiones el modelo LPE-GRM es mejor.
Adicionalmente, ambos modelos ofrecen mejor información que el tradicional análisis clásico.
Se sugiere realizar diferentes estudios de simulación para evaluar distintas condiciones para la inferencia del modelo LPE-GRM puesto que para las mismas condiciones de estimación MCMC se observa que puede ser más demorado debido a que presenta mayor autocorrelación que el modelo 2PL-GRM.",2015-07-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Inferencia bayesiana en un modelo de regresión cuantílica semiparamétrico,"Agurto Mejía, Hugo Miguel",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","Este trabajo propone un Modelo de Regresión Cuantílica Semiparamétrico. Nosotros empleamos la metodología sugerida por Crainiceanu et al. (2005) para un modelo semiparamétrico en el contexto de un modelo de regresión cuantílica. Un enfoque de inferencia Bayesiana es adoptado usando Algoritmos de Montecarlo vía Cadenas de Markov (MCMC).
Se obtuvieron formas cerradas para las distribuciones condicionales completas y así el algoritmo muestrador de Gibbs pudo ser fácilmente implementado. Un Estudio de Simulación es llevado a cabo para ilustrar el enfoque Bayesiano para estimar los parámetros del modelo. El modelo desarrollado es ilustrado usando conjuntos de datos reales.",2015-07-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Portafolios óptimos bajo estimadores robustos clásicos y bayesianos con aplicaciones al mercado peruano de acciones,"Vera Chipoco, Alberto Manuel",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","El Modelo del Portafolio, propuesto por Markowitz (1952), es uno de los más importantes
en el ámbito  nanciero. En él, un agente busca lograr un nivel óptimo de sus inversiones
considerando el nivel de riesgo y rentabilidad de un portafolio, conformado por un conjunto de acciones bursátiles.
En este trabajo se propone una extensión a la estimación clásica del riesgo en el Modelo del Portafolio usando Estimadores Robustos tales como los obtenidos por los métodos del Elipsoide de Volumen mínimo, el Determinante de Covarianza Mínima, el Estimador Ortogonalizado de Gnanadesikan y Kettenring, el Estimador con base en la matriz de Covarianzas de la distribución t-student Multivariada y la Inferencia Bayesiana. En este último caso se hace uso de los modelos Normal Multivariado y t-student multivariado. En todos los modelos descritos se evalúa el impacto económico y las bondades estadísticas que se logran si se usaran estas técnicas en el Portafolio del inversionista en lugar de la estimación clásica. Para esto se utilizarán activos de la Bolsa de Valores de Lima.",2015-07-20
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Una aplicación de intervalos de confianza para la mediana de supervivencia en el modelo de regresión de Cox,"Mondragón Arbocco, Jorge Adolfo",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","El presente trabajo estudiará el método propuesto por Tze y Zheng (2006) aplicándolo a la obtención de intervalos de confianza para la mediana de supervivencia de líneas móviles de una empresa de telecomunicaciones. Esta metodología se aplicará con el objeto de conocer el riesgo de vida promedio de la línea móvil así como de qué manera inciden las covariables sobre el tiempo hasta el incumplimiento del pago de los clientes de la empresa.
Para ello se hará uso de una extensión del modelo de Cox haciendo uso de la estimación máximo verosímil para obtener nuevas estimaciones del vector de parámetros mediante el método bootstrap lo que permita la construcción de los intervalos de confianza para la mediana de supervivencia.",2015-07-17
Pontificia Universidad Católica del Perú. Escuela de Posgrado,An empirical application of stochastic volatility models to Latin-American stock returns using GH skew student's t-distribution,"Lengua Lafosse, Patricia",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","This paper represents empirical studies of stochastic volatility (SV) models for daily stocks returns data of a set of Latin American countries (Argentina, Brazil, Chile, Mexico and Peru) for the sample period 1996:01-2013:12. We estimate SV models incorporating both leverage effects and skewed heavy-tailed disturbances taking into account the GH Skew Student’s t-distribution using the Bayesian estimation method proposed by Nakajima and Omori (2012). 
A model comparison between the competing SV models with symmetric Student´s t-disturbances is provided using the log marginal likelihoods in the empirical study. A prior sensitivity analysis is also provided. The results suggest that there are leverage effects in all indices considered but there is not enough evidence for Peru, and skewed heavy-tailed disturbances is confirmed only for Argentina, symmetric heavy-tailed disturbances for Mexico, Brazil and Chile, and symmetric Normal disturbances for Peru. Furthermore, we find that the GH Skew Student s t-disturbance distribution in the SV model is successful in describing the distribution of the daily stock return data for Peru, Argentina and Brazil over the traditional symmetric Student´s t-disturbance distribution.",2015-07-17
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Un enfoque de credibilidad bajo espacios de Hilbert y su estimación mediante modelos lineales mixtos,"Ruíz Arias, Raúl Alberto",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","La teoría de la credibilidad provee un conjunto de métodos que permiten a una compañía de seguros ajustar las primas futuras, sobre la base de la experiencia pasada individual e información de toda la  cartera. En este trabajo presentaremos los principales modelos de credibilidad utilizados en la práctica, como lo son los  modelos de Bühlmann (1967), Bühlmann-Straub (1970), Jewell (1975) y Hachemeister (1975), todos ellos analizados en  sus propiedades desde un punto de vista geométrico a través de la teoría de espacios de Hilbert y en su estimación mediante el uso de los modelos lineales mixtos. Mediante un estudio de simulación se mostrará la ventaja de utilizar este último enfoque de estimación.",2013-04-08
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Inferencia bayesiana en el modelo de regresión spline penalizado con una aplicación a los tiempos en cola de una agencia bancaria,"Huaraz Zuloaga, Diego Eduardo",Magíster en Estadística,"Bayes Rodríguez, Cristian Luis","En diversos campos de aplicación se requiere utilizar modelos de regresión para analizar la relación entre dos variables. Cuando esta relación es compleja, es difícil modelar los datos usando técnicas paramétricas tradicionales, por lo que estos casos requieren de la flexibilidad de los modelos no paramétricos para ajustar los datos. Entre los diferentes modelos no paramétricos está la regresión spline penalizada, que puede ser formulada dentro de un marco de modelos lineales mixtos. De este modo, los programas computacionales desarrollados originalmente para la inferencia clásica y Bayesiana de modelos mixtos pueden ser utilizados para estimarlo.
La presente tesis se centra en el estudio de la inferencia Bayesiana en el modelo de regresión spline penalizado. Para lograr esto, este trabajo proporciona un marco teórico breve de este modelo semiparamétrico y su relación con el modelo lineal mixto, la inferencia Bayesiana de este modelo, y un estudio de simulación donde se comparan la inferencia clásica y Bayesiana en diferentes escenarios considerando diversos valores del n umero de nodos, tamaños de muestra y niveles de dispersión en la data simulada. Finalmente, en base a los resultados del estudio de simulación, el modelo se aplica para estimar el tiempo de espera en cola de los clientes en agencias bancarias con el fin de calcular la capacidad de personal óptima bajo determinadas metas de nivel de servicio.",2013-04-08
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Análisis de votos electorales usando modelos de regresión para datos de conteo,"Contreras Vilca, Norma",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","Se presentan dos modelos de regresión para datos de conteo: el modelo de regresión
Poisson y modelo de regresión Binomial Negativa dentro del marco de los Modelos Lineales Generalizados. Los modelos son aplicados inicialmente a un conjunto de datos conocido como ((The Aircraft Damage)) presentado en Montgomery (2006) referido al número de daños en las aeronaves
durante la guerra de Vietnam.
La principal aplicación de este trabajo sería el análisis de los votos obtenidos por el candidato
Ollanta Humala Tasso en los resultados de las ((Elecciones Generales y Parlamento Andino
2011)), analizamos los datos de la primera vuelta a nivel de regiones considerando diversos
predictores. Ambos conjunto de datos, presentan sobredispersión, esto es una varianza mayor que la media, bajo estas condiciones el modelo de Regresión Binomial Negativa resulta m as adecuado que
el modelo de Regresión Poisson.
Adicionalmente, se realizaron estudios de diagnósticos que confirman la elección del modelo
Binomial Negativa como el más apropiado para estos datos.",2013-04-08
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos de regresión binaria Skew probit para el calculo de probabilidad de default en el ámbito del sistema financiero,"Pantoja Marin, Luis",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","La presente investigación se fundamenta en el uso o aplicación de Modelos Skew Probit con enlace asimétrico desde un enfoque Bayesiano. Los modelos a usar incorporan la posibilidad de usar enlaces asimétricos para estimar la probabilidad de y i =1 en muestras no balanceadas (alta proporción de ceros y por ende pequeña proporción de unos). La formulación general de esto modelos es debida a Bazán, Bolfarine y Branco (2010).
Aunque en estos modelos inicialmente su computación es complicada se usaron Cadenas de Markov por Monte Carlo (MCMC) o muestreo Gibbs (para la aplicación de estos procedimientos ver Carlin y Polson, 1992) que hacen simple la formulación del modelo y por tanto simple su implementación usando el software WinBugs (los códigos de los diferentes modelos utilizados fueron obtenidos en el programa BRMUW propuesto por Bazán y Bayes, 2010).
De acuerdo al análisis y estudio de aplicación realizado sobre una muestra de clientes de préstamos pertenecientes a una entidad micro financiera, aquellos modelos Skew Probit BBB y Estándar presentan los mejores indicadores de eficiencia.
El análisis sobre datos reales señala que el modelo tradicional Probit presenta un 56.6% (371/664) de mala clasificación versus los modelos Estándar y BBB que en promedio muestran dicho indicador alrededor de 43% (290/664).
El análisis mediante curvas COR (Receiver Operating Characteristic) ratifica lo mencionado; el área debajo de las curvas superan el 0.74 de 1 para el modelo BBB, mientras que dicho dato es de 0.70 para el caso del modelo simétrico tradicional probit. Por tanto la sensibilidad y especificidad (eficiencia) es mayor para aquellos modelos Skew Probit (mejor modelo BBB).
Dentro de los modelos con Enlaces Asimétricos los modelos (SP) BBB y Estándar son los que presentan mejores indicadores de ajuste e información as__ como mejoran la sensibilidad y especificidad de un determinado modelo. Finalmente, se pretende la sistematización de la propuesta a nivel de la entidad micro financiera y su aplicación en la estimación de la probabilidad de default de créditos pero aplicado en todos los tipos de créditos.",2013-02-05
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelo de Rasch dicotómico con aplicación a la educación,"Chincaro Del Coral, Omar Antonio",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","En investigaciones de origen cuantitativo generalmente se emplean instrumentos de medición que generan base de datos dicotómicas, en la cual cada persona responde las preguntas o ítems del instrumento. Subyacente a estas respuestas existen variables no observables o variables latentes que caracterizan a las personas evaluadas y a los ítems del instrumento de medición utilizado. En este trabajo se modeló la probabilidad de responder correctamente al ítem en función a sus parámetros mediante el uso de los modelos logísticos o modelos de Rasch. Considerando las respuestas a estas variables latentes de las personas, de lo ítems, y sus supuestos se estimó los parámetros a partir de la función de verosimilitud del modelo.

En esta tesis se mostró diferentes métodos de estimación como el de Máxima Verosimilitud Marginal (MVM) que depende de las puntuaciones que se obtenga en cada ítem, el de Máxima Verosimilitud Condicional (MVCOND) que considerara patrones de respuesta, el método de Máxima Verosimilitud Conjunta (MVC) y el método Bayesiano utilizando Cadenas de Markov y métodos de Monte Carlo (MCMC) como el algoritmo Gibbs Sampling. El Método Bayesiano fué analizado bajo dos esquemas: adaptative rejection sampling (ARS) y el data argumentation gibbs sampling (DAGS). Con estos métodos se estimaron los parámetros de los ítems y las personas evaluadas, los cuales se compararon con estudios de simulación determinándose que el mejor método de estimación es el Bayesiano. El método bayesiano presenta las estimativas más precisas considerando diferentes escenarios de tamaño de muestra y número de ítems frente a los otros métodos de estimación. Adicionalmente no tiene restricción en la estimación frente a valores extremos y finalmente es un método conjunto que estima al mismo tiempo habilidades y dificultades a diferencia de otros métodos que sólo estiman dificultades u otros que estiman ambos pero baja precisión. Finalmente se realizó una aplicación del modelo en el ámbito educacional.",2012-08-17
Pontificia Universidad Católica del Perú. Escuela de Posgrado,"Estimación no paramétrica en un proceso de Markov ""enfermedad-muerte"" aplicado a una base de clientes de una AFP","Requena Espinoza, Genaro",Magíster en Estadística,"Doig Camino, Elizabeth","En el presente trabajo, se estudian las propiedades del método de estimación no paramétrico en un modelo de “Enfermedad - Muerte"" de proceso de Markov. Este modelo posee tres estados 1, 2 y 3 correspondientes a “salud"", “enfermedad"" y “muerte"" respectivamente y solo admite las transiciones de 1-2, 1-3 y 2-3, asimismo a este proceso se le denomina de Markov porque la probabilidad de transición de un estado a otro es independiente del tiempo de permanencia en el estado inicial.
Las funciones de tiempo de muerte y enfermedad, así como la función de riesgo de muerte dada la enfermedad son los parámetros del modelo \Enfermedad - Muerte"". Sin embargo la estimación de estas funciones del modelo no es directa pues existen dos formas de censura en los datos: los intervalos censurados y la pérdida de estados de transición; por lo que se utiliza un algoritmo de autoconsistencia para calcular estos estimadores.
Los intervalos censurados y la pérdida de estados de transición se generan porque los pacientes son evaluados periódicamente. En un intervalo censurado (t1 , t2) se conoce que la enfermedad ocurrió entre un tiempo t1 y t2 pero no el momento exacto, mientras que para la pérdida de estados de transición se sabe que la enfermedad no ha ocurrido hasta la última medición pero se desconoce si la enfermedad ocurre entre esta última medición y el tiempo final del estudio.
En la aplicación del modelo \Enfermedad - Muerte"" de proceso de Markov a una base de clientes de una administradora de fondos de pensiones (AFP) se consideran los intervalos censurados para los reclamos de los clientes, as__ como la pérdida de estados de transición para los traspasos. Modelar los tiempos de traspaso y de reclamo de los afiliados bajo un proceso de Markov \Enfermedad - Muerte"" con intervalos censurados y pérdida de estados de transición intermedia, aumenta la precisión de los estimadores de las funciones de tiempo y riesgo.",2012-08-16
Pontificia Universidad Católica del Perú. Escuela de Posgrado,Modelos testlet logísticos y logísticos de exponente positivo para pruebas de compresión de textos,"Flores Ari, Sandra Elizabeth",Magíster en Estadística,"Bazán Guzmán, Jorge Luis","Los modelos de Teoría de Respuesta al Item (TRI) para datos binarios multivariados,
permiten estimar una medida latente (de habilidad) a partir de información observada, que puede ser respuestas dicotómicas (de éxito y fracaso) a un conjunto de ítems de una determinada prueba. Uno de los supuestos críticos en los modelos TRI es la independencia condicional de los ítems, que permite el cálculo directo de la verosimilitud del modelo. En muchas situaciones de evaluación este supuesto no se cumple, como es el caso de pruebas de comprensión de textos, en la que se presenta un texto y luego varias preguntas relacionadas con ese texto. Este tipo de estructuras son denominadas como testlets. Bradlow et al. (1999) desarrollaron una parametrización adicional para recoger el efecto de esta dependencia. A partir de este trabajo se presenta el modelo Testlet logístico y se propone el modelo Testlet logístico de exponente positivo (2LPET), que es una extensión del modelo LPE propuesto por Samejima (1999) y Bazan y Bolfarine (2010) y considera enlaces asimétricos.
Se desarrollaron varios estudios de simulación en los que se muestra que cuando se tiene testlets, los modelos Testlet recuperan mejor los parámetros respecto a los modelos TRI.
Finalmente se realizó una aplicación con datos del Ministerio de Educación, específicamente con los resultados de la prueba de comprensión de textos de la Evaluación Censal de Estudiantes (ECE) dirigido a estudiantes de segundo grado de primaria, en un conjunto de escuelas de Lima metropolitana. De los resultados obtenidos se concluye que los modelos TRI sobreestiman la medida de habilidad respecto a los modelos Testlets y además la información de la prueba es mejor distribuida por el modelo propuesto.",2012-08-16
Pontificia Universidad Católica del Perú. Escuela de Posgrado,El análisis de correspondencias conjunto y múltiple ajustado,"Saavedra López, Ricardo Elías",Magíster en Estadística,"Valdivieso Serrano, Luis Hilmar","Esta tesis presenta una revisión de los fundamentos teóricos de dos de las más recientes
extensiones de la técnica estadística conocida como análisis de correspondencia (AC): el análisis de correspondencia conjunto (ACC) y el análisis de correspondencia múltiple ajustado (ACMA); y muestra una aplicación práctica de éstas a una encuesta de egresados de la Pontificia Universidad Católica del Perú.
El análisis de correspondencia simple (ACS) es el primer alcance del análisis de correspondencias y se presenta cuando cada categoría de una variable se describe en función de la dependencia existente de los valores de otra única variable. Su extensión a más de 2 variables es conocida como el análisis de correspondencia múltiple (ACM).
Si bien se puede encontrar literatura sobre el ACS y el ACM, es importante destacar que
el ACC y el ACMA han sido poco difundidos, encontrándose escasa literatura sobre el tema, más aún, en nuestro idioma. Por lo tanto, se hace necesaria una revisión de las dos primeras a modo de contexto y una presentación metodológica y detallada de las dos últimas.
Con la aplicación práctica se pretende obtener una representación de las facultades de los egresados de la PUCP en función del ingreso en su primer empleo relacionado con la formación recibida en la universidad y la percepción del grado de desarrollo de la competencia de comunicación recibida en la universidad. Esta aplicación consistiría en aplicar los 4 métodos descritos, comparándolos mediante nuevas técnicas que permiten reproducir las tablas de contingencia originales a partir de las representaciones obtenidas por los métodos indicados.",2012-08-15
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Predicción del siniestro de vehículos particulares en una compañía de seguros,"Ramirez Navarro, Vanessa Judith",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","Tras el incremento de siniestros vehiculares de los últimos años en los clientes de una empresa aseguradora peruana, se decidió desarrollar diversos modelos estadísticos que permitan identificar aquellos vehículos que generen un siniestro a futuro y cuáles son los factores relevantes asociados a la siniestralidad. Inicialmente se muestra el proceso de tarificación con el cual la empresa comenzó a ofertar el producto de seguro vehicular para luego ser reemplazado con el modelo estadístico escogido y utilizarlo como factor principal en la definición de precios de las primas. En el presente trabajo de suficiencia profesional se muestra las fases de desarrollo de los modelos Logístico y Random Forest, así como el uso de la matriz de confusión para evaluar las métricas de sensibilidad y especificidad, con la finalidad de elegir el modelo que presente la mejor predicción con respecto a la ocurrencia del siniestro. Para la implementación de los modelos se muestra la metodología Cross Industry Standard Process for Data Mining, el cual sirve para asegurar la planificación y cumplimiento de las fases establecidas en los proyectos analíticos. También se muestra los factores relevantes que se incluyeron en los modelos y presentaron una asociación a la variable respuesta, como información demográfica, financiera y de manejo del propietario; y por parte del vehículo, información de sus características como valor comercial, asientos, tipo de vehículo, colore, entre otros. Finalmente se presenta la implementación de los modelos en el negocio y el impacto positivo en los resultados de distintas frentes de la empresa aseguradora. Las herramientas utilizadas para la preparación, construcción y despliegue de los modelos fueron en la plataforma de Google Cloud Platform con el software Python.",2022-12-30
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de clientes en campañas para una entidad financiera usando el método Stacking,"Alvarez Chancasanampa, Julio César",Ingeniero Estadístico Informático,"Miranda Villagómez, Clodomiro Fernando","La presente investigación, tiene como objetivo general determinar si el método de ensamble Stacking predice con mayor precisión a los clientes potenciales a quienes se les otorgará o desembolsará préstamos en las ofertas de campaña de una entidad financiera, que los algoritmos de aprendizaje supervisado de Machine Learning: Random Forest, Regresión Logística y Árbol de Decisión. La evaluación se realizó comparando el método Stacking con los modelos individuales de Regresión Logística, Árbol de Decisión y Random Forest. Para dicha evaluación se usaron los indicadores Auc, Gini, Logloss y Kolmogorov. Los resultados de sensibilidad en orden de importancia que se obtuvieron con los modelos estadísticos fueron lo siguiente: Regresión Logística 88.9%, seguido del método Stacking con 87.9%, luego el Árbol de Decisión con un 84% y por último Random Forest con un 82.7%. Mientras, que al evaluar la especificidad el de mayor importancia fue el modelo de Random Forest con un 84.8%, Árbol de Decisión 82.8%, método Stacking 81.6% y por último Regresión Logística 78.4%. Respecto a los indicadores evaluados, el que presentó mayor Auc es el método Stacking 0.9117, seguido de Random Forest con un 0.9074, la Regresión Logística reportó un 0.9064 y Árbol de Decisión 0.9074. Con respecto al indicador Gini, el que tiene mayor Gini es el método de Stacking con 0.8235, seguido de Random Forest con un 0.8148, la Regresión Logística cuyo resultado fue de 0.8128 y en última posición el Árbol de Decisión con 0.7885. Con relación al indicador Logloss, el que mostró mejor desempeño fue el método Stacking con 0.3177, seguido de Random Forest con 0.3435, Árbol de Decisión 0.3886 y Regresión Logística 0.3959. Finalmente, con respecto al indicador Kolmogorov, el que tiene mejor resultado es el método Stacking con 0.7124, seguido por Random Forest con 0.7028, Árbol de Decisión 0.6907 y por último la Regresión Logística con 0.6751.",2022-11-04
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Implementación de una solución de business intelligence para la toma de decisiones en el servicio de consulta externa de un hospital,"Tinco Curi, Elizabeth Irene",Ingeniero Estadístico Informático,"Soto Rodríguez, Iván Dennys","Actualmente el sector salud dispone de una gran cantidad de información, la cantidad masiva de datos que se genera es muy compleja, los hospitales y otras organizaciones del sector necesitan un entorno que apoye las prácticas diarias de los médicos, la administración y el resto del personal de atención médica. La información utilizada para la generación de reportes proviene de sistemas transaccionales (Sistema de gestión hospitalaria), el cual almacena información referente al paciente y a su atención. El proceso de elaboración del reporte se realiza de manera manual-operativa ocasionando demoras e inconsistencias de información. El presente trabajo tiene como finalidad la implementación de una solución de Business Intelligence con el objetivo de obtener información única, limpia confiable y actualizada para facilitar la toma de decisiones en el servicio de consulta externa de un hospital, para ello se utilizó la metodología de Ralph Kimball que es la que se ajusta a las necesidades de la empresa, esencialmente por su esquematización a nivel de áreas específicas, tiempo de desarrollo y complejidad. Para el proceso de análisis y depuración de datos, así como para extracción, transformación y carga de procesos de actualización del Datamart para el servicio de consulta externa se hizo uso de la herramienta SQL Server Enterprise 2016. Los resultados de la implementación de la solución de Business Intelligence permitieron generar reportes y carga de datos en menor tiempo, lo que conlleva a obtener información confiable y concisa. El usuario podrá hacer uso de la información mediante reportes diseñados de acuerdo con sus necesidades. Finalmente se evidenció que el uso de herramientas de Business Intelligence tuvo impactos positivos en el área involucrada, facilitando la organización de la información a través de las herramientas de visualización y análisis de datos como: Excel, Reporting Services y el Power BI lo que permitió tomar decisiones acertadas en beneficio de los usuarios.",2022-03-29
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de clientes que realizaron fuga de equipos móviles en una empresa de telecomunicaciones utilizando el algoritmo Random Forest,"Marquez Meza, Francisco",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","El presente trabajo tiene como objetivo ilustrar el proceso de construcción del modelo predictivo que  permitió  detectar  los  casos  fuga  de equipos móviles en la Empresa, de forma anticipada al monitoreo existente, y en concreto para  el  canal  de  venta  Canal  1  y  para  la  ventana  de  monitoreo  de  45  días.  Lo anterior, mediante la utilización del algoritmo de aprendizaje  automático  denominado  Random Forest.",2022-03-29
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Estimación del monto de siniestros ocurridos y no reportados para el SOAT con el método Double Cchain Ladder,"Alarcón Pimentel, Sandra Elena",Ingeniero Estadístico e Informático,"Menacho Chiok, César Higinio","El sector asegurador peruano es supervisado en todo momento por la Superintendencia de Banca y Seguros – SBS. Esta entidad vigila y evalúa el comportamiento, prácticas e información relacionada a las aseguradoras con el objetivo de fomentar la rentabilidad, transparencia y una mayor protección de asegurados y beneficiarios. Para lograrlo las aseguradoras deben manejar métodos de estimación de provisiones (reservas) técnicas que contengan fundamentos estadísticos y que los resultados obtenidos sean lo más certero posible. En esta monografía, se presenta la metodología y la aplicación del método estocástico Double Chain Ladder - DCL para el Seguro Obligatorio de Accidentes de Tránsito – SOAT, siendo este tipo de método el más ventajoso para la estimación de pagos futuros de siniestros. Los resultados con la metodología DCL para estimación de la provisión de pagos futuros de los accidentes en el SOAT para la empresa aseguradora, se obtuvieron valores estimados para las reservas de siniestros ocurridos (RBNS) de S/. 293,206 y para las reservas de siniestros ocurridos y no reportados (IBNYR) de S/. 48,826 y para el total de las  reservas IBNR en S/. 342,033. En comparación, con el método Double Chain Ladder se obtuvo una reserva de S/. 342,033 menor que con el método Chain Ladder S/349,117. En la repartición de los siniestros, el 90% es de las reservas son RBNS y un 10% de las IBNYR, lo que implica que la gran mayoría de siniestros ya han sido notificados a la compañía, pero hay un retraso en la liquidación",2022-02-17
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Registro de los planes operativos y estratégicos usando el aplicativo CEPLAN V 1.0,"Flores Santos, José Alberto",Ingeniero Estadístico Informático,"Soto Rodríguez, Iván Dennys","El Centro Nacional de Planeamiento Estratégico (CEPLAN) es un organismo técnico especializado que ejerce la rectoría efectiva del Sistema Nacional de Planeamiento Estratégico conduciéndolo de manera participativa, transparente y concertada, contribuyendo así al mejoramiento de la calidad de vida de la población y al desarrollo sostenible del país. Ejercer la rectoría y orientar a las entidades del Sistema Nacional de Planeamiento Estratégico (SINAPLAN), en una gestión eficaz y eficiente, para alcanzar la visión concertada del futuro del país permitiendo el desarrollo armónico y sostenible. Dentro de sus funciones se encuentra desarrollar metodologías e instrumentos técnicos para asegurar la consistencia y coherencia del plan estratégico de desarrollo nacional. Asesorar a las entidades del estado y los gobiernos regionales y orientar a los gobiernos locales en la formulación, el seguimiento y la evaluación de políticas y planes estratégicos de desarrollo para lograr los objetivos estratégicos de desarrollo nacional propuestos oportunamente. Efectuar el seguimiento y la evaluación de la gestión estratégica del estado en forma continua. Dentro de sus órganos institucionales se encuentra la oficina de Administración entre cuyas principales funciones está la automatización y la mejora continua de procesos informáticos que sirva de apoyo al público en general.",2022-02-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Diseño e implementación de un Datamart para el área de análisis de una instancia técnica del sector educativo,"Ayala Flores, Karen Angela",Ingeniero Estadístico Informático,"Soto Rodríguez, Iván Dennys","La Instancia Técnica del sector educativo al que hace referencia el presente proyecto, posee gran cantidad de datos históricos producto de las evaluaciones de logros de aprendizaje que realiza anualmente. Sin embargo, no cuenta con un almacén de bases de datos estructurado, integrado y ordenado que permita el uso de esta información de manera óptima. En consecuencia, demanda de mucho tiempo atender los pedidos de información y generar reportes, además de que por ser este un proceso manual se incurre muchas veces al error. Por todo ello, surgió la necesidad de incorporar soluciones tecnológicas e innovadoras, como es el proceso de Inteligencia de Negocios. Por consiguiente, el presente proyecto tiene como objetivo principal la implementación de un Datamart para el área de Análisis de la Instancia Técnica, con la finalidad de gestionar la información y apoyar en la toma de decisiones. Esta implementación se realizó empleando la metodología de Ralph Kimball describiendo detalladamente cada etapa. Además, cabe mencionar, que para la implementación del Datamart se utilizaron únicamente softwares libres, como el Pentaho Data Integration – Kettle (herramienta de extracción, transformación y carga) y el MySQL Workbench (herramienta de diseño y gestión de bases de datos). Adicionalmente, para elaborar los reportes e informes, se utilizaron las herramientas Microsoft Excel y Power BI. La implementación del Datamart permitió automatizar procesos, con la creación de consultas para la atención de los pedidos de información y generación de reportes, originando que los tiempos de respuesta disminuyeran. Además, toda la información ingresada al Datamart fue validada por lo que ahora es confiable y consistente. Se generaron también accesos para los usuarios en base a sus perfiles. Adicionalmente, se crearon Dashboards para el monitoreo, análisis y visualización de los principales indicadores y métricas para la toma de decisiones acertadas.",2022-02-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Predicción del riesgo de incumplimiento en el pago de los créditos del portafolio de una entidad financiera utilizando regresión logística,"Miranda Pilco, Adriana",Ingeniero Estadístico Informático,"Menacho Chiok, César Higinio","El éxito de toda entidad financiera radica en la adecuada gestión de los riesgos a los que se encuentra expuesta, siendo uno de ellos el Riesgo de Crédito definido como la posibilidad de pérdida a consecuencia del incumplimiento de las obligaciones por parte del prestatario. Las herramientas analíticas usadas en la gestión de este tipo de riesgos han ido evolucionando a lo largo del tiempo e incluyendo a la estadística y la minería de datos como parte de estas. En esta memoria de Trabajo de Suficiencia Profesional se describe como la aplicación de la metodología de Credit Scoring conjuntamente con la metodología de minería de datos CRISP DM para la construcción de un modelo de riesgo comportamental en una entidad financiera, permitió obtener un indicador de gini de 64% y segmentar de mejor manera al portafolio de clientes de dicha entidad al incrementar en un 20% la participación de mejores clientes.",2021-11-22
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Propuesta de estrategia de monitoreo transaccional anti lavado de activos empleando el método de Ward y el teorema de Chebyshev,"Romero Domínguez, Adrián",Ingeniero Estadístico Informático,"Soto Rodríguez, Iván Dennys","La adecuada identificación de casos de investigación y la detección de operaciones sospechosas son pilares del sistema de monitoreo anti lavado de activos. En esa línea, el presente estudio ha propuesto implementar dentro del sistema de monitoreo anti lavado una estrategia basada en una segmentación empleando el método de Ward, definiendo umbrales de alertamiento mediante el teorema de Chebyshev. Mediante el método de Ward se logró segmentar el portafolio de clientes de depósitos de plazo fijo en cuatro segmentos homogéneos al interno pero heterogéneos entre sí. Luego en cada uno de estos se definieron sendos umbrales de alertamiento partiendo del teorema de Chebyshev. Esto permitiría a la empresa donde se realizó el estudio acentuar y priorizar los casos a investigar a fin de identificar con mayor celeridad el riesgo de lavado de activos y reportar los casos a la autoridad competente, objetivo central de la unidad de Prevención de Lavado de Activos.",2021-11-22
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Diseño e implementación de un sistema de información web en una empresa de medios de comunicación de Perú,"Jiménez Saravia, Juan Gabriel",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","El presente trabajo consiste en diseñar e implementar un sistema de información Web para una empresa líder en el mercado de investigación de medios de comunicación en América Latina, este sistema tiene como objetivo la automatización y digitalización de los procesos manuales que se ejecutan en el área de operaciones; entre estos procesos tenemos: control de calidad, sistemas para entrada y análisis de datos, reportes automatizados, etc. Para el desarrollo del sistema utilicé el gestor de base de datos MySQL, los lenguajes de programación PHP y JavaScript; y para el desarrollo de reportes y tableros de control automáticos utilicé el Software Power BI. Con el sistema implementado logre reducir el uso de papel a 0, eliminar el proceso de digitación de los formatos de recolección de datos, reducir los tiempos de los procesos de campo, además de consolidar y estandarizar todos los datos de la operación en una única base de datos MySQL.",2021-11-17
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Aplicación del análisis Pathway para incrementar la predisposición de compra de productos de belleza,"Huaman Torres, Karen Lizbeth",Ingeniero Estadístico Informático,"Gamboa Unsihuay, Jesús Eduardo","Las diferentes empresas tienen como uno de sus principales retos motivar la compra de los distintos bienes o servicios que brindan. De allí que surge la importancia de entender lo que el público valora al momento de elegir un producto. Teniendo ello en cuenta, el presente trabajo tiene como objetivo describir el camino crítico respecto a los factores que se deben accionar para lograr incrementar la predisposición de compra de productos de una marca de belleza. Su análisis se desarrolló en 3 fases: la primera consistió en un Análisis Factorial Exploratorio (AFE), donde los factores identificados permitieron reducir el número de variables que se consideraron para el análisis y cumplieron así el rol de variables latentes en el modelo. En la segunda fase se desarrolló un Análisis Factorial Confirmatorio (AFC), con el fin de confirmar el modelo obtenido en la fase previa, donde se verificó que la relación entre las variables y su respectivo factor era significativa. Por último, en la fase 3 se identificó a través del análisis Pathway, la relación de las variables que se consideraron en el modelo y cuál fue su efecto sobre la variable objetivo (predisposición de compra). Cabe mencionar, que para entender la variable objetivo se consideró como variables observables la significancia, diferenciación y presencia de una marca, mientras que como variables latentes o factores: el aspecto emocional, oferta, eficacia y vanguardia, que fueron medidas desde la percepción del consumidor. Es así como se identificó el efecto para todas las variables incluidas en el modelo sobre la predisposición de compra, y se concluyó que el efecto de mayor magnitud corresponde al factor emocional, lo que resultaría clave para incentivar la compra de los productos de alguna marca de belleza.",2021-11-17
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Análisis y pronóstico de la recaudación del impuesto vehicular municipal mediante la metodología Box - Jenkins,"Huamán Inga, Cinthia Katheryn",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","El principal objetivo del presente trabajo de suficiencia profesional es: determinar el mejor modelo que se ajuste a las características de la recaudación del Impuesto Vehicular de la Provincia de Lima mediante la técnica estadística de series de tiempo, con la finalidad de pronosticar el registro de ingresos de dicho concepto en tiempos futuros.",2021-10-06
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Modelo de factores asociado al desarrollo infantil temprano de niños peruano con indicadores  del Instituto Nacional de Estadística e Informática,"Guzmán Alanya, Miguel Alonso",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","En el puesto de analista de procesamiento de información en el Ministerio de Desarrollo e Inclusión Social se realizaron las siguientes funciones: apoyar en el cálculo de indicadores, generar reportes con las bases de datos de los sectores relacionados con salud, educación y agua y asistir técnicamente al equipo del Fondo de Estímulo al Desempeño y Logro de Resultados Sociales. En los últimos cinco años, se desarrollaron diversas tareas que pudieron ser realizadas adecuadamente con los contenidos de los siguientes cursos: Estadística Aplicada, Técnicas de Muestreo, Bases de datos y Técnicas Multivariadas. En el Ministerio de Desarrollo Inclusión Social, en el área del Fondo de Estímulo al Desempeño y Logro de Resultados Sociales, hasta el año 2019, no se contaba con una metodología capaz de reducir el gran número de indicadores relacionados con el Desarrollo Infantil Temprano, con la finalidad de ser usados posteriormente para el cálculo de los incentivos monetarios a los gobiernos regionales. En esta área se contaba con el apoyo de especialistas temáticos en salud, agua y educación, que explicaban la finalidad y utilidad de cada indicador, pero solo contaban con su criterio y experiencia para seleccionar los indicadores. Por lo tanto, evaluada la necesidad del área, se decidió a realizar un análisis factorial exploratorio, utilizando indicadores de agua, educación y salud publicados por el Instituto Nacional de Estadística e Informática relacionado con el Desarrollo Infantil Temprano, el cual permitió obtener un número reducido de indicadores con sustento técnico. El trabajo monográfico se encuentra dividido en seis partes, una vez presentado la introducción se pasará al marco teórico, seguido del marco metodológico, posteriormente, los resultados y discusión, luego las conclusiones y recomendaciones y finalmente las referencias bibliográficas.",2021-10-04
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de perfiles de los Centros de Educación Técnico - Productiva Públicos usando indicadores de condiciones básicas de calidad mediante clúster bietápico,"Tonconi Calisaya, Cesar Anthony",Ingeniero Estadístico Informático,"Valencia Chacón, Raphael Félix","El aumento continuo de grandes volúmenes de datos y la importancia de la utilización de éstos, junto con la búsqueda de información se han vuelto un gran reto que las grandes entidades públicas hoy en día quieren superar. En la actualidad las entidades públicas conocen la importancia que tiene el almacenamiento, la captura de datos y el beneficio que le puede resultar si se explotan correctamente. Este estudio presenta una técnica estadística para segmentar e identificar perfiles de los Centros de Educación Técnico-Productiva (CETPRO) públicos. El conjunto de datos está conformado por información de todos los CETPRO públicos a nivel nacional recogida en el 2019. El conjunto de datos inicial estaba compuesto por 704 instituciones educativas; posteriormente, luego de proceder con el análisis exploratorio y la limpieza de datos, correspondiente a la eliminación de datos outliers y faltantes; se trabajó un conjunto de datos compuesto por 684 instituciones. Se aplicó análisis de clúster bietápico, que es una técnica de segmentación que permite trabajar con variables cuantitativas y categóricas. El resultado de la aplicación de la técnica brindó 2 conglomerados: el primero con 324 CETPRO (47.4%), el segundo conglomerado con 360 (52.6%). Después de esto se procedió a describir los perfiles de cada conglomerado y se identificaron las principales características en función de las variables relacionadas a las 5 condiciones básicas de calidad planteadas.",2021-09-29
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Segmentación de usuarios que visitan el sitio web de una empresa utilizando la regresión logística con la técnica de sobremuestreo,"Tasayco Silva, Carlos Marcial",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","El trabajo de monografía se desarrolló en una empresa consultora líder en Latinoamérica especializada en soluciones analíticas para las áreas de marketing digital de empresas multinacionales. El trabajo consistió en la implementación y automatización de un modelo de regresión logística binaria para determinar los segmentos de los usuarios que visitan la Web de la empresa. Para realizar el modelo estadístico mencionado se empezó desde el análisis univariante de cada variable independiente, seguido por una técnica de sobremuestreo “SMOTE” para evitar el desbalance de las clases en la variable dependiente, se realizó además una matriz de confusión en la cual se obtuvo una precisión del 76%, hasta finalmente validar la predictibilidad analizando la curva ROC. Los resultados de la investigación ayudaron a demostrar y concluir que existen variables que son significativas para determinar si un usuario que visita la Web realiza una transacción. Por ejemplo: Los usuarios que usan canales orgánicos sin medios publicitarios como referencia y directo o pagados como las redes sociales contribuyen negativamente a la probabilidad de que el usuario haga la transacción, así como también se observó que tanto el tiempo de la visita o si el usuario visita la Web recurrentemente contribuyen a que la probabilidad de hacer la transacción se incremente. Finalmente, la segmentación que se realizó basado en las puntuaciones calculadas por la regresión logística binaria para tener tres segmentos bien diferenciadas que son alto, medio y bajo probabilidad. Al final del trabajo, la empresa aceptó y mostró su satisfacción con los resultados obtenidos.",2021-09-29
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Diseño muestral para determinar la demanda nacional de hoja de coca destinada al uso lícito,"Arroyo Maury, Paola Cinthya",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","El Instituto Nacional de Estadística e Informática en coordinación con la Comisión Nacional para el Desarrollo y Vida sin Drogas, ejecutó en el año 2019 por tercera vez en el Perú la encuesta nacional sobre consumo tradicional de hoja de coca, cuyo objetivo fue conocer la cantidad de hoja de coca adquirida en su forma natural a fin de caracterizar a la población para el análisis y diseño de políticas públicas. Para aplicar esta encuesta por muestreo, se llevó a cabo un plan de muestreo o diseño muestral complejo. La metodología utilizada para el diseño muestral se basó en cinco pasos. El primer paso fue para determinar la población objetivo, la cual estuvo constituida por las personas de 12 años y más de edad residentes habituales en las viviendas particulares del área urbana y rural del país. En el segundo paso se determinó el marco muestral, el cual se basó en un marco de áreas (conglomerados) y de lista (viviendas). El tercer paso fue elegir la técnica de muestreo, el cual consistió en un muestreo bietápico. El cuarto paso consistió en determinar el tamaño óptimo de muestra. La muestra estimada para la encuesta fue de 8600 viviendas, distribuidas en 849 conglomerados. El quinto paso consistió en llevar a cabo el proceso de muestreo. Los dominios de estudio fueron: nacional, nacional urbano, nacional rural, costa, sierra, selva y el área metropolitana de Lima y Callao. Las entrevistas fueron realizadas en los meses de julio y agosto del 2019, lográndose entrevistar a 8 371 viviendas, y un total de 23041 personas con una tasa de no respuesta de 2.7% y un error de marco del 12%. La estimación de la población objetivo que adquiere hoja de coca fue de 3 millones 692 mil 694 personas y la cantidad adquirida corresponde a 9558,62 TM.",2021-09-28
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Predicción de la prima de los clientes de una compañía aseguradora usando el modelo lineal generalizado Tweedie,"Armas Alvarado, Alberto Reimundo",Ingeniero Estadístico Informático,"Menacho Chiok, César Higinio",El objetivo de este trabajo es predecir la prima de los clientes de seguros vehiculares usando el modelo lineal generalizado Tweedie.,2021-09-28
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,"Validación para la determinación de minerales: calcio, hierro, magnesio, manganeso y zinc, en galletas con el método AOAC 985.35","Palomino Salazar, Leslie Liliana",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","El objetivo general de la memoria de trabajo de suficiencia profesional es validar la robustez del método para la determinación de minerales: Calcio, Hierro, Magnesio, Manganeso y Zinc, en galletas con el método AOAC 985.35",2021-08-20
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Evaluación del impacto de la fortificación de la harina de trigo en el nivel de anemia utilizando la técnica PSM,"Rivera Huamaní, Gianinna Magaly",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","La presente monografía de trabajo de suficiencia profesional se realizó en base a una investigación, que permite evaluar el impacto de la intervención de salud pública de fortificación de la harina de trigo con hierro sobre la prevalencia de anemia y deficiencia de hierro; en mujeres en edad fértil no gestantes entre los 15 a 49 años de edad de Chiclayo e Ica. Donde se evidencia a la anemia como un problema de salud pública que afecta al 21% de dicho segmento de la población de nuestro país. El diseño usado en la investigación fue cuasiexperimental, analítico y transversal; se realizó el análisis secundario de la base de datos recopilados en la encuesta de evaluación de impacto de la fortificación de la harina de trigo diseñada por el Centro Nacional de Alimentación y Nutrición del Instituto Nacional de Salud. En el marco metodológico, se empleó la técnica Propensity Score Matching para evaluar el efecto de la intervención para las participantes (consumo alto de alimentos derivados de la harina de trigo fortificada con hierro), mediante la utilización de covariables de un grupo de controles que no participan en la intervención, asignando a cada participante un par o medida de “control” lo más parecido a ella en términos de sus características, mediante el puntaje de propensión o probabilidad de participación en el programa. Los resultados muestran que la fortificación de la harina de trigo con hierro no alcanzó un impacto significativo sobre la prevalencia de anemia y déficit de hierro en el grupo de estudio. Dado que la fortificación no dio los resultados esperados en la mejora de la prevalencia de anemia, sería recomendable que se realicen inspecciones de control de calidad de la harina de trigo en los productos terminados de consumo masivo.",2021-08-18
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Modelo de ecuaciones estructurales en innovación de empresas de manufactura peruanas con información del Instituto Nacional de Estadística e Informática,"Soria Gomez, Eduardo Javier",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","La presente monografía se elaboró en base a una investigación cuyo objetivo era verificar el modelo de ecuaciones estructurales de la innovación tecnológica en empresas de manufactura peruana utilizando los resultados de la encuesta Nacional de Innovación en la Industria Manufacturera aplicada en el 2012 por el Instituto Nacional de Estadística e Informática. La evaluación de los resultados se realizó mediante con el software SmartPLS, para la investigación del paper Analyzing technological innovation in low and medium-low tech Peruvian manufacturing companies, desarrollado por un grupo de investigadores Christian M. Ringle, Sven Wende y Jan-Michael Becker de universidades europeas. Dicho estudio se enmarca en la política de promoción de investigación de la Facultad de Ingeniería de la Universidad ESAN. Como parte de la promoción de la investigación se identificó la necesidad de proponer un modelo de ecuaciones estructurales que explique la innovación tecnológica de empresas de manufactura peruanas, mediante las interrelaciones de las variables latentes capacidad de absorción, fuentes de información y adquisición tecnológica. La investigación se realizó con 856 empresas. Los resultados más importantes de la investigación muestran que las interrelaciones del modelo propuesto de ecuaciones estructurales verifican que la innovación tecnológica de empresas de manufactura peruanas se explica únicamente por las variables latentes capacidad de absorción y adquisición tecnológica, y que dicho modelo presenta un buen ajuste, debido a que el indicador coeficiente de determinación ajustado fue 0.520; y el indicador GoF fue de 0.5, por ser mayor al valor óptimo de 0.31.",2021-08-11
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Imputación de datos faltantes en los ingresos por hogar en la Enaho utilizando el método del K-vecino más cercano,"Collazos Tuesta, Oscar Ronald",Ingeniero Estadístico Informático,"Menacho Chiok, César Higinio","La Encuesta Nacional de Hogares (ENAHO), es el instrumento que utiliza el Instituto Nacional de Estadística e Informática (INEI) para recopilar a nivel nacional los datos de los hogares sobre su condiciones económicas, educativas, salud, etc. y que permiten generar indicadores que miden el estado y la evolución de la pobreza, el bienestar y las condiciones de vida de los hogares del Perú, así como para efectuar diagnósticos y medir el alcance de los programas sociales (alimentarios y no alimentarios) en la mejora de las condiciones de vida de la población peruana. Sin embargo, un problema que debe enfrentar la ENAHO es la no respuesta total o parcial en las unidades de muestreo (no respuesta en unidades) o en una pregunta específica (no respuesta por ítem); sobre todo a las preguntas referidas a los ingresos de los hogares. Para el tratamiento de los datos faltantes, se han propuesto una variedad de métodos que comprenden desde el más simple que consiste en la eliminación de las observaciones que tengan algún dato faltante en una de las variables hasta métodos más consistentes basados en un proceso de imputación con los datos faltantes a partir de los datos completos. El objetivo de esta investigación es presentar y aplicar los métodos de imputación de la media y mediana, el método Hot-Deck y el k vecino más cercano para estimar los datos faltantes del Ingreso por hogar en la ENAHO 2017 trimestre 3. Los resultados indican que los datos faltantes del ingreso tienen un mecanismo MCAR. La estimación del intervalo de confianza del 95% para la media de los ingresos imputados, tuvieron amplitudes por el método de la media 131,41 (el menor) mientras que por el k vecino más cercano fue 139,4. Para estimación de la desviación estándar del ingreso, fue el menor para la media 92,97 y k vecino más cercano 100,99. Los resultados de la comparación de los métodos de imputación, fueron usando los datos completos para generar una muestra aleatoria de datos faltantes artificiales y luego se hallaron el Cuadrado Medio del Error (ECM) y correlaciones con los datos observados e imputados para cada método. El método del k vecino más cercano tuvo los menores valores de ECM 1412,6 y 444,4 para la media y mediana; mientras que los otros métodos sus valores fueron por la media 1504,5; por la mediana 1619,9 y por el Hot-Deck 1963,7. Los coeficientes de correlaciones resultaron con valores muy similares, para k vecino más cercano 0,968 con la media y 0,964 con la mediana.",2021-08-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Diseño y análisis de un sistema de información para el registro nacional de personas certificadas del SINEACE,"Castro Rojas, Jheynner Adler",Ingeniero Estadístico Informático,"Menacho Chiok, César Higinio","Hoy en día, la información de las organizaciones es uno de los activos más importantes para estas, y las entidades del Estado no son ajenas a ello. Sin embargo, en ocasiones estas tienen serios problemas para salvaguardar su propia información. El Sistema Nacional de Evaluación, Acreditación y Certificación (SINEACE), ante la falta de un sistema diseñado a medida de sus necesidades, hacía uso de ficheros en Excel para almacenar información referente al proceso de certificación de competencias. El presente trabajo busca diseñar y analizar cada tramo del desarrollo de los procesos de certificación, haciendo uso de una adaptación del ciclo de vida del desarrollo de sistemas de información, con la finalidad establecer con claridad el adecuado procedimiento de flujos de datos y optimizar los diversos procesos para la recuperación de estos, involucrando a todas las partes interesadas; y logrando como resultado el diseño de un sistema adecuado a las necesidades de la institución, incluyendo el modelo entidad-relación, que permita aclarar las relaciones entre las diversas entidades involucradas en el proceso de captura de información; así como, también, un prototipo que ayude al posterior desarrollo del sistema que tenga la función de gestionar la información de los procesos de certificación de competencias",2021-08-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Estimación del capital económico por riesgo operacional financiero usando el método de distribución de pérdidas con convolución Poisson y Log-normal,"Polo Sánchez, Silvia Patricia",Ingeniero Estadístico Informático,"Menacho Chiok, César Higinio","En una institución bancaria el área de Administración de Riesgo Operacional es la encargada de gestionar, evaluar, medir y supervisar los riesgos operacionales en todas las áreas de la financiera en base al cumplimiento de la normativa regulatoria, en el Perú el ente regulador bancario es la SBS (comité de superintendencia de banca y seguros), el cual hace el requerimiento del capital económico por riesgo operacional a las instituciones bancarias, este capital permite a la empresa provisionar un dinero adecuado para cubrir las existencias de pérdidas generadas por riesgos operacionales (fallas en procesos, errores humanos, fallas en sistemas, etc). Se aplicó para estimar el capital económico por riesgo operacional el método de medición avanzada (AMA) usando el modelo de distribución de pérdidas (LDA) con convolución Poisson y Log-Normal. Los resultados obtuvieron una estimación de la pérdida esperada de 154,000,000 soles, pérdida no esperada de 27,000,000 soles y un capital económico de 181,000,000 soles; obteniendo la empresa bancaria un ahorro de capital de 54,000,000 soles.",2021-08-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Segmentación de lectores digitales registrados de un sitio web informativo con el algoritmo de análisis Cluster k-means,"Clemente Rivera, Brian Erick",Ingeniero Estadístico Informático,"Soto Rodríguez, Iván Dennys","Debido a la gran diversidad de negocios de la empresa, un importante grupo de medios de comunicación, es que se genera mucha información, cada vez más específica y detallada; y es aquí en donde entra la gerencia de Inteligencia de Negocios, la cual centraliza todos estos datos provenientes de distintas fuentes y plataformas con la finalidad de analizarlos y brindar soporte a las diferentes áreas que requieran de algún análisis a detalle para sustentar una venta, una adquisición, el desarrollo de proyectos, etc. Una de estos requerimientos especializados tiene que ver con la integración de datos de distintos orígenes tanto digitales como los que se generan en los sitios web, las redes sociales; o las tradicionales como los ingresos que genera la publicidad para la compañía, base de datos de audiencia, entre otros, ya que permitirán abordar análisis más complejos para encontrar hallazgos más específicos, diferenciales y relevantes. La presente monografía aborda el desarrollo de una nueva metodología de segmentación de usuarios registrados en la página web, en la cual se ha planteado considerar el tipo de contenido que ellos visitan según la sección en la que están alojadas las notas y complementándolos con la información personal, sociodemográfica, de ubicación y otras disponibles en el negocio, todo esto apoyado en el análisis clúster, específicamente el algoritmo k-means. Para el preprocesamiento de datos, limpieza, construcción del conjunto de datos y ejecución de la metodología se utilizó el software R, que posee múltiples funciones que ayudaron con estas tareas. Estas seis agrupaciones encontradas permitirán ofrecer a los clientes un nuevo producto comercial, que además otorgará una ventaja para los clientes ya que podrán especificar la audiencia específica a la que quieren impactar mejorando significativamente los resultados que se obtendrían a diferencia del método tradicional de publicidad digital.",2021-08-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Clasificación de los adolescentes infractores del centro juvenil de diagnóstico y rehabilitación de lima utilizando partición alrededor de medoides (PAM),"Che Piu Deza, Gilda",Ingeniero Estadístico Informático,"Gamboa Unsihuay, Jesús Eduardo","El país sufre de delincuencia desde hace años, percepción que se mantiene alrededor del 40% de los mayores de edad, incluso a marzo del año 2020. Algunos actos delictivos son cometidos por adolescentes, quienes son vulnerables física y psicológicamente a cometer infracciones, afectando a toda la sociedad, sin embargo, información sobre esta realidad nacional es limitada, por ello el este estudio tiene el objetivo de segmentar a los adolescentes infractores del Centro Juvenil de Diagnóstico y Rehabilitación de Lima utilizando el algoritmo PAM (Partición Alrededor de Medoides), que conglomera los datos en k grupos, donde k es calculado aplicando el método de la silueta. Se analizaron variables relacionadas al listado de factores de riesgo del Plan Nacional de Prevención y Tratamiento del Adolescente en Conflicto con la Ley Penal, y de 790 internos, se obtuvieron 2 segmentos, el primero compuesto por adolescentes internos por diversas infracciones (contra el patrimonio, contra la vida el cuerpo y la salud, contra la libertad y contra la seguridad pública) y motivos (lucro personal, emoción violenta, ajuste de cuentas y venganza), los que nunca integraron una familia nuclear y lo que prácticamente pasaron toda su adolescencia con sus padres, con historial de consumo de drogas y/o alcohol, pero que nunca se escaparon de su casa, ni tenían familia con historial penitenciario, ni mejores amigos infractores y vivían en zonas tranquilas; en cambio el segundo segmentó a los que en su mayoría cometieron infracciones contra el patrimonio, que vivieron en familias mono parentales o a cargo de otras personas, todos con historial de consumo de drogas y/o alcohol, algunos se escaparon de su casa antes de los 15 años, tuvieron familiares con historial penitenciario, también mejores amigos que cometieron infracciones contra la Ley Penal y vivían en barrios con pandillas y bandas delictivas",2021-08-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de la propensión a la adquisición de un subproducto de una tarjeta de crédito en una entidad bancaria,"Zarabia Yupanqui, Cinthia",Ingeniero Estadístico Informático,"Salinas Flores, Jesús Walter",El objetivo del presente trabajo es identificar a los clientes con tarjeta de crédito más propensos a la adquisición del subproducto Extra Línea de la tarjeta de crédito en una entidad financiera.,2021-08-09
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Predicción de adquisición de un préstamo personal bancario a través del canal de televentas utilizando el algoritmo Random Forest,"De la Cruz Flores, Fiorella Pamela",Ingeniero Estadístico Informático,"Salinas Flores, Jesús Walter","El objetivo de este trabajo es desarrollar un modelo de clasificación binaria para predecir si el cliente va a aceptar o rechazar el préstamo ofrecido por los asesores al contactarlos vía telefónica. Para ello, se utilizó el algoritmo Random Forest que permitió redecir a los clientes con mayor probabilidad a adquirir el producto y así, gestionarlos óptimamente para priorizar su venta. Para el desarrollo del modelo se usó una muestra de seis meses (desde marzo 2017 hasta agosto 2017) de datos de los clientes que cuentan como mínimo con una tarjeta de crédito y tienen un préstamo pre- aprobado con la entidad bancaria. En total la base contó con 991619 registros de clientes",2021-08-09
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Diseño muestral de una línea base para caracterizar a los productores de cacao en el Perú,"Orellano Bedón, María Del Carmen Nataly",Ingeniero Estadístico Informático,"Rosas Villena, Fernando René","En muchos países del mundo existen entidades que dentro de sus competencias está el diseño y ejecución de encuestas, censos y otras investigaciones, esto con el fin de obtener información relevante, oportuna y confiable para la toma de decisiones. Inicialmente estos estudios se realizaban siguiendo estrictamente las especificaciones señaladas en los términos de referencia que forman parte del contrato entre la empresa solicitante y la empresa prestadora del servicio. Sin embargo, este procedimiento no garantizaba la obtención de resultados óptimos y por ende ponía en peligro el prestigio de la empresa con los clientes. Ante esta problemática, surgió la necesidad de conducir los estudios siguiendo los pasos de un diseño muestral estándar que tome en cuenta el cumplimiento de los protocolos de seguridad en todas sus etapas. En la memoria del Trabajo de Suficiencia Profesional se expone el caso de una importante institución del estado que contrata los servicios de la empresa para diseñar una línea base para caracterizar a los productores de cacao en el Perú. El objetivo de la línea base es medir los cambios en las variables que la conforman a través de estudios de corte longitudinal. La metodología estandarizada; que en adelante llamaremos diseño muestral estándar, utilizada tiene seis etapas: (1) Identificación de la población de objetivo, (2) Elección del método de recolección de datos, (3) Identificación del marco de muestreo, (4) Selección del método(s) de muestreo y determinación del tamaño de muestra, (5) Planificación del trabajo de campo y aplicación de la prueba piloto, y (6) Ejecución del trabajo de campo",2021-06-30
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de las variables determinantes en el cese voluntario de un colaborador con la regresión de Cox,"Palomino Gonzales, Javier Norberto",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","En presente trabajo de investigación se explica el desarrollo del modelamiento del evento renuncia voluntaria de los colaboradores de una organización financiera a través del modelo de regresión de Cox. Esta iniciativa fue de mucha utilidad para identificar cuáles son las variablesque determinan este evento y cómo estas impactan en el evento renuncia. Así mismo es muy importante resaltar que para llegar a realizar este estudio, se tuvo que resolver varios desafíos respecto al conocimiento del negocio, así como la obtención los datos a través de una automatización de las consultas a las bases de datos de recursos humanos para obtener datos confiables y relevantes para el estudio. Dicha automatización fue de mucho aporte para la organización, ya que permitió ahorrar 15 días de proceso manual para su obtención, actualmente el proceso manual se ha reducido a actualizar un tablero de indicadores que solo demora 3 minutos en actualizar. Por ello consideramos que el presente estudio representa el trabajo de un año de comprender la dinámica interna de los procesos y a partir de estos proponer nuevas formas de abordar tanto los procesos, así como la formas de analizar los resultados del mismo",2021-06-26
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,"Comportamiento del gasto público de una entidad pública usando el modelo Box - Jenkins, 2009 - 2017","Cormán Trujillo, Juan",Ingeniero Estadístico e Informático,"Soto Rodríguez, Iván Dennys","La entidad pública brinda servicios de capacitación para el Sector Vivienda, Construcción y Saneamiento, inicia su vida institucional en el año 1977. Tiene como finalidad la formación de los trabajadores del sector construcción, la educación superior no universitaria, el desarrollo de investigaciones vinculadas a la problemática de la vivienda y edificación, así como la propuesta de normas técnicas de aplicación nacional. Y se ha establecido en doce sedes a nivel nacional y capacita a más de 30,000 trabajadores en el desarrollo de nuevas tecnologías. La entidad según su estructura organizacional se encuentra conformado por: Consejo Directivo, Gerencia General, Oficina de Administración y Finanzas, Oficina de Planificación y Presupuesto, Gerencia de Formación profesional y Gerencia de Investigación y Normalización; cabe señalar que el Departamento de Informática forma parte de la Oficina de Administración y Finanzas, en la cual se desempeñó el cargo.",2021-03-30
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Caracterización del perfil del ingresante de una Universidad Pública aplicando algoritmos clustering K-Prototypes y K- Medoids,"Chavez Valderrama, Ledvir Ayrton Walter",Ingeniero Estadístico Informático,"Salinas Flores, Jesús Walter","En el presente trabajo de investigación se realizó un estudio comparativo de algoritmos no supervisados para la caracterización del perfil del ingresante de una universidad pública respecto a sus variables sociodemográficas, económicas y de rendimiento académico utilizando algoritmos de segmentación K-prototypes y K-medoids, con el fin de generar conocimientos valiosos y útiles para lograr una mejor comprensión de la diversidad de universitarios que ingresan y con ello conocer el tipo de estudiante que la institución forma, La aplicación se efectuó con datos de alumnos ingresantes a la Universidad Nacional Agraria La Molina de los ciclos académicos 2015-I y 2015-II de las modalidades de Concurso Ordinario y Dos Primeros Puestos de Colegios de Educación Secundaria con un total de 690 postulantes. Se realizó el preprocesamiento de los datos y la aplicación de algoritmos clustering trabajando tanto con variables cuantitativas como cualitativas, para luego determinar el número óptimo de conglomerados y el algoritmo más adecuado utilizando índices de validación interna. Se realizó la validación de los clusters obtenidos de manera univariada (análisis de variancia o ANOVA y prueba Chi cuadrado) y multivariada (algoritmo Boruta y árbol C5.0), por último, se determinó las variables más importantes para caracterizar el perfil de los ingresantes. Con la investigación realizada se logró identificar 3 tipos de alumnos: Ingresante previsto, Ingresante en proceso y el Ingresante en inicio; cada uno con características peculiares, las cuales permitirán a los responsables de las políticas educativas y en especial a los profesores consejeros saber el tipo de alumno que tienen a su cargo desde que ingresa a la universidad y empezar con ello políticas educativas como el emprendimiento del acompañamiento especializado, sistemático e integral; buscando la realización del paradigma del aprendizaje que la universidad se ha propuesto en su Modelo Educativo.",2021-03-30
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Cambios en la producción agrícola y el rol de la investigación y extensión agrícola en el Perú: 1950-2011,"Gómez Galarza, Vilma Elvira",Ingeniero Estadístico,"Sotomayor Ruíz, Rino Nicanor","Este trabajo presenta el análisis de 60 años de evolución de la producción del sector agrícola desde 1950 al 2011 y lo relaciona con la investigación y extensión agrícola. Es una investigación no experimental que se expone en tres partes cuyo centro es el análisis del comportamiento de la producción agrícola y los factores: superficie cosechada, rendimientos por ha y la estructura de uso del suelo, luego se relaciona con la investigación y extensión agrícola. Este trabajo ha utilizado más de 200 series estadísticas que fueron sometidos a análisis exploratorio mediante representaciones gráficas de secuencias con diagrama de líneas, análisis de consistencia y homogeneización a través de conversiones y ajustes mediante el promedio trienal móvil y ponderaciones. A las series estadísticas ajustadas se aplicó el Método de los Efectos para cada uno de los 49 cultivos y sus correspondientes grupos. Los resultados de las investigaciones muestran que: (a) El sector agropecuario ha estado supeditado a los modelos de desarrollo que privilegiaron a otros sectores de la economía lo que generó una producción agrícola con más de 30 años de estancamiento continuo en el periodo de 1950-2011. (b) Al relacionar la producción con los factores señalados, los cálculos evidencian que, el factor superficie cosechada ha definido la evolución de la producción agrícola tanto en los periodos de expansión como de estancamiento. Los factores rendimiento y estructura de uso del suelo han jugado un rol secundario y en los periodos de estancamiento ha atenuado la tendencia de descenso de la producción agrícola. (c) Al analizar el factor rendimiento como indicador del aporte de la investigación a la producción agrícola se evidencia su contribución positiva en especial en las épocas de estancamiento a pesar de la débil institucionalidad y del escaso financiamiento del estado y del sector privado. Finalmente se presentan las conclusiones y recomendaciones.",2019-12-02
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Variables que explican los rangos remunerativos del primer empleo de los egresados universitarios del Perú aplicando regresión logística ordinal,"Aquino Gamboa, Juan Carlos",Ingeniero Estadístico e Informático,"Menacho Chiok, César Higinio","En la presente tesis se aplica la regresión logística ordinal, con la finalidad de identificar las variables que mejor explican los rangos de los ingresos de los egresados universitarios del Perú. Para el estudio se usa los datos de la encuesta de egresados universitarios del año 2014 realizada por el INEI. Se ajustan los datos a tres modelos aplicando la regresión logística ordinal con función de enlace logit proporcional acumulativo y para las universidades públicas y privadas. La variable dependiente son los rangos de ingresos (Bajo, Medio, Alto y Muy alto) y los conjuntos de variables independientes agrupadas en cuatro categorías: Grupo A: socio-académicas (7), Grupo B: referidas a la evaluación de las competencias recibidas en la universidad (12), Grupo C: referidas a la importancia de las competencias para su desarrollo profesional (12) y Grupo D: respecto a los profesores de la carrera (5). Para explicar los rangos de los ingresos, la regresión logística ordinal identificó para los tres modelos y para las universidades públicas y privadas, variables significativas socio-académicas: sexo, pertenencia al cuadro de méritos, si obtuvo o no el título profesional, su primer empleo relacionado con la formación profesional. Respecto a la calificación sobre la preparación recibida en la universidad para el desarrollo de las competencias: para coordinar actividades, para los conocimientos básicos de otros campos (públicas) y para el dominio del área de disciplina, el utilizar herramientas informáticas básicas, el utilizar software específico de la carrera (privadas). Respecto a la importancia de las competencias para su experiencia laboral: redactar informes o documentos, tener conocimientos básicos de otros campos o disciplinas (públicas) y rendir bajo presión y cumplir con los objetivos y coordinar actividades (privadas).",2019-10-03
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de las reglas de asociación utilizando los algoritmos secuenciales Spade y GSP,"Lévano Chiroque, Fernando Thomas",Ingeniero Estadístico Informático,"Salinas Flores, Jesús Walter","Hoy en día los datos secuenciales son de gran importancia, debido a que pueden encontrarse en distintas aplicaciones como: registros de ventas, registros médicos de pacientes, registros webs, bolsa de valores, base de datos en geofísica, etc. Es por esta razón que se han estudiado las tendencias o patrones a través del tiempo con el algoritmo secuencial. En esta investigación se estudia con mayor profundidad el algoritmo secuencial Sequential Pattern Discovery using Equivalent Class (SPADE, por sus siglas en inglés), debido a que usa una eficiente búsqueda de las reglas que se generan por el algoritmo, reduciendo el número de reglas y costos de la memoria. En primer lugar, se ilustra el procedimiento para obtener las reglas de asociación y luego con un conjunto de datos se identifican las reglas de asociación computacionalmente. Por último se compara los resultados obtenidos con el algoritmo Generalized Sequential Patterns (GSP, por sus siglas en inglés), debido a que ambos algoritmos tienen el mismo enfoque. Uno de los resultados más resaltantes fue “el cliente compra pavo en un tiempo máximo de tres meses, dado que compró antes costilla de cordero”. Los resultados que se obtuvieron sirven para incrementar las ventas del establecimiento a través de ventas cruzadas. El algoritmo SPADE permitió obtener reglas más completas que el GSP.",2019-07-08
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Estudio de las principales variables que determinan el consumo de una marca de bebida gaseosa usando técnicas discriminantes,"Alcedo Zambrano, Rosario Jessica",Ingeniero Estadístico Informático,N/A,"La investigación de mercados y el uso de la estadística básica y avanzada en los últimos tiempos han pasado a ser una de las herramientas más importantes para los Jefes y Gerentes de Producto, un claro ejemplo de ello es el presente estudio, que a través del análisis multivariante busca resolver algunas inquietudes. El presente estudio, tiene como finalidad construir un modelo que permita predecir si un consumidor de gaseosas consume la Marca X. El modelo será utilizado para identificar a sus consumidores y conocer sus expectativas, evaluar sus gustos/ preferencias y medir el impacto de la publicidad y las promociones. La data con la que se construyó el modelo, fue proporcionada por la Empresa X, ellos cuentan con información pasada y presente de una serie de variables que son de su interés. Lo que se buscó con este estudio, es aprovechar dicha información para realizar el modelo. Para el análisis se utilizaron dos técnicas discriminantes, Análisis de Regresión Logístico y Arboles de decisión, ambos modelos se construyeron con uso del Enterprise Miner (SAS). El resultado final, fue un árbol de decisión con seis variables independientes, se trata de un modelo fácil de interpretar y aplicar.",2019-05-09
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Clasificación de resultados en la prueba de admisión de la UNALM utilizando análisis discriminante lineal de Fisher y Máquina de Soporte Vectorial,"Vivanco Huaytara, Fredy",Ingeniero Estadístico e Informático,"Rosas Villena, Fernando René","El objetivo general de la investigación es la identificación entre las técnicas del Análisis Discriminante Lineal de Fisher y Máquina de Soporte Vectorial la que presenta mejores indicadores de clasificación del rendimiento de los postulantes en la prueba de admisión 2015-II en la Universidad Nacional Agraria La Molina (UNALM). Ambas técnicas estadísticas se aplicaron en dos oportunidades, en la primera se evaluó el resultado en la prueba admisión de la UNALM de los postulantes que no se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 1) y en la segunda se evaluó resultado en la prueba admisión de la UNALM de los postulantes que si se prepararon en el CEP-UNALM y que ingresaron o no a la universidad (aplicación 2). Los resultados muestran que la técnica Máquina de Soporte Vectorial presenta mejores indicadores de clasificación que el Análisis Discriminante Lineal de Fisher.",2018-11-27
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción metodológica del modelo espacial autorregresivo en el error,"Polo Lucero, Marco",Ingeniero Estadístico Informático,N/A,"El propósito de este trabajo es presentar y explicar la metodología de un modelo espacial autorregresivo en el error. Para esto se comenzó explicando la teoría necesaria para comprender la estructura que tiene un modelo espacial autorregresivo en el error. En esta parte se estableció que para este modelo es necesario utilizar variables de tipo de corte transversal, que la unidad espacial es un área geográfica delimitada por un polígono y que el comportamiento de la variable de interés debe ser diferente en cada unidad espacial en el área de estudio, esto para no tener problemas de autocorrelación espacial. Finalmente, este modelo utiliza una matriz de pesos que tiene la función de controlar y capturar la autocorrelación espacial, obteniendo un modelo con parámetros estimados insesgados y consistentes.  Para explicar la aplicación del modelo espacial autorregresivo en el error, se utilizaron los resultados de una investigación que se hizo en Argentina acerca de la Fecundidad (promedio de hijos nacidos vivos al nacer por mujer) en mujeres entre 15 y 29 años de edad. Donde primero se obtuvieron estadísticas básicas de las variable dependiente (variable en interés) y de las independientes. Luego se dividió en 531 unidades espaciales (partidos) al territorio Argentino. Después se eligió dos tipos de matrices de pesos (reina y 4-vecinos más cercanos). Posteriormente se probó con la estadística de I Moran, que si existía autocorrelación espacial global utilizando solo la variable fecundidad. Luego se utilizó la variable fecundidad con sus variables explicativas y se concluyó utilizando las pruebas estadísticas LM-ERR y LM-EL que la autocorrelación espacial se encontraba en la estructura del error. Lo anterior sugería que el modelo espacial autorregresivo en el error era el recomendable a utilizar. Finalmente se presentaron y estimaron a los dos modelos espaciales autorregresivos, uno utilizando la matriz de pesos tipo “reina” y el otro modelo utilizando la matriz de pesos tipo “4-vecinos más cercanos”, ambos modelos con variables explicativas significativas y capturando la autocorrelación espacial en la estructura del error,  concluyendo que ambos modelos espaciales autorregresivos en el error son igual de óptimos para el estudio de la Fecundidad en Argentina.",2018-06-15
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción metodológica del anàlisis Clúster utilizando el algoritmo de Ward,"Dongo Román, Andie Bryan",Ingeniero Estadístico Informático,N/A,"El presenta trabajo tiene como objetivo principal describir la metodología que se debe seguir al realizar un análisis clúster utilizando el algoritmo de Ward, mostrando una serie de pasos para su correcta aplicación. Además, exponer cuáles son las características y las ventajas de elegir este algoritmo como criterio de agrupamiento.  El algoritmo de Ward es uno de los diversos métodos jerárquicos del análisis clúster, el cual viene a ser uno de lo más usados por tener un fundamento estadístico (mientras que los demás suelen ser heurísticos), pues se basa en el criterio de la suma de cuadrados para medir la proximidad entre clústeres durante el proceso de agrupamiento.  Como ejemplo aplicativo se planteó el caso de las Comunidades Autónomas de España, las cuáles se agruparon en base a la actividad de sus salas de proyección de cine. En este caso, siguiendo los pasos correspondientes, el algoritmo de Ward determinó que estas Comunidades se agrupaban en cuatro clústeres, los cuales mostraron características que los diferenciaban entre sí en función de las variables de estudio.",2018-06-15
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción del procedimiento metodológico del análisis cluster no jerárquico con el algoritmo Clarans,"Salvador Alfaro, Carlos Agustín",Ingeniero Estadístico Informático,N/A,"El algoritmo CLARANS, perteneciente a los métodos clúster no jerárquico. Lo que se pretende describir en este trabajo es explicar el procedimiento del algoritmo CLARANS. El proceso que realiza este algoritmo es encontrar una muestra con una cierta aleatoriedad en cada paso de la búsqueda. El agrupamiento obtenido después de sustituirlo a un solo medoide se denomina el vecino del agrupamiento actual. Si en el camino el objeto (individuo) encuentra un mejor vecino, CLARANS lo mueve al nodo del vecino y el proceso comienza de nuevo; si ya no lo encuentra entonces el agrupamiento actual para y se produce un óptimo local (Cluster). Se presenta un ejemplo que ilustra la metodología y se explica el paso a paso del algoritmo CLARANS.",2018-06-04
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción metodológica del análisis conjunto con perfiles completos,"Morales Plaza, Sonia Paola",Ingeniero Estadístico Informático,N/A,"El comportamiento del consumidor se ve influenciado por diversos factores al tomar una decisión de compra, generando preferencias por aquel producto o servicio que reúna las características deseadas. Por ello las organizaciones y empresas han enfocado sus esfuerzos por desarrollar productos o servicios basados en las preferencias de sus clientes potenciales. Una técnica estadística multivariante que permite conocer cuáles características de un producto o servicio son las de mayor preferencia por los consumidores, es el Análisis Conjunto.  Este trabajo presenta y describe la metodología de Análisis Conjunto con perfiles completos, desarrollada para conocer la estructura de las preferencias de los consumidores de un modo más cercano a la realidad. Así mismo, se ilustra la utilización de esta técnica, sus variados ámbitos de aplicación, diversos tipos de Análisis Conjuntos existentes y en qué contexto resulta más apropiado aplicarlas. Finalmente, se realiza una aplicación con datos sobre una empresa que desea lanzar al mercado un nuevo producto, con el objetivo de conocer la combinación ideal de características que deba poseer éste para obtener una mayor preferencia por sus clientes potenciales y así desarrollar un diseño de producto eficaz.",2018-06-01
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Clasificación de fuga de clientes en una entidad financiera utilizando el algoritmo Smote para datos desbalanceados en una regresión logística,"Pariona Huarhuachi, Jefferson Clauss",Ingeniero Estadístico e Informático,"Salinas Flores, Jesús Walter","La retención de clientes ha tomado mucha importancia en los últimos años en las entidades financieras debido a la competencia agresiva por parte del sector, así como la autonomía del cliente en buscar mejores beneficios dentro de todas las ofertas que existen en el mercado bancario lo que se ve reflejado en el aumento de la tasa de clientes fugados. Ante esto se ha visto necesaria la implementación de técnicas estadísticas y/o técnicas de minería de datos, con la finalidad de construir un clasificador predictivo que pueda ayudar a identificar a clientes potenciales a fugarse. En muchos casos cuando se aplican técnicas de clasificación, es común que la clase a predecir ocurra con menor frecuencia que la otra clase: la presencia de datos desbalanceados. Es decir, se tiene menor número de clientes fugados que no fugados, lo cual representa un inconveniente debido a que el clasificador necesita datos suficientes de ambas clases para poder aprender de ellas y así alcanzar una buena predicción. En esta investigación se propone el algoritmo Syntetic Minority Over-sampling Technique (SMOTE) como solución a este problema. SMOTE crea instancias nuevas a partir de un sobre-muestreo de las instancias existentes, llevando la clase minoritaria a un número suficiente para ser considerada balanceada y la clase mayoritaria si es necesaria reducirla mediante sub-muestreo aleatorio. En la presente investigación se validarán tales beneficios con la construcción de un modelo de regresión logística binaria con datos desbalanceados con y sin la aplicación del algoritmo de SMOTE; con el fin predecir la fuga de clientes en una entidad financiera. Se usarán para medir la precisión, la curva ROC y elementos de la comprobación de tabla cruzada como la especificidad y la sensibilidad.",2018-05-30
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Segmentación de clientes de un casino utilizando el algoritmo partición alrededor de medoides (PAM) con datos mixtos,"Elguera Vega, Rhony Miguel",Ingeniero Estadístico e Informático,"Salinas Flores, Jesús Walter","En la actualidad, la gran cantidad de datos que se almacenan de los clientes en las diferentes empresas y la capacidad de procesamiento que brindan las computadoras, han generado gran interés por investigar; así como, desarrollar métodos y algoritmos para el análisis de agrupamiento. Los métodos de agrupamiento dirigidos a la segmentación de clientes permiten a las empresas identificar los patrones y perfiles de compra o servicios, ayudando a tomar mejores decisiones de las estrategias de canales y publicidad para sus clientes. En la presente investigación se aplica el método de agrupamiento basado en las particiones de k-Medoides con el algoritmo PAM (Partición Alrededor de Medoides). El algoritmo PAM se basa en particionar el conjunto de datos en k grupos, donde k es conocido; es considerado más robusto ante datos atípicos y el ruido, se basa en minimizar la suma de disimilitudes entre un objeto y el Medoide (centro del grupo). El objetivo de la presente investigación es aplicar el algoritmo PAM para segmentar a los clientes de un casino con los datos obtenidos, a través del uso de tarjetas en el tragamonedas. El método de la silueta permitió identificar tres clústers como el número óptimo. El análisis de agrupamiento con el algoritmo PAM usando la medida de distancia Gower, resultó la segmentación de clientes para los tres clúster con porcentajes de 49.4%, 11.3% y 39.4% respectivamente. La agrupación fue validada, al obtener para las 6 variables cuantitativas todos los ANVAs significativos y con el árbol de clasificación C5.0 un 99.35% de precisión.  Los resultados de la caracterización muestran que el clúster 1 son clientes con valores de los promedios para las 6 variables en un nivel intermedio, el 67.0% son hombres y 100% el tipo de tarjeta es classic. En el clúster 2 están los clientes con los valores más altos en los promedio de las 6 variables, el 59% son hombres y el 100% usan la tarjeta silver. En el clúster 3, se encuentran los clientes con los promedios más bajos, el 64% son hombres y el 100% usan tarjeta classic",2018-05-24
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Detección de outliers espaciales utilizando el diagrama de dispersión de Moran y el variograma Nube,"Palacios Mosquera, Maritza Sarela",Ingeniero Estadístico Informático,"Chue Gallardo, Jorge","Uno de los problemas del análisis del datos es la presencia de outliers, esto puede afectar las medidas estadísticas que se desean estimar de una población. La presente investigación se enfoca a la detección de los outliers pero en un contexto geográfico; para ello se empleó los datos obtenidos de la encuesta nacional de los egresados universitarios peruanos en el 2014. Como variable de estudio se consideró el ingreso total de los egresados universitarios en las diferentes regiones del pais para observar si existen ingresos muy atípicos respecto a una región a otra, o si dentro de una región existen valores muy altos respecto a su alrededor, estos valores anormales o raros dentro de un contexto geográfico se consideran como outliers espaciales que es muy diferente a los outliers tradicionales, para poder identificar dichos valores atípicos espaciales se empleó dos técnicas gráficas exploratorias para la detección de outliers espaciales: el variograma nube y el diagrama de dispersión de Morán, que tienen la particularidad de ser muy sensibles a la presencia de outliers espaciales. Se utilizó las dos pruebas los datos del ingreso total de la encuesta, se consideró una muestra 250 datos para su óptimo procesamiento, luego se logró detectar los outliers espaciales de la variable de la investigación que fue de ingreso total de S/7’400, y donde el variograma nube fue más sensible a la presencia de outliers que el diagrama de dispersión de Morán.",2018-05-23
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción metodológica de las series de tiempo con redes neuronales artificiales,"Neira Campos, Mike Alex",Ingeniero Estadístico e Informático,N/A,"Este trabajo monográfico gira en torno a las series de tiempo con Redes Neuronales Artificiales a fin de realizar pronósticos. Para este propósito, el presente trabajo se compone de 4 capítulos, donde el primer capítulo versa sobre las definiciones y conceptos principales del pronóstico de una serie temporal que otorga validez teórica a la investigación. En el segundo capítulo, el lector podrá encontrar la descripción de las Redes Neuronales Artificiales en la predicción de datos. El tercer capítulo, da cuenta de las implicaciones de una metodología del pronóstico de datos, utilizando las Redes Neuronales Artificiales. Finalmente, el capitulo 4, dilucida la aplicabilidad de las mencionadas Redes y se hace un paralelo con otros métodos de pronóstico con el objeto de resaltar sus diferencias y características. En conclusión, podemos decir que el lector podrá encontrar en este trabajo las etapas necesarias para llevar a cabo la elaboración de una red neuronal que pueda predecir valores futuros de una serie de tiempo.",2018-05-14
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción metodológica del modelo de ecuaciones estructurales con el método de estimación de mínimos cuadrados parciales,"Mamani Tone, Edith Rita",Ingeniero Estadístico e Informático,N/A,"La presente monografía estudia la Descripción Metodológica del Modelo de Ecuaciones Estructurales con el Método de Estimación de Mínimos Cuadrados Parciales. En el primer capitulo se describe detalladamente la sustentación teórica del Modelo de Ecuaciones Estructurales; en el capítulo 2 se describe el Método de Mínimos Cuadrados Parciales (PLS); en capítulo 3 se describe la Metodología detallando los pasos del Modelo de Ecuaciones Estructurales por el Método de Mínimos Cuadrados Parciales y en el capítulo 4 se observa la Aplicación, paso a paso en un caso para ejemplificar la metodología. Se concluyó los modelos de ecuaciones estructurales SEM es una extensión de la regresión múltiple, se aplica esta técnica para encontrar relaciones entre variables observables y no observables llamadas (latentes) para pasar posteriormente a estimar los parámetros. tiene como objetivo la predicción, no es preciso que los datos provengan de una distribución normal y puede aplicarse a estudios de muestras pequeñas, permite estimar modelos muy complejos con muchas variables latentes y medibles. En la aplicación de la descripción metodológica se realizó un estudio empírico el segundo semestre de 2013 sobre una muestra correspondiente a 300 alumnos universitarios chilenos con acceso a bases de datos científicas. Para los cálculos de PLS se utilizó el software WarpPLS 4.0. Los resultados del ejemplo en el análisis de PLS del caso indicaron la buena capacidad predictiva del modelo de investigación, y a su vez, la explicación del análisis logró ejemplificar en forma clara la metodología propuesta.",2018-05-14
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Mixtura finita basada en la distribución Birnbaum-Saunders normal asimétrica,"Maehara Aliaga, Rocío Paola",Ingeniero Estadístico Informático,"Vargas Paredes, Ana Cecilia","Los modelos de mixtura han recibido una gran atención en el área de estadística debido a la amplia gama de aplicaciones encontradas en los últimos años. Por otro lado el modelo Birnbaum-Saunders (BS) surgió en un contexto de fatiga de materiales. Este modelo ha sido aplicado en otras áreas como por ejemplo, ciencias de la salud, ambiental, forestal demográficas, actuarial, financiera, entre otras. Teniendo en cuenta que la distribución Birnbaum-Saunders Normal Asimétrica (BS-NA) es una extensión de la distribución BS, ya que permite predecir percentiles extremos especialmente en la cola izquierza y a su vez modelar datos asimétricos. Este trabajo discute el modelo de Mixtura Finita Birnbaum-Saunders Normal Asimétrica con G componentes, como una extensión del trabajo desarrollado por Benites et al. (2017), Vilca et al. (2011) y Balakrishnan et al. (2011). Esta propuesta es una clase flexible de distribuciones de probabilidad que permite modelar datos con comportamiento asimétrico, que poseen observaciones atípicas y que a su vez son provenientes de poblaciones heterogéneas. Para obtener los estimadores de máxima verosimilitud se usa el algoritmo EM con maximización condicional. Además, la matriz de información empírica se deriva analíticamente para obtener el error estándar. También se realizan estudios de simulación y analizan dos conjuntos de datos reales para ilustrar la utilidad del método propuesto. Finalmente, la propuesta del algoritmo y métodos son implementados en el programa R y posteriormente introducidos en el paquete bssn y en el portafolio GitHub",2018-05-08
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Estrategias de mercado en un centro educativo privado en la localidad de Arequipa,"Linares Torres, Miguel Angel",Ingeniero Estadístico e Informático,"Espinoza Villanueva, Luis Enrique","Se realizó una investigación cuantitativa, con el fin de identificar los factores más influyentes al momento de evaluar el servicio que ofrece el centro educativo, el instrumento de medición es la encuesta vía telefónica y se  entrevistó a los  padres de familia que matricularon a sus hijos en el año 2016. La recolección de información se realizó por medio de un cuestionario,  seguidamente se analizó en una base de datos y con procedimientos estadísticos se ejecutó pruebas para determinar  qué factores  son los más influyentes al momento de evaluar la calidad del servicio. Asimismo, se analizó el entorno como la infraestructura, calidad de servicio, equipamiento tecnológico y calidad educativa.  Finalmente con los resultados de las pruebas se procedió a elaborar estrategias de mercado para un óptimo posicionamiento",2018-05-02
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Descripción de la metodología de análisis de cluster con algoritmo Fuzzy C-means,"Flores Bellido, Giovanna",Ingeniero Estadístico Informático,N/A,"En el presente trabajo se presenta la metodología del algoritmo Fuzzy C-means para el análisis de cluster, el cual fue presentado por Bezdek y Dunn en 1973, la cual combina los métodos basados en la función objetivo con los de la lógica Fuzzy término presentado por Lofty Zadeh en 1960 como medio para modelar la incertidumbre a través de las etapas de fuzzificación, reglas de evaluación y defuzzificación. El algoritmo Fuzzy C-means realiza la formación de cluster a través de una partición suave de los datos, es decir para realizar del reconocmiento de patrones a través del hallazgo de los grados de pertenencia de cada individuo a los diferentes cluster, donde un individuo no tendría pertenecía exclusiva a un solo grupo, sino que un individuo podría tener grados de pertenencia a distintos grupos, a diferencia de otros métodos que realizan la formación de los cluster basados en la lógica binaria o partición dura. Utilizando el software estadístico R se realizó la aplicación del algoritmo Fuzzy C-means sobre datos de jugadores para la formación de cluster a través de rapidez y resistencia",2018-04-24
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Análisis de datos en una auditoría de mercado para productos de consumo masivo en bodegas de Lima Metropolitana,"Recuay Denegri, Briggitt Giuliana",Ingeniero Estadístico e Informático,"Gonzáles Chavesta, Celso","Ante un mercado cambiante las empresas del rubro de consumo masivo se encuentran sumergidas en una serie de interrogantes en cuanto a la participación de sus productos. Para ello aplican herramientas como la auditoría de mercado que a través del resultado de una de sus etapas el “análisis de datos” encuentran confianza a la hora de tomar decisiones. Este estudio presenta el proceso del análisis de datos en una auditoría de mercado para productos de consumo masivo en el canal tradicional. Los productos analizados fueron las cremas dentales, los desodorantes y los energizantes. Se trabajó con una muestra de 1200 bodegas ubicadas dentro de Lima Metropolitana. El conjunto de datos contiene variables cuantitativas como las ventas, las compras, los precios y los inventarios, además de las variables cualitativas como son los atributos (marca, tamaño, sabor, etc.) de las variedades. Los datos fueron recolectados de las bodegas el primer semestre del año 2016 vía celular mediante el aplicativo de relevamiento “Audit” para luego ser cargados al sistema de trabajo. Culminada cada fase del análisis de datos (limpieza de datos, análisis de las variables y visualización de resultados) se supervisaron las bodegas cuyas variedades presentaron incoherencias en sus datos y se procedió a corregir. Para los tres productos en su mayoría se encontraron: ausencia de datos en la variable compra e inventario y errores en los precios y ventas. Estos errores suelen suceder debido a una mala digitación (error de codificación) o a la omisión de parte del supervisor al momento de ingresar los datos al aplicativo. Como resultado del estudio se obtuvieron datos consistentes para las cremas dentales, desodorantes y energizantes, los cuales fueron de mucha relevancia para las empresas. Otros productos de consumo masivo pueden seguir también la misma estructura para el análisis de sus datos.",2017-12-27
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Análisis y diseño de un sistema distribuido de pago middleware orientado a la mensajería entre una entidad bancaria y una empresa farmacéutica,"Reyna Colona, Tino Fabricio",Ingeniero Estadístico e Informático,"Menacho Chiok, César Higinio","En este tiempo presente, las empresas desarrollan diversos planes de trabajo conjunto de negocio, a modo de corporaciones, joint ventures, etc., y surge la necesidad de manejo eficiente y ordenado de volúmenes grandes de datos e intercambio de información entre las corporaciones con los fines de negocio visibles en el ámbito comercial. En tal sentido, el trabajo de investigación provee una metodología de desarrollo de un sistema de integración basado en Middleware Orientado a la Mensajería (MOM) que media y favorece la integración financiera entre dos entidades de ejemplo, una empresa farmacéutica y un banco. El propósito del trabajo de investigación es que, con la metodología presentada, se identifiquen los procesos clave y la dinamización de los mismos para representar el flujo de transacciones electrónicas para la debida integración entre las dos entidades señaladas anteriormente. La metodología consta de las secciones de: análisis del sistema, diseño del sistema, arquitectura y secciones referidas a la interconexión, mensajería e interfaces gráficas de usuario para la debida gestión del sistema. Los resultados de la aplicación de esta metodología son: el reconocimiento de los procesos clave según los objetivos del trabajo, la presentación de la dinamización de tales procesos, y comprender de manera preliminar lo referido a la interconexión entre las entidades presentadas, empresa farmacéutica y banco, definición de mensajes para las transacciones financieras entre las entidades señaladas y la visualización de la gestión del sistema por medio de interfaces gráficas de usuario. Tales resultados conducen a la idoneidad de la metodología en todas las secciones presentadas, abriendo el espacio para posteriores oportunidades de investigación en banca electrónica.",2017-09-20
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Análisis del coste de los siniestros en una compañía de seguros utilizando las distribuciones asimétricas Skew-Normal y Skew-T,"Mendoza  Quevedo, Diego Alonso",Ingeniero Estadístico e Informático,"López de Castilla Vásquez, Carlos","La presente investigación tiene por objetivo principal determinar si las distribuciones asimétricas skew-normal y skew-tson buenos modelos para describir datos relativos al coste de los siniestros. Para lo cual se analizaron dos conjuntos de datos de una compañía de seguros, ajustando las distribuciones en estudio y las tradicionales a estos datos. Luego se compararon las distribuciones empleando el Criterio de Información de Akaike (AIC) y del Logaritmo de la función de Verosimilitud, complementados con la prueba de bondad de ajuste Kolmogorov-Smirnov,obteniendo resultados positivos. Además, se calcularon medidas de riesgo como el Valor en Riesgo (VaR) y el Valor en Riesgo Condicional (TVaR), que brindaron mayor solidez a los resultados previos: los costes de los siniestros en los seguros se ajustan bien a las distribuciones asimétricas skew-normal y skew-t.",2017-02-14
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Estimación de la incertidumbre asociado al método de ensayo para el análisis de ácido carmínico en cochinilla,"Morán Huamani, Violeta",Ingeniero Estadístico Informático,"Sotomayor Ruíz, Rino Nicanor","De acuerdo a la Norma NTP /IEC/ISO /17025 de Requisitos Generales relativos a la competencia de los laboratorios de ensayos y calibración pone especial énfasis en la necesidad de estimar la incertidumbre asociada con calibraciones internas. La presente monografía se describe un procedimiento para la estimación de la Incertidumbre en el método de ensayo en el análisis del Ácido Carmínico en Cochinilla. El cálculo de la Incertidumbre está basado en el Método de Evaluación (de incertidumbre) Tipo A y Método de Evaluación (de incertidumbre) Tipo B. Estos métodos consisten en la identificación y cuantificación de las diferentes fuentes de incertidumbre tanto internas como externas. Hallando la incertidumbre estándar de la calibración, incertidumbre estándar de la medida de la masa, incertidumbre estándar por deriva de la balanza. Se ilustran lo factores que más influyen en este tipo de análisis como son los equipos e instrumentos de medición utilizados; y la destreza de los ejecutores del ensayo, bajo los conceptos estadísticos como son la responsabilidad del método. De igual forma se presenta la descripción de la metodología utilizada donde se describe los parámetros metrológicos usados.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Clasificación de familias en Cajamarca según su situación económica mediante el análisis de conglomerados,"Vicente Vasquez, Juana Mercedes",Ingeniero Estadístico e Informático,N/A,"El estudio tiene como objetivo clasificar a las familias encuestadas en los distritos de San Pablo, San Luis y San Bernardino de la provincia de San Pablo en el departamento de Cajamarca según un conjunto de variables socio-económicas. Estos datos corresponden a una investigación realizada por un grupo de personas que laboran en la Universidad del Pacifico, la encuesta fue realizada en Diciembre del 2006. Se desea clasificar a las familias para poder brindar un mejor control en el estudio longitudinal de los proyectos a ser evaluados. Para esto, al culminar la encuesta se planteó una clasificación preliminarmente la existencia de 4 grupos de familias. Para verificar esta clasificación se utilizó el “Análisis de Clúster”, que es un método multivariado de clasificación. Para el procesamiento de los datos se utilizó el programa “Minitab versión 17” y “Microsoft Excel”",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Segmentación de usuarios de productos tecnológicos a partir de valores y actitudes,"Flores Espinoza, María del Carmen",Ingeniero Estadístico e Informático,N/A,"El presente trabajo tiene como propósito esencial, realizar una segmentación psicográfica, en base a valores y estilos de vida, así como las actitudes frente a la tecnología de los habitantes de Lima Metropolitana y Callao de los niveles socioeconómicos A, B, C y D, entre 15 y 69 años de edad. Se utilizó una muestra de 906 casos, a los cuales se le aplicó un cuestionario estructurado que contenía 35 atributos previamente definidos en base a una investigación cualitativa, y otras preguntas de control, tal como edad, nivel socioeconómico y tenencia de productos tecnológicos en el hogar. Del total de 35 atributos evaluados los cuales se redujeron en 8 factores tras realizar un análisis factorial. Seguidamente se realizó un análisis clúster, donde se identificaron 5 segmentos caracterizados por los atributos previamente mencionados: Aquellos que se encuentran Orientados a la tecnología; los que, a pesar de tener productos tecnológicos prefieren estar esconectados; los que se encuentran Orientados al poder, los que tienden a ser Tradicionales por sus actitudes frente a las costumbres, y aquellos Orientados a los valores que tienen como eje virtudes tales como la solidaridad, sencillez, etc.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Determinación de perfiles de turistas nacionales de los niveles socioeconómicos medio y alto mediante el análisis conglomerado bietápico,"Porras Huamán, Beatriz Eta",Ingeniero Estadístico e Informático,N/A,"El turismo en el Perú ha venido creciendo rápidamente en los últimos años MINCETUR - Plan Estratégico Nacional de Turismo 2012 - 2021 – PENTUR, pág. 11). En el proceso de la investigación turística se recopila los datos, para ser analizados y efectuar una crítica, con fines de alcanzar elementos de trabajo hacia otras fases de desarrollo turístico, las mismas que permitirán obtener nuevos conocimientos que como aporte se utilizarán en la actividad turismo.En este sentido el presente trabajo busca caracterizar o establecer cuáles son los perfiles de los turistas internos en el Perú de los niveles socioeconómicos medio y alto con la finalidad de diversificar la estrategia turística y de este modo poder administrar en forma sostenida recursos del sector público como el privado. Esta investigación utiliza una muestra de 2,400 turistas de los principales destinos turísticos del Perú. El objetivo planteado para el desarrollo del presente trabajo es la determinación de los grupos de individuos observando sus características socio-demográficas, los hábitos de vida y las preferencias con respecto a viajes al interior del país. La metodología empleada corresponde a la técnica de agrupamiento Análisis de Conglomerados Bietápico. La pregunta de investigación planteada para el desarrollo del proyecto se orienta a determinar los diversos tipos de perfiles de turistas nacionales en los niveles socioeconómicos A, B y C. Finalmente se determinó la homogeneidad de los grupos en función a la variabilidad intra-grupos, encontrándose dos grupos, descritos estos en función a los estadísticos encontrados dentro de cada grupo.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,"Modelación de la volatilidad del índice general de la Bolsa de Valores de Lima, periodo 2009-2011","Castillo Gamarra, Jorge Enrique",Ingeniero Estadístico e Informático,"Sotomayor Ruíz, Rino Nicanor","El presente trabajo tiene como objetivo describir los modelos de varianza condicional ARCH y GARCH junto con sus propiedades y demostraciones, estos modelos se aplican en series de tiempo financieras, debido a que estas presentan como característica principal una fuerte volatilidad con periodos de calma o agitación, lo cual no permite utilizar los modelos de series de tiempo tradicionales que asumen varianzas constantes. Así mismo se realizó una aplicación utilizando como variable el valor diario del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2009 – 2011, para la aplicación  se utilizó el software econométrico Eviews 7. Al analizar los resultados de las estimaciones de los modelos que explicarían la volatilidad diaria de la Rentabilidad del Índice General de la Bolsa de Valores (RIGBVL), periodo 2009 – 2011, se  concluyó que  el  modelo GARCH (1,1) es el adecuado, debido a que el modelo GARCH (1,1) tiene a diferencia de los demás modelos el menor valor tanto en el criterio de información de Akaike (AIC) como en el criterio de información de Schwarz. Previamente se modeló la media de la RIGVBL con el modelo AR (1)",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de perfiles de clientes crediticios aplicando técnicas de segmentación y regresión logística multinomial,"Ramírez Soplin, Magally Loidit",Ingeniero Estadístico e Informático,N/A,"El presente estudio de investigación se centró en identificar los perfiles más adecuados, en una muestra de 8, 504 clientes que realizaron transacciones crediticias en el primer trimestre del año. Se agruparon los casos mediante las técnicas de segmentación: K-means, Bietápico y Kohonen, utilizando variables cuantitativas y categóricas. De las tres técnicas, la que obtuvo mayor medida de silueta de cohesión y separación, fue K-means, indicando una estructura “buena” en cuanto a la cohesión al interior de los grupos y la separación de los mismos. Por otro lado, también se analizó las proporciones de los conglomerados, siendo la técnica K-means la que presentó las proporciones más adecuadas en función a las variables de historial crediticio y transacciones realizadas. Posterior a la obtención de los conglomerados, se procedió al proceso de obtención de la reglas de clasificación, mediante la técnica de regresión logística multinomial, la cual nos permitirá realizar predicciones futuras. El procedimiento se aplicó a la muestra particionada, es decir, una parte de entrenamiento y otra de comprobación. Finalmente, se obtuvo una adecuada tasa de eficiencia en ambas muestras. Además, los análisis permitieron identificar a dos conglomerados que muestran una alerta para la empresa, es decir necesitan ser gestionados de forma oportuna, ya que constituyen un futuro comportamiento de no pago de acuerdo a la caracterización obtenida de dichos conglomerados.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Predicción del rendimiento en el exámen de admisión a la UNALM [Universidad Nacional Agraria La Molina] utilizando las técnicas de análisis discriminante lineal y análisis discriminante con algoritmos genéticos,"Rado Huaringa, Joao Manuel",Ingeniero Estadístico e Informático,"Rosas Villena, Fernando René","El objetivo de la investigación fue probar la hipótesis que la tasa de error de clasificación utilizando  el análisis discriminante con algoritmos genéticos es menor a la que se obtiene con el análisis discriminante lineal de Fisher. La aplicación se efectuó en la predicción del rendimiento en el examen de admisión de la Universidad Nacional Agraria La Molina de los postulantes cuya preparación se realizó en su Centro de Estudios Preuniversitarios. En la técnica de algoritmos genéticos  se empleó el método de selección, cruce y mutación que permitió realizar la búsqueda de funciones discriminantes con error mínimo. Los resultados del estudio indican que el análisis discriminante con algoritmos genéticos proporcionó una función discriminante más eficiente que la proporcionada por Fisher.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Perfil de los clientes que aceptan una tarjeta de crédito de un banco via Call Center utilizando el algoritmo Chaid exhaustivo,"Acosta Pizarro, Diana Rosa",Ingeniero Estadístico e Informático,N/A,"El presente estudio tiene como objetivo principal identificar el perfil de los clientes del departamento de Lima que aceptan una tarjeta de crédito de una entidad financiera cuando el productoes ofrecido por el canal de ventas Call Center. Se utilizó la técnica de Árboles de Clasificación CHAID Exhaustivo el cual proporciona buenos resultados de clasificación correcta de los clientes que aceptan una tarjeta de crédito vía Call Center. Se consideró una muestra en un período de cinco meses (Diciembre 2013 a Abril 2014) logrando identificar que las variables más significativas que aportan en el modelo son la edad, el ingreso neto mensual y el tipo de tarjeta que se le ofrece al cliente. Estas variables presentan importancia relevante en el cliente para tomar la decisión de aceptar una tarjeta de crédito. Los resultados obtenidos mediante el algoritmo CHAID Exhaustivopermitieron identificar los patrones que definen el perfil de los clientes que aceptan una tarjeta de crédito vía Call center con el fin de ser más efectivos, aumentando el número de ventas, reduciendo el número de llamadas, minimizando costos y tiempo",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Identificación de un modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria aplicando regresión logística y árboles de clasificación CART,"Huamaní Miranda, María Alejandra",Ingeniero Estadístico e Informático,N/A,"La realidad competitiva que en estos días enfrentan las entidades bancarias ha provocado que éstas no sólo concentren sus esfuerzos de marketing exclusivamente en estrategias de captación de clientes, sino también en estrategias de retención y ﬁdelización; la fuga de clientes es una situación que afecta la rentabilidad de la gran mayoría de las instituciones bancarias dado que se invierte mucho más en la captación de clientes que en campañas para la retención, por ello, es un tema de intensivo estudio cientíﬁco en los últimos años. Las entidades bancarias requieren contar con herramientas que les permitan estimar probabilidades de fuga para su cartera de clientes y así decidir sobre que clientes concentrar sus esfuerzos de retención. En el presente trabajo se utilizó la regresión logística de respuesta binaria  y el algoritmo de árbol de clasificación CART para predecir y clasificar a los clientes con riesgo de fuga y así identificar el mejor modelo explicativo de retención de clientes con riesgo de fuga para una entidad bancaria. El modelo que mejor explica el riesgo de fuga de un cliente fue la Regresión Logística binaria que obtuvo como variables predictoras número de transacciones, ingreso bruto, número de tarjetas usadas y línea de crédito. Las variables identificadas permitirán a la entidad bancaria reorientar las estrategias en las campañas de retención de clientes.",2017-01-13
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Uso del criterio AHP para la toma de decisiones,"Loaiza Alamo, Marco Antonio",Ingeniero Estadístico e Informático,"Sotomayor Ruíz, Rino Nicanor","El objetivo de esta investigación es determinar la mejor selección de los siete programas especializados para la implementación de un laboratorio mediante la aplicación del Análisis de Proceso Jerárquico (AHP). Con esta técnica se logró un consenso para identificar cuáles son los criterios y las alternativas más relevantes para la toma de decisiones. Para validar el AHP se necesitó los Índices de Consistencia: el Cociente de Resistencia (CR) para la matriz de comparación de criterios y alternativas por pares, el Índice de Consistencia Geométrica (GCI) y el Indicador de Consenso AHP (S*) para la matriz consolidada. Con el desarrollo del algoritmo de AHP se optó por cinco criterios y cuatro alternativas para la toma de decisión. Se concluyó que la alternativa A, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y Microsoft Project, fue la más importante. No obstante, no hubo diferencia significativa considerable con la alternativa C, conformada por los programas informáticos Minitab, SPSS, SQL, Eviews y QlikView. En base a los resultados obtenidos se concluye que los programas más adecuados para la implementación del laboratorio informático son: Minitab, SPSS, SQL, Eviews, Microsoft Proyect y QlikView",2017-01-10
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Muestreo de unidades agrícolas a través de puntos fijados aleatoriamente,"Alarcón Novoa, Jorge Alfonso",Ingeniero Estadístico,"Rubio Donet, Arturo","En esta investigación se presenta un método de selección de unidades de análisis a través de puntos fijados aleatoriamente en un mapa como marco de muestreo, el cual puede ser utilizado en estudio de muestreo que presentan limitaciones drásticas sobre la inexistencia de padrones o listas de las unidades de muestreo. Asimismo, los parámetros son estimados a través de funciones que respetan las probabilidades asignadas a las unidades seleccionadas, consignando sus indicadores de confiabilidad respectivos. Finalmente, son estimados tamaños de muestra alternativos para futuros trabajos que pudieran implementarse en el área de aplicación. La presente metodología propuesta tiene su aplicación experimental en el Valle de Cañete para lo cual se estima: - Superficie total sembrada con papa en la Campaña 1978 - Cantidad total de semilla de papa utilizada en la Campaña 1978 - Producción total de papa obtenida en la misma Campaña",2016-08-17
Universidad Nacional Agraria La Molina. Facultad de Economía y Planificación,Modelo predictivo de quiebre de stock en un supermercado comparando dos métodos de selección de variables,"Montaño Miranda, Beatriz del Carmen Lidia",Ingeniero Estadístico e Informático,N/A,"El presente estudio tuvo como objetivo principal verificar la disponibilidad de productos en el almacén de un Supermercado, así las decisiones a tomar ante la falta de productos serían más certeras y se tendría un mejor panorama al respecto de la situación del abastecimiento del Supermercado. El trabajo consiste en un modelo de predicción de quiebres de stock para un supermercado. Se analizaron las ventas en unidades, el stock de los productos, los despachos de proveedores, los días del mes de Agosto (con ventas y sin ellas) de un grupo de productos de diferentes gerencias del supermercado (Abarrotes Comestibles, Abarrotes no Comestibles y Bebidas). Con la información recopilada se consideró el valor de la variable dependiente (en quiebre con valor 1 y O en caso contrario). La selección de variables por Boruta permitió obtener un modelo con menos cantidad de variables y con un mejor ajuste al realizar el análisis usando la regresión logística en comparación con la selección de variables por Stepwise.",2016-07-07
Universidad Nacional Agraria La Molina. Escuela de Posgrado,"Clasificación de la eficiencia del gasto público en las regiones del Perú aplicando conglomerados de series temporales, 2007 - 2019","Romero Cuadros, Italo Brayan",Magister Scientiae - Estadística Aplicada,"Gamboa Unsihhuay, Jesús Eduardo","Esta investigación tiene como objetivo de determinar los patrones de clasificación del desempeño del sector público mediante la técnica de análisis conglomerados de series de tiempo a las regiones del Perú para el periodo 2007 – 2019. El desarrollo se realizó con los indicadores creados bajo la metodología de la frontera de posibilidades de producción y se evaluó la asociación a través del tiempo con el método de distancia de deformación del tiempo. El indicador utilizado para decidir el número de los conglomerados fue el de Silueta y Calinski. Entre los resultados más importantes se encontró que se mantiene un patrón que predomina entre las regiones que mejor usan los recursos y obtienen resultados idóneos donde resalta Moquegua, Ica y Lima mientras que otro grupo que mantiene fuertes tendencias a ser ineficientes en comparación como Ayacucho y Huancavelica. Finalmente, se observó una predominancia en las regiones que subdivide entre dos conglomerados y se mantiene tanto sectorial y global.",2022-10-04
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Factores que determinan la calidad de servicio y su relación con la satisfacción estudiantil universitaria estatal utilizando ecuaciones estructurales,"Chumpitaz Ramos, Domingo Guzmán",Magister Scientiae - Estadística Aplicada,"Sotomayor Ruíz, Rino Nicanor","Los Modelos de Ecuaciones Estructurales (SEM), es una extensión de varias técnicas multivariantes, entre ellas el Análisis Factorial, se ha utilizado casi en todos los campos de estudio, principalmente en el área de la educación, esta técnica nos proporciona un método directo para tratar múltiples relaciones de variables observables y no observables. El objetivo principal de la investigación fue determinar la relación entre la calidad de servicios con los factores que la determinan (tales como la seguridad, fiabilidad, empatía, aspectos tangibles y capacidad de respuesta); y la satisfacción estudiantil universitaria utilizando Modelos de Ecuaciones Estructurales (SEM). En la investigación se diseñó un cuestionario en base al instrumento de calidad de servicio SERVQUAL relacionado a un modelo estructural teórico del autor Lobos y Sepúlveda. Se hizo un análisis de fiabilidad del cuestionario, y con el análisis factorial exploratorio se demostró su validez. Luego, se recolectó información de 158 estudiantes del Área de Ciencias Económicas y de la Gestión de la UNMSM. Para desarrollar la investigación se aplicó el Análisis Factorial Confirmatorio (AFC) y Modelos de Ecuaciones Estructurales (SEM). En el modelo estructural inicial, los constructos empatía y fiabilidad no fueron significativos, entonces se reespecificó el modelo con los índices de modificación. En el modelo estructural 2, las variables observadas X5, X6 y X12 no fueron significativas en los constructos de fiabilidad y empatía, por lo tanto se retiraron del modelo. En el modelo estructural reespecificado 3, empatía no fue significativo, entonces no fue considerado en el siguiente modelo. En el modelo estructural final los constructos de seguridad, fiabilidad y aspectos tangibles fueron significativos con un nivel de significancia de 0.05. La calidad de servicio esperada tiene una relación directa con la satisfacción estudiantil y, los constructos de seguridad, fiabilidad y aspectos tangibles están relacionados positivamente con la calidad de servicio esperada.",2021-04-05
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Influencia de la violencia contra las mujeres en la productividad laboral de microempresas utilizando ecuaciones estructurales con mínimos cuadrados parciales,"Asencios Gonzalez, Zaida Beatriz",Magister Scientiae - Estadística Aplicada,"Porras Cerrón, Jaime Carlos","En la presente investigacion, se empleo Modelos de Ecuaciones Estructurales con Minimos Cuadrados Parciales (PLS-SEM por sus siglas en ingles), es una tecnica de segunda generacion que utiliza metodos estadisticos para el analisis simultaneo de relaciones complejas entre dos a mas constructos latentes. Se aplico PLS-SEM con el objetivo principal de comprender como o por que medios la violencia contra las mujeres en relaciones de pareja (VcM, constructo o variable independiente) afecta a la productividad laboral (constructo o variable dependiente) medido en terminos de ausentismo y presentismo, y la explicacion de esta relacion es por medio del dano a la salud mental y fisica (constructo o variable mediadora). Para ello, se entrevistaron a 357 duenas de microempresas formales en 10 departamentos del Peru y a 977 duenas de microempresas informales o formales con acceso a credito en Paraguay, se aplico un cuestionario estructurado cuyas preguntas estuvieron medidas en escala ordinal. En el modelo de media, los resultados del PLS-SEM muestran que tanto en Peru y Paraguay los tres constructos analizados son validos y confiables, fundamentados por la fiabilidad compuesta, las cargas de los indicadores, la varianza extraida media (AVE), las cargas cruzadas, el criterios de Fornell Larcker y el Heterorrasgo-Monorrasgo (HTMT). En cuanto al modelo estructural, tanto en Peru como en Paraguay los hallazgos proporcionan evidencia empirica de que el dano a la salud mental y fisica explica la relacion entre VcM y productividad laboral. Por consiguiente, la presente investigación posee implicancias por la confirmacion del efecto mediador, la aplicacion de esta tecnica en este tipo de tematica y el desarrollo del marco teorico y practico del PLS-SEM.",2019-12-30
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Aplicación de los modelos de ecuaciones estructurales a las empresas del sector artesanal peruano,"Chafloque Cespedes, María Raquel",Magister Scientiae - Estadística Aplicada,"Miranda Villagomez, Clodomiro Fernando","Los Modelos de Ecuaciones Estructurales con Mínimos Cuadrados Parciales (PLS – SEM, por sus siglas en inglés), son un método de segunda generación, con gran aceptación en la actualidad en el mundo académico, en especial en el área de ciencias empresariales. Asimismo, el enfoque de esta técnica es más robusto y flexible al momento de utilizarlo en variables no observables. La presente investigación muestra una aplicación del PLS – SEM dentro del área de ciencias empresariales, en un sector económico donde no existe evidencia empírica cuando se habla de estrategias de marketing y desempeño empresarial. La investigación tuvo como objetivo determinar la relación entre la orientación de mercado, la innovación del producto y el desempeño de las empresas en el sector artesanal peruano - periodo 2018 mediante la aplicación del PLS-SEM. Se aplicó una encuesta estructurada a 301 microempresas del sector artesanal, específicamente las que están en el rubro comercial, siendo estas las principales intermediarias entre el consumidor final y el productor. Se encontró que la orientación de mercado y la innovación del producto explican el 34.3% del desempeño de la empresa; así mismo la orientación de mercado se relaciona positivamente a la innovación del producto, y esta última variable se relaciona positivamente al desempeño de la empresa. Se concluye que la técnica de PLS – SEM es adecuada para ser aplicada a investigaciones de ciencias empresariales. Finalmente, se recomienda que se incremente la evidencia empírica con el fin de fomentar el uso de esta técnica estadística de segunda generación.",2019-12-23
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Comparación del pronóstico de riesgo de crédito utilizando regresión binaria asimétrica cloglog y perceptrón multicapa,"Valdivia Carbajal, Manuel",Magister Scientiae - Estadística Aplicada,"Chue Gallardo, Jorge","Esta tesis toma como caso de estudio a una empresa de cosméticos reconocida de la ciudad de Lima, Perú. Para pronosticar el riesgo de crédito se analizaron dos modelos: la Regresión Binaria Asimétrica Cloglog y las Redes Neuronales Artificiales Perceptrón Multicapa. La selección de estos modelos surge a raíz de recientes estudios que revelan las ventajas de las técnicas de inteligencia artificial sobre los modelos estadísticos en cuanto a predicción por su alta capacidad de discernimiento de patrones. “La empresa” cuenta con un modelo de negocio llamado Red Binaria, esto quiere decir que se contrata vendedoras y éstas ofrecen productos a sus clientes a través de catálogos. Debido a que no se cuenta con información de los clientes finales, se midió la probabilidad de no pago a través de las vendedoras. La población de estudio estuvo conformada por las vendedoras de la empresa las cuales manejan una cartera de clientes de 51183 personas a julio del 2017. Los datos se trataron previamente considerando el análisis de valores atípicos a nivel univariado y multivariado, este último mediante el algoritmo de segmentación K-means. Concluido ello para realizar la clasificación de vendedoras en buenas y malas pagadoras se utilizó un modelo de Redes Neuronales Artificiales Perceptrón Multicapa con una sola capa intermedia y un modelo de regresión Binaria sobre el cual se eligió el enlace asimétrico Cloglog debido a la naturaleza de los datos. Los resultados mostraron un 0.846 y 0.809 de índice ROC en las muestras de entrenamiento, y un 0.762 y 0.733 de índice ROC en las muestras de testeo respectivamente para cada modelo. Finalmente, se concluye que la aplicación de la técnica de Redes Neuronales Perceptrón Multicapa define una mejor regla de discriminación que la Regresión Binaria Asimétrica Cloglog en el estudio de probabilidad de impago. Además, las Redes Neuronales presentan mejores indicadores de pronóstico.",2019-11-25
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Equiparaciòn de puntuaciones en el examen de admisión de la Universidad Nacional Agraria La Molina utilizando los métodos lineal y equipercentil,"Rado Huaringa, Joao Manuel",Magister Scientiae - Estadística Aplicada,"Porras Cerrón, Jaime Carlos","En esta investigación se realizó la aplicación de los métodos de equiparación lineal y equipercentil a los puntajes obtenidos de los postulantes a los exámenes de admisión 2016-I y 2016-II de la Universidad Nacional Agraria La Molina. El desarrollo se realizó en las seis áreas que se evalúan en el examen de admisión: Razonamiento Verbal, Razonamiento Matemático, Matemática, Física, Química y Biología. El indicador utilizado para comparar ambos métodos fue el error estándar de equiparación. Entre los resultados más importantes se encontró que el método de equiparación lineal tuvo un mejor ajuste que el método equipercentil. Respecto a la dificultad de los exámenes de admisión, se obtuvo que el examen 2016-II presentó una mayor dificultad que el examen 2016-I. Finalmente, en relación a las seis áreas evaluadas en los exámenes, fue Matemática la que presentó una mayor dificultad en el examen de admisión 2016-II que en el 2016-I.",2019-09-24
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Modelo de ecuación estructural explicativo del rendimiento académico de los estudiantes del curso de estadística general en la UNALM,"Salazar Vega, Rolando Jesús",Magister Scientiae - Estadística Aplicada,"Rosas Villena, Fernando René","El propósito principal de la investigación fue comprobar si el rendimiento académico de los estudiantes en el curso de Estadística General de la Universidad Nacional Agraria La Molina (UNALM) es explicado a través de un modelo propuesto de Ecuación Estructural de tres factores. El primero denominado “desempeño docente”, medido por las variables: planificación del curso, dominio del curso, métodos y recursos de instrucción, obligaciones docentes, método evaluativo, y motivación e interacción con los alumnos; el segundo llamado “autoconcepto”, medido por las variables: académico/laboral, social, emocional, familiar y físico y finalmente el tercero “rendimiento pasado”, medido a través del promedio ponderado acumulado. Los datos utilizados corresponden a las notas de los alumnos matriculados en el ciclo académico 2014-I en el curso de Estadística General, al promedio ponderado acumulado; y los valores se registraron en la escala de Likert de 1 al 10 de las encuestas de desempeño docente y autoconcepto. Estos dos instrumentos, cumplen con los requisitos de confiabilidad y validez al registrar en ambos casos indicadores por encima de los mínimos aceptables. El modelo de ecuación estructural propuesto fue reespecificado (mejorado) mediante la inclusión de una nueva relación de interdependencia, el rendimiento pasado como predictor del autoconcepto. Se verificó el ajuste del modelo de ecuación estructural reespecificado a través de los principales indicadores de ajuste absoluto e incremental. Entre los resultados más importantes de la investigación se verificó que el factor rendimiento pasado es el mejor predictor del factor rendimiento académico de los estudiantes en el curso de Estadística General y que los factores desempeño docente y rendimiento pasado explican al factor autoconcepto.",2019-07-24
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Uso de los modelos heterocedásticos con Bootstrap en el análisis del Índice General de la Bolsa de Valores de Lima,"Orosco Gavilán, Juan Carlos",Magister Scientiae - Estadística Aplicada,"Febres Huamán, Grimaldo","La presente investigación es de naturaleza aplicada, y tiene el objetivo de analizar y evaluar la metodología Bootstrap en modelos heterocedásticos aplicados en la predicción del Índice General de la Bolsa de Valores de Lima (IGBVL), periodo 2010 - 2014. Se presenta sucintamente, los conceptos básicos de series temporales, los procesos seriales heterocedásticos, la metodología Bootstrap y sus aplicaciones a la inferencia estadística y a las series temporales, donde se presenta el algoritmo para procesos heterocedásticos GARCH propuesto por Pascual et al. (2006) y generalizados para los modelos EGARCH y TGARCH. Con los procedimientos mostrados fueron obtenidas las predicciones mediante la metodología paramétrica y metodología Bootstrap, que fueron comparados con valores reales y finalmente fueron evaluados los desempeños de ambas metodologías. Del estudio se obtuvo que los modelos que mejor ajustan a la serie son los modelos ARMA(1,1)-GARCH(1,1), ARMA(1,1)-EGARCH(1,1) y ARMA(1,1)-TGARCH(1,1) cada uno de ellos con el supuesto de distribución t de Student con 5 grados de libertad de los residuales, el estudio comparativo mostró que la aplicación de la metodología Bootstrap en la serie de los retornos del Índice General de la Bolsa de Valores de Lima, permite obtener intervalos de predicciones con mayores e iguales amplitudes en algunos horizontes hacia adelante en comparación con la metodología paramétrica, y también permitió construir con un buen desempeño los intervalos de predicción para las volatilidades, así siendo esta una alternativa para la construcción de intervalos de predicción en los modelos GARCH, EGARCH y TGARCH.",2019-04-12
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Clasificación de especies forestales maderables de la Amazonía Peruana aplicando análisis Clúster con algoritmo Clara,"Montenegro Muro, Rolando Antonio",Magister Scientiae - Estadística Aplicada,"Acevedo Mallque, Moisés Pascual","El bosque amazónico cuenta con una gran variedad de especies arbóreas, la cual se estima en cuatro mil especies. Unas pocas especies amazónicas enfrentan la desaparición por la alta demanda de su madera. Para reducir la presión sobre las especies demandadas y promover el uso de nuevas especies es necesario conocer sus características tecnológicas, principalmente, las características físico mecánicas. Debido a la gran cantidad de propiedades que encierran dichas características, se propuso estudiar a las especies con técnicas multivariadas, específicamente a través del análisis de conglomerados. Ello con la finalidad de agruparlas en función a la similitud que tengan en sus propiedades físicas y mecánicas. Así, se pueden agrupar especies poco conocidas en el mercado con especies muy demandadas y sugerir potenciales usos. Para el estudio presente se utilizó el algoritmo CLARA (Clustering Large Applications), el cual es empleado en grandes conjuntos de datos. Para seleccionar el número de conglomerados óptimo se probó hacer de dos hasta diez grupos; luego se comparó el ancho de la silueta promedio y el índice de Dunn por grupo y se eligió el de valores más altos. Se encontró que con un ancho de la Silueta promedio de 0,339 el número óptimo de conglomerados es de dos. El número de conglomerados indicado coincide con el análisis realizado a partir del índice de Dunn, el cual alcanza su más alto valor en 0,1264 con dos clústeres. Los conglomerados tuvieron como medóides a Guarea subridiflora (“requia de altura”) y Retrophyllum tospigliosii (“ulcumano). El primer conglomerado se caracterizó por tener propiedades mecánicas y físicas altas, de acuerdo a lo establecido por Aróstegui et al (1986). Por otro lado, el conglomerado de medóide “ulcumano” se caracterizó por tener propiedades físico mecánicas bajas, a excepción del clivaje, el cual resultó ser medio.",2018-12-27
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Detección de datos multivariados atípicos con series finitas de Fourier,"Rubio Donet, Jorge Luis",Magister Scientiae - Estadística Aplicada,"Miranda Villagomez, Clodomiro Fernando","La presencia de observaciones atípicas en un conjunto de datos es una de las causas que generan distorsiones en el análisis. La detección de dichas observaciones puede ayudar a una correcta evaluación de las tendencias en el comportamiento de los datos. Para el caso de datos multivariados se han desarrollado diversos métodos que permiten la detección de comportamientos atípicos, basados en métodos gráficos, y otros asumiendo una distribución normal multivariada. No obstante, en muchos casos el supuesto de normalidad multivariada no se cumple. El presente trabajo propone una prueba no paramétrica basada en la aplicación del método Bootstrap, utilizando como indicador de similitud a las distancias entre las representaciones obtenidas con series finitas de Fourier, propuesta por Andrews. El método propuesto permite detectar datos multivariados atípicos, combinando la significación estadística de la prueba Bootstrap y el análisis gráfico sugerido por Andrews, y que puede ser también aplicado a datos medidos en una escala ordinal. El método fue aplicado a cuatro conjuntos de datos, encontrando resultados satisfactorios en todos los casos.",2018-11-06
Universidad Nacional Agraria La Molina. Escuela de Posgrado,"Comparación del análisis discriminante no métrico, árboles de clasificación Chaid y la regresión logística multinormal","Sucari Sucari, Ruben Elvis",Magister Scientiae - Estadística Aplicada,"Porras Cerrón, Jaime Carlos","En la presente tesis se desarrolló el método de clasificación llamado Análisis Discriminante No Métrico, y se comparó su desempeño con el Árbol de Clasificación CHAID y la Regresión Logística Multinomial, los cuales también son métodos que no necesitan la condición de normalidad multivariada, linealidad ni varianza homogénea para las variables independientes. Esta comparación de desempeño fue evaluado mediante la Validación Cruzada. Para la realización del estudio comparativo de estos clasificadores se utilizó conjuntos de datos que son proporcionados por la Universidad de California Irving (UCI). Se concluye que la Regresión Logística Multinomial tiene mejor desempeño en la clasificación de datos teniendo en cuenta la tasa de clasificación promedio y el tiempo de procesamiento",2018-05-03
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Predicción de fuga de clientes en una empresa de telefonía utilizando el algoritmo Adaboost desbalanceado y la regresión logística asimétrica,"Meza Rodríguez, Aldo Richard",Magister Scientiae - Estadística Aplicada,"Chue Gallardo, Jorge","La presente investigación tiene como propósito aplicar y comparar el modelo de regresión logística y el algoritmo Adaboost en datos desbalanceados, esto a efecto de predecir la fuga  de clientes en una empresa del sector de telefonía móvil. El algoritmo Adaboost se sustenta  en el aprendizaje adaptativo al entrenar clasificadores débiles combinándolos en conjunto  para obtener un clasificador cuyo rendimiento sea fuerte. En cuanto a la regresión logística su modelamiento se realizó estrictamente desde una perspectiva de minería de datos, donde la clasificación es el objetivo y el rendimiento se evaluó en un conjunto de validación. Ambas técnicas se compararon mediante dos procedimientos, el primero mediante métodos de muestreo (sub-muestreo, sobre-muestreo y SMOTE) y el segundo modificando y/o  ajustando el algoritmo o función. Al trabajar con datos desbalanceados la tasa de error de clasificación es ineficiente, por lo que las medidas de desempeño para elegir al mejor modelo fueron la precisión, el recall (sensibilidad), el F-measure, y como medida principal el AUC a través de curvas ROC. Al formar modelos logísticos con los métodos de muestreo, las medidas de desempeño arrojaron resultados similares, lo mismo pasó al formar modelos con el algoritmo Adaboost, sin embargo al comparar la regresión logística (AUC=0.86) con el algoritmo Adaboost (AUC =0.93), este último tuvo el mejor desempeño. En cuanto al ajuste a nivel de algoritmo o función, en la regresión logística se trabajó de dos maneras, el primero (Logit Asym) incluyendo en la FDA un valor Kappa (k) y el segundo (Power Logit) un valor Lambda (λ), en ambos modelos se identificaron los valores óptimos de k (0.02) y λ (2.5), en cuanto al algoritmo Adaboost (Adaboost Asym) se ajustó el peso de la clase minoritaria cuyo costo de clasificación fue errónea. La comparación de estos tres modelos ajustados dio como mayor rendimiento al algoritmo Adaboost. Finalmente se realizó la validación cruzada con 10 iteraciones para todos los modelos dando resultados similares al método de retención. Realizada todas las comparaciones y las medidas de desempeño se concluye que el modelo óptimo para la predicción de fuga de clientes en la empresa de telefonía es el algoritmo Adaboost",2018-04-25
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Identificación de conglomerados en el grado de coautorias formado por las instituciones peruanas con investigación en medicina indizada en Scopus,"Málaga Sabogal, Lucía",Magister Scientiae - Estadística Aplicada,"Miranda Villagomez, Clodomiro Fernando","Este estudio halló los grupos de investigación conformados por las instituciones peruanas con investigación en medicina indizada en Scopus en base a las coautorías. La información se descargó de Scopus en formato no estandarizado y se utilizó aprendizaje supervisado con k-medias y un conjunto de datos de entrenamiento, para la identificación de las instituciones involucradas. El procesamiento de los datos se hizo con R. Las instituciones identificadas se clasificaron en ocho categorías: universidades, institutos públicos de investigación, clínicas y hospitales, organismos y dependencias del gobierno nacional, organismos y dependencias del gobierno local, empresas, organizaciones internacionales con filiales en Perú, instituciones privadas sin fines de lucro; y dos sectores: público y privado. Posteriormente se identificó los conglomerados existentes utilizando la metodología de particionamiento jerárquico aglomerativo propuesta por Moore, Clauset y Newman e implementada en el paquete igraph en R. Se halló que las instituciones del sector salud tienden a colaborar con sus símiles pero que no existe relación entre el tipo y sector de la institución y los patrones de colaboración para otras instituciones",2018-04-09
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Aplicación del muestreo para estimar la desnutrición crónica en escolares de 6 a 9 años en la Región Callao,"Bazán Baca, Juan Francisco",Magister Scientiae - Estadística Aplicada,"Gonzáles Chavesta, Celso","El estudio ha tenido como propósito determinar el método de muestreo apropiado, así como realizar estimaciones de la desnutrición crónica (DC) por edades, sexo y gestión educativa. Se ha empleado el muestreo por conglomerados (escuelas) combinado con estratificado (gestión educativa) y selección sistemática; en una muestra de 16 escuelas de gestión estatal y 20 de no estatal, obteniéndose información de 3000 alumnos del primero al cuarto grado de primaria, de la Región Callao. Se analizó la relación talla/edad con la referencia del National Center for Health Statistic (NCHS) de los Estados Unidos de América y de la Organización Mundial de Salud (OMS) considerando como niño en DC aquel con talla debajo de menos dos desviaciones estándar. Se determinó que la talla promedio de los alumnos, tuvo un incremento de 3 cm. al 2016, respecto al año 2005. La DC el año 2016 alcanza el 2.5% (1455 alumnos) con el patrón del NCHS y 2.9% (1724 alumnos) con el de la OMS. Según la referencia NCHS, la DC baja en 4.1% el año 2016 (2.5%) respecto al año 2005 (6.6%). En este período, también baja la DC por edades, sexo y gestión de la institución educativa. El Odds Ratios de Prevalencia (ORP) con el patrón NCHS por sexo y gestión disminuye el año 2016 respecto al año 2005 a 1.27 y 2.29 respectivamente; resulta que un estudiante este desnutrido crónico es 1.27 veces más probable si es hombre a que sea mujer, y que un estudiante este desnutrido crónico es 2.29 veces más probable si estudia en una institución educativa estatal a que estudie en una no estatal. Para tener alrededor del 96% de casos válidos, para las edades de 6 a 9 años en la encuesta de talla a escolares del primero al cuarto grado de primaria de menores, es recomendable efectuarla en el primer semestre del año, preferentemente los meses de mayo y junio",2018-03-27
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Análisis comparativo de la teoría clásica de los test y la teoría de respuesta al ítem aplicadas en evaluaciones informatizadas,"Cano Alva Trinidad, Jesús María",Magister Scientiae - Estadística Aplicada,"Menacho Chiok, César Higinio","Las instituciones de educación superior, cada vez más están implementando en sus diversos cursos evaluaciones virtuales vía web, con la finalidad de automatizar y medir con mayor precisión los conocimientos que van adquiriendo los estudiantes. La presente investigación, tiene como objetivo realizar un estudio comparativo de la Teoría Clásica del Test y la Teoría de Respuesta al Ítem, que pueden ser aplicados a los test informatizados con la finalidad de evaluar y medir las propiedades psicométricas y estadísticas, enfocando la confiabilidad y validez de los test de evaluación. Como caso de estudio, se usó un test informatizado de 30 preguntas (ítems) que se aplicó virtualmente a 775 estudiantes de una institución de educación superior matriculados en un curso de Estadística básica en el semestre 2016 II. El análisis de la confiabilidad, resultó con un alfa de Cronbach de 0.8325 pudiendo indicar una confiabilidad buena para la prueba informatizada, también fue corroborada con una correlación de Spearman-Brown de 0.815. El análisis con la TCT, el índice de dificultad identificó tres preguntas muy fáciles (V7, V8 y V12) que fueron retiradas, mientras el índice de discriminación no encontró ninguna pregunta con problemas. El supuesto de la unidimensionalidad de la prueba usando el análisis factorial fue probado con una variancia explicada del primer factor de 24.7%. El modelo logístico binario de la TRI que mejor se ajustó a los datos de la prueba fue el de tres parámetros (3PL). El proceso de calibración con el modelo 3PL, permitió retirar las preguntas V28 (índice de discriminación mayor 0.65) y V8, V12, V16 y V18 (índice del azar mayores a 0.4). Mientras que todas la preguntas estuvieron dentro del rango permitido para índice de dificultad",2018-02-20
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Regresión bayesiana con enlaces asimétricos para la clasificación de clientes con propensión a caer en mora en una entidad bancaria,"Fernández Vásquez, Richard Fernando",Magister Scientiae - Estadística Aplicada,"Chue Gallardo, Jorge","En la actualidad las entidades bancarias conviven con clientes que no cumplen con sus obligaciones crediticias y se exceden del plazo estipulado acordado con el banco, a estos clientes se les denomina clientes morosos, por tal motivo el objetivo del presente trabajo es determinar el modelo de regresión binaria bayesiano con enlace asimétrico más adecuado para clasificar a los clientes que incumplirán sus pagos de sus tarjetas de crédito según sus probabilidades de mora en la entidad bancaria UNIBANK y haciendo uso de las variables más significativas. Se realizó un análisis comparativo entre los modelos de regresión bayesiana con enlaces asimétricos cloglog, power logit y scobit, y se determinó que el modelo de regresión binaria bayesiano con enlace asimétrico cloglog fue el más adecuado para clasificar a los clientes que incumplen sus obligaciones crediticias con sus tarjetas de crédito en la entidad bancaria UNIBANK según su probabilidad de mora, pues este modelo presentó un valor mucho mayor de sensibilidad que los modelos power logit y scobit, siendo las diferencias 8.5% y 9.1%, respectivamente",2018-02-20
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Estimación de componentes de varianza utilizando los métodos bayesianos y máxima verosimilitud restringida para el estudio de la heredabilidad,"Vargas Paredes, Ana Cecilia",Magister Scientiae - Estadística Aplicada,"Maehara Oyata, Víctor Manuel","Se estimó mediante un modelo lineal mixto los componentes de varianza y heredabilidad de la producción de leche, a partir de los registros de 3397 lactaciones, provenientes de 1359 vacas de raza Holsteins, de 57 rebaños con información genealógica de 5 generaciones, utilizando máxima verosimilitud restringida conocida como REML y muestreo de Gibbs basado en procedimientos bayesianos. Con ambas metodologías se obtuvo una heredabilidad, en sentido amplio, moderada de 0.135 vía REML y una media de 0.318 vía muestreo de Gibbs. Para realizar el análisis exploratorio de residuales (en función de los tres tipos: marginal, residual condicional y efectos aleatorios) del modelo lineal mixto estimado vía REML, se adaptó funciones en R para incorporar la información genealógica o pedigrí al modelo. Como resultado de esto se verificó la linealidad de los efectos fijos y la normalidad del componente genético del animal. No se encontró normalidad para el efecto aleatorio del rebaño ni para los residuales condicionales. Para  estos últimos tampoco se observó homocedasticidad. Además, se encontró que para 132 animales la estructura de covarianza considerada en el modelo no es adecuada. También, se observó 215 animales y 7 rebaños con efectos atípicos. En el diagnóstico del procedimiento de simulación del muestreo de Gibbs desde la perspectiva bayesiana no se encontró problemas de convergencia. Se obtuvieron errores de Montecarlo bajos y tamaños efectivos de muestra mayores a 1000 para cada componente del modelo.",2017-11-13
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Modelos semiparamétricos de eventos recurrentes: caso aplicación a pacientes con cáncer de mama,"Rebaza Fernández, Diana del Rocío",Magister Scientiae - Estadística Aplicada,"Maehara Oyata, Víctor Manuel","La recurrencia de un evento en un paciente es la frecuencia observada de este en un periodo de tiempo durante el seguimiento al individuo, por ejemplo hospitalizaciones sucesivas de neumonía, episodios de epilepsia, recaídas de cáncer, entre otros. Los modelos de eventos recurrentes son muy útiles para la aplicación en estos fenómenos, y la presente investigación pretende ilustrar y comparar modelos particulares de datos de eventos recurrentes sin efecto aleatorio: Andersen y Gill (A-D); Wei, Lin y Weissfeld (WLW); y, Prentice, Williams  y Peterson (PWP), los cuales son modelos basados en la extensión de Cox de riesgos proporcionales, en estos modelos se asumen independencia de eventos. Otro modelo estudiado es el modelo de Fragilidad Compartida Gamma para eventos recurrentes que considera un término de fragilidad y asume que este término influye en la recurrencia de los eventos de un mismo sujeto. Para la estimación de los parámetros en los modelos sin efecto aleatorio se utilizó el método de máxima verosimilitud parcial mientras que para el modelo de fragilidad  fue el método de máxima verosimilitud penalizado, el cual penaliza la función de riesgo base. Los datos usados para la aplicación de estas metodologías fue proporcionada por el médico Ginecólogo Oncólogo Dr. Vladimir Villoslada Terrones del Instituto Nacional de Enfermedades Neoplásicas (INEN). Estos datos describen un conjunto de variables relacionados al cáncer de mama en una cohorte prospectiva de 68 pacientes con diagnóstico positivo, sometidos a una cirugía mastectomía. Al procesar y analizar los resultados obtenidos, se encontró que el modelo Andersen y Gill (A-D) y Prentice, Williams y Peterson (PWP) son los que ajustan mejor a este conjunto de datos. Entre los resultados encontrados se obtuvo que los factores asociados al riesgo de recurrencia de cáncer de mama son la edad de inicio al estudio, la edad de primera menstruación (menarquia) y tipo carcinoma lobulillar. Estos modelos presentan similares resultados debido a la significancia estadística en las variables y el cumplimiento del supuesto de riesgos proporcionales.",2017-11-13
Universidad Nacional Agraria La Molina. Escuela de Posgrado,"Modelos estadísticos en procesos puntuales espaciales Poisson para evaluar la distribución espacial de los hechos delictivos en Lima, Perú","Quispe Quispe, Braulio",Magister Scientiae - Estadística Aplicada,"López de Castilla Vásquez, Carlos","La presente tesis plantea una aplicación de los modelos estadísticos de procesos puntuales espaciales Poisson así como de los modelos Clúster del tipo Neyman - Scott. Particularmente, se enfoca en evaluar la distribución espacial de hechos delictivos y su relación con algunas covariables espaciales. De esta forma se permitirá orientar y/o establecer políticas referidas a seguridad ciudadana de índole nacional y/o local. El área de estudio corresponde a los distritos de Lima Centro y Residencial, para lo cual se toma en cuenta la información de ubicaciones georreferenciadas de los hechos delictivos reportados por las víctimas a finales del año 2013 hasta inicios del 2014. Las ubicaciones de los delitos son representadas por puntos, y el conjunto de estos se consideran un patrón puntual, el cual representa una realización de un proceso puntual espacial subyacente en el espacio de estudio. El modelamiento estadístico se realiza a través de la intensidad de puntos, la cual puede ser estimada para cualquier ubicación específica del área de estudio y son los modelos log-lineales los más usados para representar su relación con un conjunto de covariables espaciales cuyos efectos podemos representar en un conjunto de parámetros; a estos modelos se les conocen como modelos paramétricos de procesos puntuales espaciales. Las estadísticas de resumen, conocidas también como propiedades de primer y segundo orden de un proceso puntual así como los métodos basados en distancia entre puntos, han sido aplicados con fines de realizar el análisis exploratorio y determinar: el tipo de distribución espacial (regular, aleatorio o clústeres) que siguen los hechos delictivos (patrón puntual), la distribución de la distancia de un punto arbitrario a un lugar de ocurrencia de un delito y de la distancia de un hecho delictivo a otro, entre otras. Finalmente, se concluye que la distribución espacial de los hechos delictivos en Lima, no es homogénea, existiendo clustering o agregación de puntos, los cuales se traducen en zonas con mayor incidencia de hechos delictivos y su intensidad guarda relación con la ubicación de los límites distritales, la inversión destinada al orden interno y la densidad poblacional.",2017-10-03
Universidad Nacional Agraria La Molina. Escuela de Posgrado,Aplicación de redes neuronales y regresión logística para predecir el éxito de la compra de deuda de una entidad financiera,"Muñoz Muñoz, Emanuel Guillermo",Magister Scientiae - Estadística Aplicada,"Chue Gallardo, Jorge","El propósito de este trabajo es presentar y aplicar la técnica de redes neuronales para predecir el éxito de la compra de deuda de una entidad financiera a otra. La técnica de redes neuronales se sustenta en el perceptron multicapa y en el algoritmo de retropropagación. Se explica el uso de la técnica mostrando sus diferentes pasos: establecimiento de la estructura, la función de activación (sigmoidea), el paradigma de aprendizaje, el factor de aprendizaje, la regla de aprendizaje, el algoritmo de aprendizaje (retropropagación), el entrenamiento y la evaluación de la red neuronal. Se realizó un pre procesamiento para tener datos de calidad, con estos datos limpios se aplicó la técnica de redes neuronales en la clasificación y para evaluar esta clasificación se realizó el post procesamiento, se puede notar que todo este proceso es minería de datos. Lo más resaltante en las redes neuronales son los valores hallados para los parámetros, identificados a través del entrenamiento. Estos parámetros son denominados pesos dinámicos, en estos pesos está el conocimiento de la red neuronal para poder lograr la predicción. El error de clasificación que se obtiene al aplicar a los datos de prueba a la red neuronal entrenada con las especificaciones ya señaladas fue de 22.89% y 4.31% para la red de cuatro y dos neuronas en la capa de salida respectivamente. Al probar con más de siete neuronas se obtuvieron errores de clasificaciones similares o mayores, esto se logró con un mayor costo computacional. Al comparar los resultados de los mismos datos con la regresión logística, se obtuvieron estos errores de clasificación 22.72% y 4.31% para los datos con la variable respuesta de cuatro y dos clases respectivamente.También evaluaron los modelos de clasificación con la técnica de validación cruzada, en este caso se obtuvo  76.57% y 77.29% que son los porcentajes de la eficiencia de predicción de redes neuronales y regresión logística respectivamente, solo para los casos donde la variable respuesta de los datos era de cuatro niveles",2016-08-02
