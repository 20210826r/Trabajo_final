{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f014921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###-Repositorio estadistica informatica pregrado UNALM-### \n",
    "\n",
    "#Trabajo Final → AVANCE.\n",
    "#1. Relación de repositorios con sus respectivos url\n",
    "#2. Automatización de la descarga de la información de un repositorio.\n",
    "#3. Estructuración de la información descargada en formato csv\n",
    "\n",
    "#repositorios\n",
    "\n",
    "#1. UNALM Pregrado #http://repositorio.lamolina.edu.pe/handle/20.500.12996/26/recent-submissions\n",
    "#2. PUCP Posgrado #https://tesis.pucp.edu.pe/repositorio/handle/20.500.12404/757/recent-submissions\n",
    "#3. UNALM Posgrado #http://repositorio.lamolina.edu.pe/handle/20.500.12996/329\n",
    "\n",
    "\n",
    "## Repositorio 1 PUCP pregrado \n",
    "url = 'https://tesis.pucp.edu.pe/repositorio/handle/20.500.12404/757/recent-submissions'\n",
    "\n",
    "#paquetes \n",
    "import urllib.request, urllib.error\n",
    "from bs4 import BeautifulSoup\n",
    "import re as re \n",
    "import pandas as pd\n",
    "\n",
    "import ssl\n",
    "context = ssl._create_unverified_context() #la pagina pide un socket seguro \n",
    "\n",
    "#Las tesis se encuentran en 4 paginas distinas \n",
    "\n",
    "#devuelve la url de la siguiente pagina \n",
    "def next_page(url):\n",
    "    page = urllib.request.urlopen(url,context = context )\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    \n",
    "    line = soup.find('a',{'class':'next-page-link'})\n",
    "    line = line.decode().strip()\n",
    "    offset = re.findall(r'\\?offset=.{2}',line)\n",
    "    url_diff = offset[0]\n",
    "    \n",
    "    url = re.sub('\\?offset.*','',url)\n",
    "    n_url = url + url_diff\n",
    "    return n_url\n",
    "\n",
    "URL = []\n",
    "url1 = url\n",
    "\n",
    "#devuelve las url de todas las paginas \n",
    "def all_pages(url,URL):\n",
    "    \n",
    "    page = urllib.request.urlopen(url,context = context)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    last = soup.find('li',{'class':'next pull-right disabled'})\n",
    "    URL.append(url)\n",
    "    if last == None: \n",
    "        url = next_page(url)\n",
    "        all_pages(url,URL)\n",
    "    else: \n",
    "        return URL \n",
    "    return URL \n",
    "all_pages(url1,URL)\n",
    "\n",
    "###Almacenar links de las tesis \n",
    "\n",
    "handles = [] \n",
    "\n",
    "#las url de las tesis se diferencian por el codigo handle/20.500.12404/***** <-- diferencia \n",
    "for urls in URL: \n",
    "    page = urllib.request.urlopen(urls,context = context)\n",
    "    soup = BeautifulSoup(page, \"html.parser\")\n",
    "    for lines in soup.find_all('h4'):\n",
    "        lines = lines.decode().strip()\n",
    "        link = re.findall(r'(?<=20.500.12404/).+?(?=\\\")',lines) #handle unico a cada tesis (#12996)\n",
    "        handles.append(link)\n",
    "       \n",
    "## Links completos \n",
    "\n",
    "#creando las url especificas de cada tesis \n",
    "links=[]\n",
    "def get_links(handles,links):\n",
    "    handle_url = re.sub('(?<=20.500.12404/).+','',url)\n",
    "    for i in range(len(handles)): \n",
    "        links.append(handle_url+handles[i][0])\n",
    "    return links\n",
    "\n",
    "get_links(handles,links)    \n",
    "\n",
    "#agregando ?show=full a las url se muestra informacion mas detallada de cada tesis \n",
    "links_full = [] \n",
    "for link in links: \n",
    "    links_full.append(link+'?show=full')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf710827",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## almacenando en diccionario \n",
    "\n",
    "rep_pucp = { \n",
    "        'unis' : [],\n",
    "        'titulos' : [],\n",
    "        'nombres' : [],\n",
    "        'grados' : [], \n",
    "        'asesores' : [],\n",
    "        'resumenes': [],\n",
    "        'años' : [] }\n",
    " \n",
    "##buscar informacion especifica en las url de las tesis  \n",
    "def get_all(links_full,reps_unalm):\n",
    "    for i in range(len(links_full)): \n",
    "        page = urllib.request.urlopen(links_full[i],context = context)\n",
    "        soup = BeautifulSoup(page)\n",
    "        full_t=''\n",
    "        for line in soup.find_all('tr'):\n",
    "            full_t = line.decode().strip()+full_t\n",
    "        #se encuentra la informacion \n",
    "        uni = re.findall(r'(?<=>thesis\\.degree\\.grantor</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        titulo = re.findall(r'(?<=>dc\\.title</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        nombre = re.findall(r'(?<=>dc\\.contributor\\.author</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        grado = re.findall(r'(?<=>thesis\\.degree\\.name</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        asesor = re.findall(r'(?<=>dc\\.contributor\\.advisor</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        #algunos resumenes tienen \\n asi que se usa [\\s\\S]\n",
    "        resumen = re.findall(r'(?<=</tr><tr class=\\\"ds-table-row even\\\">\\n<td class=\\\"label-cell\\\">dc.description.abstract</td><td class=\\\"word-break\\\">)[\\s\\S]*?(?=</td><td>es_ES)',full_t)  \n",
    "        año = re.findall(r'(?<=>dc\\.date\\.issued</td><td class=\\\"word-break\\\">).+?(?=</td>)',full_t)\n",
    "        \n",
    "        rep_unalm['unis'].append(uni[0])\n",
    "        rep_unalm['titulos'].append(titulo[0])     \n",
    "        rep_unalm['nombres'].append(nombre[0])\n",
    "        rep_unalm['grados'].append(grado[0]) \n",
    "        \n",
    "        #estas dos variables generaron N/A en algun momento del codigo \n",
    "        if asesor == []: \n",
    "            rep_unalm['asesores'].append('Sin asesor')\n",
    "        else:\n",
    "            rep_unalm['asesores'].append(asesor[0])\n",
    "        if resumen == []: \n",
    "            rep_unalm['resumenes'].append('Sin resumen')\n",
    "        else:\n",
    "            rep_unalm['resumenes'].append(resumen[0])\n",
    "            \n",
    "        rep_unalm['años'].append(año[0])\n",
    "    return reps_unalm\n",
    "\n",
    "universities = get_all(links_full,rep_unalm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "5c0a2c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "La presente tesis propone un modelo de regresi on cuant  lica en d onde la variable es no\r\n",
      "negativa y posee censura intervalar, es decir que esta no es directamente observable, y la  unica\r\n",
      "informaci on que se conoce sobre ella es que se encuentra en cierto intervalo. Para evaluar si\r\n",
      "la metodolog  a de estimaci on captura adecuadamente los par ametros poblacionales desde el\r\n",
      "punto de vista de la inferencia cl asica, se desarrolla un estudio de simulaci on. Finalmente, se\r\n",
      "aplica el modelo a los datos de la Encuesta Nacional de Satisfacci on de Salud ejecutada el\r\n",
      "a~no 2015. La estructura del modelo permite evaluar los factores relacionados al sueldo de los\r\n",
      "profesionales en salud (el cual hab  a sido censurado desde el proceso de recolecci on de datos).\r\n",
      "El presente modelo es una extensi on al modelo de regresi on de censura intervalar expuesto\r\n",
      "en Sal y Rosas et al. (2019), pues eval ua los factores subyacentes a una variable respuesta a\r\n",
      "lo largo de sus cuantiles.\n",
      "En la tariﬁcación de seguros generales, en particular en seguros de vehículos, es valioso incorporar toda la información disponible del asegurado, del bien asegurado y de los siniestros que se han presentado, con el ﬁn de obtener modelos que consideren las variables relevantes en la estimación y así generar una prima de riesgo adecuada para el riesgo que se está analizando.\r\n",
      "Los modelos a considerar están construidos con base en las reclamaciones que ha presentado el asegurado y su estimaci´on se obtiene mediante distribuciones del número y monto de siniestros dando como resultado tarifas que incluyen recargos y descuentos en base a la experiencia siniestral, lo que se conoce como Sistema Bonus-Malus. Adicionalmente se han analizado modelos de regresión que incluyen información tanto del asegurado como del vehículo y cuya estimación de la prima de riesgo se realiza a través de la media tanto de la frecuencia como de la severidad. Sin embargo, dado que los riesgos en la cartera expuesta son heterogéneos, se plantean también modelos de regresión en los que la estimación de la frecuencia y la severidad se realiza a través de parámetros como: la media, la varianza, el sesgo y la curtosis, estos últimos son denominados modelos aditivos generalizados de localización, escala y forma (GAMLSS).\n",
      "La prevalencia de enfermedades epidemiológicas recolectadas en áreas geográficamente limitadas, como distritos o provincias, son cruciales para la toma de decisiones en salud pública. Usualmente esta variable respuesta presenta dependencia espacial, es decir, es similar en áreas vecinas, debido a la naturaleza de la enfermedad, clima, nivel económico y cultural, entre otras razones. En este sentido, se proponen modelos espaciales de datos áreas para identificar tendencias y factores asociados a enfermedades epidemiológicas, tomando en cuenta la dependencia espacial entre áreas geográficas. Por lo general, estos modelos ajustan a la dependencia espacial a través de efectos aleatorios derivados a través de grafos. En particular, el modelo autorregresivo de gráfico acíclico dirigido (DAGAR) se basa en un grafo acíclico dirigido y algunos efectos aleatorios \\del pasado\". Como consecuencia, la matriz de precisión (inversa de la covarianza) del modelo es dispersa. Este modelo tiene una interpretación intuitiva de los parámetros asociados con la dependencia espacial y se puede representar como un modelo gaussiano latente. En este contexto, en esta tesis se propone implementar el modelo DAGAR a través del método de inferencia bayesiano aproximado INLA que es determinista, bastante preciso y eficiente. Dentro de este enfoque, la estimación de datos grandes se puede realizar en segundos o minutos, y permite ajustar los datos con distribución gaussiana o no gaussiana. Finalmente, para mostrar el aporte de esta propuesta, el modelo DAGAR se ajusta a datos reales.\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame(universities)\n",
    "df.to_csv('tesisPUCPfinal.csv', index=False,encoding='utf-8-sig',\n",
    "          header = ['Institución','Título de la tesis', 'Nombre del tesista', 'Grado', 'Nombre del asesor', 'Resumen','Año'])\n",
    "\n",
    "df_read = pd.read_csv('tesisPUCPfinal.csv')\n",
    "\n",
    "for resumen in df_read.loc[3:5,'Resumen']:\n",
    "    print(resumen) \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "aa2056bb4ca5633605a78966d1098f4e7323f18ac516816354bb04acd5b66807"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
